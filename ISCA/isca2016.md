### isca 2016 | 58 papers.
---
### Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.11
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551378
* **Key Words**: Neurons, Computer architecture, Hardware, Acceleration, Neural networks, Delays, Feature extraction, decision making, memory architecture, neural nets, parallel architectures, performance evaluation, power aware computing, Cnvlutin, ineffectual-neuron-free deep neural network computing, DNN, value-based approach, hardware acceleration, hierarchical data-parallel units, data storage format encoding, computation elimination decisions, data parallel units, data storage format, data-parallel architecture, memory hierarchy, energy efficiency, image classification, zero-valued operand multiplications, EDP, energy delay product, ED2P, energy delay squared product, 
* **Abstract**: This work observes that a large fraction of the computations performed by Deep Neural Networks (DNNs) are intrinsically ineffectual as they involve a multiplication where one of the inputs is zero. This observation motivates Cnvolutin (CNV), a value-based approach to hardware acceleration that eliminates most of these ineffectual operations, improving performance and energy over a state-of-the-art accelerator with no accuracy loss. CNV uses hierarchical data-parallel units, allowing groups of lanes to proceed mostly independently enabling them to skip over the ineffectual computations. A co-designed data storage format encodes the computation elimination decisions taking them off the critical path while avoiding control divergence in the data parallel units. Combined, the units and the data storage format result in a data-parallel architecture that maintains wide, aligned accesses to its memory hierarchy and that keeps its data lanes busy. By loosening the ineffectual computation identification criterion, CNV enables further performance and energy efficiency improvements, and more so if a loss in accuracy is acceptable. Experimental measurements over a set of state-of-the-art DNNs for image classification show that CNV improves performance over a state-of-the-art accelerator from 1.24× to 1.55× and by 1.37× on average without any loss in accuracy by removing zero-valued operand multiplications alone. While CNV incurs an area overhead of 4.49%, it improves overall EDP (Energy Delay Product) and ED 2 P (Energy Delay Squared Product) on average by 1.47× and 2.01×, respectively. The average performance improvements increase to 1.52× without any loss in accuracy with a broader ineffectual identification policy. Further improvements are demonstrated with a loss in accuracy.

---
### ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.12
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551379
* **Key Words**: Neurons, Computer architecture, Kernel, Machine learning algorithms, Memristors, Pipelines, Biological neural networks, digital arithmetic, DRAM chips, learning (artificial intelligence), memristor circuits, neural nets, DaDianNao architecture, ISAAC architecture, memristor storage, ADC, analog-to-digital conversion, data encoding, pipelined architecture design, dot-product operations, memristor crossbar arrays, in-situ processing approach, eDRAM banks, digital arithmetic operations, machine learning algorithms, in-situ analog arithmetic crossbars, convolutional neural network accelerator, CNN, DNN, memristor, analog, neural, accelerator, 
* **Abstract**: A number of recent efforts have attempted to design accelerators for popular machine learning algorithms, such as those involving convolutional and deep neural networks (CNNs and DNNs). These algorithms typically involve a large number of multiply-accumulate (dot-product) operations. A recent project, DaDianNao, adopts a near data processing approach, where a specialized neural functional unit performs all the digital arithmetic operations and receives input weights from adjacent eDRAM banks. This work explores an in-situ processing approach, where memristor crossbar arrays not only store input weights, but are also used to perform dot-product operations in an analog manner. While the use of crossbar memory as an analog dot-product engine is well known, no prior work has designed or characterized a full-fledged accelerator based on crossbars. In particular, our work makes the following contributions: (i) We design a pipelined architecture, with some crossbars dedicated for each neural network layer, and eDRAM buffers that aggregate data between pipeline stages. (ii) We define new data encoding techniques that are amenable to analog computations and that can reduce the high overheads of analog-to-digital conversion (ADC). (iii) We define the many supporting digital components required in an analog CNN accelerator and carry out a design space exploration to identify the best balance of memristor storage/compute, ADCs, and eDRAM storage on a chip. On a suite of CNN and DNN workloads, the proposed ISAAC architecture yields improvements of 14.8×, 5.5×, and 7.5× in throughput, energy, and computational density (respectively), relative to the state-of-the-art DaDianNao architecture.

---
### PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.13
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551380
* **Key Words**: Artificial neural networks, Random access memory, Microprocessors, Acceleration, Biological neural networks, Memory management, learning (artificial intelligence), matrix algebra, memory architecture, neural nets, resistive RAM, vectors, PRIME, processing-in-memory architecture, neural network computation, ReRAM, main memory, PIM, memory wall, metal-oxide resistive random access memory, matrix-vector multiplication, microarchitecture, circuit designs, software/hardware interface, machine learning benchmarks, processing in memory, neural network, resistive random access memory, 
* **Abstract**: Processing-in-memory (PIM) is a promising solution to address the “memory wall” challenges for future computer systems. Prior proposed PIM architectures put additional computation logic in or near memory. The emerging metal-oxide resistive random access memory (ReRAM) has showed its potential to be used for main memory. Moreover, with its crossbar array structure, ReRAM can perform matrixvector multiplication efficiently, and has been widely studied to accelerate neural network (NN) applications. In this work, we propose a novel PIM architecture, called PRIME, to accelerate NN applications in ReRAM based main memory. In PRIME, a portion of ReRAM crossbar arrays can be configured as accelerators for NN applications or as normal memory for a larger memory space. We provide microarchitecture and circuit designs to enable the morphable functions with an insignificant area overhead. We also design a software/hardware interface for software developers to implement various NNs on PRIME. Benefiting from both the PIM architecture and the efficiency of using ReRAM for NN computation, PRIME distinguishes itself from prior work on NN acceleration, with significant performance improvement and energy saving. Our experimental results show that, compared with a state-of-the-art neural processing unit design, PRIME improves the performance by ~2360x and the energy consumption by ~895x, across the evaluated machine learning benchmarks.

---
### Asymmetry-Aware Work-Stealing Runtimes.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.14
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551381
* **Key Words**: Runtime, Multicore processing, Throughput, Voltage control, Software, Very large scale integration, multiprocessing systems, power aware computing, VLSI, voltage regulators, asymmetry-aware work-stealing runtimes, AAWS runtimes, Amdahl's law, multicore processor, task distribution, task-based parallelism, static asymmetry, dynamic asymmetry, work-pacing, work-sprinting, work-mugging, marginal-utility-based approach, integrated voltage regulators, energy efficiency, lightweight user-level interrupts, VLSI, multicore systems, 
* **Abstract**: Amdahl's law provides architects a compelling reason to introduce system asymmetry to optimize for both serial and parallel regions of execution. Asymmetry in a multicore processor can arise statically (e.g., from core microarchitecture) or dynamically (e.g., applying dynamic voltage/frequency scaling). Work stealing is an increasingly popular approach to task distribution that elegantly balances task-based parallelism across multiple worker threads. In this paper, we propose asymmetry-aware work-stealing (AAWS) runtimes, which are carefully designed to exploit both the static and dynamic asymmetry in modern systems. AAWS runtimes use three key hardware/software techniques: work-pacing, work-sprinting, and work-mugging. Work-pacing and work-sprinting are novel techniques that combine a marginal-utility-based approach with integrated voltage regulators to improve performance and energy efficiency in high-and low-parallel regions. Work-mugging is a previously proposed technique that enables a waiting big core to preemptively migrate work from a busy little core. We propose a simple implementation of work-mugging based on lightweight user-level interrupts. We use a vertically integrated research methodology spanning software, architecture, and VLSI to make the case that holistically combining static asymmetry, dynamic asymmetry, and work-stealing runtimes can improve both performance and energy efficiency in future multicore systems.

---
### Morpheus: Creating Application Objects Efficiently for Heterogeneous Computing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.15
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551382
* **Key Words**: Computational modeling, Central Processing Unit, Bandwidth, Graphics processing units, Nonvolatile memory, Performance evaluation, Data models, coprocessors, memory architecture, Morpheus model, application objects, heterogeneous computing, high performance computing systems, object deserialization, storage device, system overheads, CPU, main memory resources, compute-intensive workloads, I/O bandwidth, power consumption reduction, heterogeneous coprocessor-equipped systems, peer-to-peer transfer, main memory utilizations, Morpheus-SSD, NVMe-P2P, 
* **Abstract**: In high performance computing systems, object deserialization can become a surprisingly important bottleneck-in our test, a set of general-purpose, highly parallelized applications spends 64% of total execution time deserializing data into objects. This paper presents the Morpheus model, which allows applications to move such computations to a storage device. We use this model to deserialize data into application objects inside storage devices, rather than in the host CPU. Using the Morpheus model for object deserialization avoids unnecessary system overheads, frees up scarce CPU and main memory resources for compute-intensive workloads, saves I/O bandwidth, and reduces power consumption. In heterogeneous, co-processor-equipped systems, Morpheus allows application objects to be sent directly from a storage device to a coprocessor (e.g., a GPU) by peer-to-peer transfer, further improving application performance as well as reducing the CPU and main memory utilizations. This paper implements Morpheus-SSD, an SSD supporting the Morpheus model. Morpheus-SSD improves the performance of object deserialization by 1.66×, reduces power consumption by 7%, uses 42% less energy, and speeds up the total execution time by 1.32×. By using NVMe-P2P that realizes peer-to-peer communication between Morpheus-SSD and a GPU, Morpheus-SSD can speed up the total execution time by 1.39× in a heterogeneous computing platform.

---
### Towards Statistical Guarantees in Controlling Quality Tradeoffs for Approximate Acceleration.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.16
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551383
* **Key Words**: Hardware, Acceleration, Optimization, Software, Training, Runtime, Quality control, hardware-software codesign, inference mechanisms, neural nets, optimisation, pattern classification, quality control, statistical analysis, quality control, approximate accelerator, quality degradation, decision space, MITHRA, codesigned hardware-software solution, quality loss, binary classification task, software mechanism, statistical optimization problem, table-based hardware classifiers, neural network based hardware classifiers, table-based design, Approximate computing, accelerators, quality control, statistical guarantees, statistical compiler optimization, 
* **Abstract**: Conventionally, an approximate accelerator replaces every invocation of a frequently executed region of code without considering the final quality degradation. However, there is a vast decision space in which each invocation can either be delegated to the accelerator-improving performance and efficiency-or run on the precise core-maintaining quality. In this paper we introduce MITHRA, a co-designed hardware-software solution, that navigates these tradeoffs to deliver high performance and efficiency while lowering the final quality loss. MITHRA seeks to identify whether each individual accelerator invocation will lead to an undesirable quality loss and, if so, directs the processor to run the original precise code. This identification is cast as a binary classification task that requires a cohesive co-design of hardware and software. The hardware component performs the classification at runtime and exposes a knob to the software mechanism to control quality tradeoffs. The software tunes this knob by solving a statistical optimization problem that maximizes benefits from approximation while providing statistical guarantees that final quality level will be met with high confidence. The software uses this knob to tune and train the hardware classifiers. We devise two distinct hardware classifiers, one table-based and one neural network based. To understand the efficacy of these mechanisms, we compare them with an ideal, but infeasible design, the oracle. Results show that, with 95% confidence the table-based design can restrict the final output quality loss to 5% for 90% of unseen input sets while providing 2.5× speedup and 2.6× energy efficiency. The neural design shows similar speedup however, improves the efficiency by 13%. Compared to the table-based design, the oracle improves speedup by 26% and efficiency by 36%. These results show that MITHRA performs within a close range of the oracle and can effectively navigate the quality tradeoffs in approximate acceleration...

---
### Back to the Future: Leveraging Belady's Algorithm for Improved Cache Replacement.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.17
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551384
* **Key Words**: Optimized production technology, History, Prediction algorithms, Marine vehicles, Benchmark testing, Hardware, Art, cache storage, Belady algorithm, cache replacement, cache accesses, Belady behavior, sampling techniques, LLC, replacement state, tag array, memory-intensive subset, 4-core system, Cache replacement, Belady's Algorithm, 
* **Abstract**: Belady's algorithm is optimal but infeasible because it requires knowledge of the future. This paper explains how a cache replacement algorithm can nonetheless learn from Belady's algorithm by applying it to past cache accesses to inform future cache replacement decisions. We show that the implementation is surprisingly efficient, as we introduce a new method of efficiently simulating Belady's behavior, and we use known sampling techniques to compactly represent the long history information that is needed for high accuracy. For a 2MB LLC, our solution uses a 16KB hardware budget (excluding replacement state in the tag array). When applied to a memory-intensive subset of the SPEC 2006 CPU benchmarks, our solution improves performance over LRU by 8.4%, as opposed to 6.2% for the previous state-of-the-art. For a 4-core system with a shared 8MB LLC, our solution improves performance by 15.0%, compared to 12.0% for the previous state-of-the-art.

---
### Efficient Synonym Filtering and Scalable Delayed Translation for Hybrid Virtual Caching.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.18
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551385
* **Key Words**: Coherence, Memory management, System-on-chip, Power demand, Energy consumption, Protocols, Operating systems, cache storage, data structures, memory architecture, power aware computing, storage allocation, synonym filtering, scalable delayed translation, hybrid virtual caching, translation look-aside buffers, address translation, memory accesses, L1 cache hits, TLB latency restrictions, TLB miss reductions, energy consumption reduction, instruction fetch, data access, L1 cache misses, hybrid virtual memory architecture, cache hierarchy, virtual addresses, address space identifiers, ASID, common nonsynonym addresses, last-level cache misses, LLC misses, uncommon synonym addresses, synonym detection mechanism, Bloom filters, fixed-granularity delayed TLBs, translation scalability problem, delayed many segment translation, address translation, hybrid virtual cache, synonym detection, segmented translation, 
* **Abstract**: Conventional translation look-aside buffers (TLBs) are required to complete address translation with short latencies, as the address translation is on the critical path of all memory accesses even for L1 cache hits. Such strict TLB latency restrictions limit the TLB capacity, as the latency increase with large TLBs may lower the overall performance even with potential TLB miss reductions. Furthermore, TLBs consume a significant amount of energy as they are accessed for every instruction fetch and data access. To avoid the latency restriction and reduce the energy consumption, virtual caching techniques have been proposed to defer translation to after L1 cache misses. However, an efficient solution for the synonym problem has been a critical issue hindering the wide adoption of virtual caching. Based on the virtual caching concept, this study proposes a hybrid virtual memory architecture extending virtual caching to the entire cache hierarchy, aiming to improve both performance and energy consumption. The hybrid virtual caching uses virtual addresses augmented with address space identifiers (ASID) in the cache hierarchy for common non-synonym addresses. For such non-synonyms, the address translation occurs only after last-level cache (LLC) misses. For uncommon synonym addresses, the addresses are translated to physical addresses with conventional TLBs before L1 cache accesses. To support such hybrid translation, we propose an efficient synonym detection mechanism based on Bloom filters which can identify synonym candidates with few false positives. For large memory applications, delayed translation alone cannot solve the address translation problem, as fixed-granularity delayed TLBs may not scale with the increasing memory requirements. To mitigate the translation scalability problem, this study proposes a delayed many segment translation designed for the hybrid virtual caching. The experimental results show that our approach effectively lowers accesses to the TLBs, ...

---
### LAP: Loop-Block Aware Inclusion Properties for Energy-Efficient Asymmetric Last Level Caches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.19
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551386
* **Key Words**: Random access memory, Nonvolatile memory, Energy consumption, Hardware, Torque, Magnetic tunneling, Throughput, cache storage, energy conservation, energy consumption, power aware computing, random-access storage, LAP, loop-block aware inclusion properties, energy-efficient asymmetric last level caches, selective inclusion policy, loop-block-aware policy, energy consumption reduction, asymmetric read-write properties, energy-efficient data placement, NVM, STT-RAM, nonvolatile memory, spin-transfer torque RAM, non-volatile memory, inclusion property, energy efficiency, 
* **Abstract**: Emerging non-volatile memory (NVM) technologies, such as spin-transfer torque RAM (STT-RAM), are attractive options for replacing or augmenting SRAM in implementing last-level caches (LLCs). However, the asymmetric read/write energy and latency associated with NVM introduces new challenges in designing caches where, in contrast to SRAM, dynamic energy from write operations can be responsible for a larger fraction of total cache energy than leakage. These properties lead to the fact that no single traditional inclusion policy being dominant in terms of LLC energy consumption for asymmetric LLCs. We propose a novel selective inclusion policy, Loop-block-Aware Policy (LAP), to reduce energy consumption in LLCs with asymmetric read/write properties. In order to eliminate redundant writes to the LLC, LAP incorporates advantages from both non-inclusive and exclusive designs to selectively cache only part of upper-level data in the LLC. Results show that LAP outperforms other variants of selective inclusion policies and consumes 20% and 12% less energy than non-inclusive and exclusive STT-RAM-based LLCs, respectively. We extend LAP to a system with SRAM/STT-RAM hybrid LLCs to achieve energy-efficient data placement, reducing the energy consumption by 22% and 15% over non-inclusion and exclusion on average, with average-case performance improvements, small worst-case performance loss, and minimal hardware overheads.

---
### Automatic Generation of Efficient Accelerators for Reconfigurable Hardware.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.20
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551387
* **Key Words**: Field programmable gate arrays, Hardware, Pipeline processing, Space exploration, Estimation, Design tools, estimation theory, field programmable gate arrays, neural chips, random-access storage, reconfigurable architectures, automatic generation, reconfigurable hardware, general purpose processor, application-specific accelerator, high-level FPGA design tool, high-level programming, resource estimation, automatic design space exploration, high-level language, parallel pattern, hybrid area estimation technique, template-level model, design-level artificial neural network, hardware place-and-route tool, block RAM duplication, LUT packing, off-chip memory access, tile size, parallelization factor, optional coarse-grained pipelining, CPU code, hardware generation, design space exploration, FPGAs, parallel patterns, hardware definition language, reconfigurable hardware, application-specific accelerators, 
* **Abstract**: Acceleration in the form of customized datapaths offer large performance and energy improvements over general purpose processors. Reconfigurable fabrics such as FPGAs are gaining popularity for use in implementing application-specific accelerators, thereby increasing the importance of having good high-level FPGA design tools. However, current tools for targeting FPGAs offer inadequate support for high-level programming, resource estimation, and rapid and automatic design space exploration. We describe a design framework that addresses these challenges. We introduce a new representation of hardware using parameterized templates that captures locality and parallelism information at multiple levels of nesting. This representation is designed to be automatically generated from high-level languages based on parallel patterns. We describe a hybrid area estimation technique which uses template-level models and design-level artificial neural networks to account for effects from hardware place-and-route tools, including routing overheads, register and block RAM duplication, and LUT packing. Our runtime estimation accounts for off-chip memory accesses. We use our estimation capabilities to rapidly explore a large space of designs across tile sizes, parallelization factors, and optional coarse-grained pipelining, all at multiple loop levels. We show that estimates average 4.8% error for logic resources, 6.1% error for runtimes, and are 279 to 6533 times faster than a commercial high-level synthesis tool. We compare the best-performing designs to optimized CPU code running on a server-grade 6 core processor and show speedups of up to 16.7×.

---
### Strober: Fast and Accurate Sample-Based Energy Simulation for Arbitrary RTL.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.21
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551388
* **Key Words**: Solid modeling, Field programmable gate arrays, Sociology, Statistics, Computational modeling, Estimation, Logic gates, CAD, circuit simulation, field programmable gate arrays, logic design, public domain software, Strober, arbitrary RTL designs, FPGA, RTL state snapshots, workload-specific average power estimate, confidence intervals, four-orders-of-magnitude speedup, CAD gate-level simulation tools, energy estimates, open-source sample-based energy simulation tool, design-space exploration, field programmable gate array, register transfer logic, Design, Energy, Experimentation, FPGA, Hardware, Modeling, Performance, Power estimation, Statistical sampling, 
* **Abstract**: This paper presents a sample-based energy simulation methodology that enables fast and accurate estimations of performance and average power for arbitrary RTL designs. Our approach uses an FPGA to simultaneously simulate the performance of an RTL design and to collect samples containing exact RTL state snapshots. Each snapshot is then replayed in gate-level simulation, resulting in a workload-specific average power estimate with confidence intervals. For arbitrary RTL and workloads, our methodology guarantees a minimum of four-orders-of-magnitude speedup over commercial CAD gate-level simulation tools and gives average energy estimates guaranteed to be within 5% of the true average energy with 99% confidence. We believe our open-source sample-based energy simulation tool Strober can not only rapidly provide ground truth for more abstract power models, but can enable productive design-space exploration early in the RTL design process.

---
### PowerChop: Identifying and Managing Non-critical Units in Hybrid Processor Architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.22
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551389
* **Key Words**: Graphics processing units, Hybrid power systems, Computer architecture, Logic gates, Hardware, Microarchitecture, hardware-software codesign, microprocessor chips, power consumption, PowerChop, noncritical units identification, noncritical units management, hybrid processor architectures, on-core microarchitectural structures, processor power budget, HW/SW codesigned hybrid processors, unit-level power management, application phase level, hardware units, phase identification, power states, software layer, power consumption, leakage power, hybrid server processor, hybrid mobile processor, power gating units, power management, hybrid architecture, 
* **Abstract**: On-core microarchitectural structures consume significant portions of a processor's power budget. However, depending on application characteristics, those structures do not always provide (much) performance benefit. While timeout-based power gating techniques have been leveraged for underutilized cores and inactive functional units, these techniques have not directly translated to high-activity units such as vector processing units, complex branch predictors, and caches. The performance benefit provided by these units does not necessarily correspond with unit activity, but instead is a function of application characteristics. This work introduces PowerChop, a novel technique that leverages the unique capabilities of HW/SW co-designed hybrid processors to enact unit-level power management at the application phase level. PowerChop adds two small additional hardware units to facilitate phase identification and triggering different power states, enabling the software layer to cheaply track, predict and take advantage of varying unit criticality across application phases by powering gating units that are not needed for performant execution. Through detailed experimentation, we find that PowerChop significantly decreases power consumption, reducing the leakage power of a hybrid server processor by 9% on average (up to 33%) and a hybrid mobile processor by 19% (up to 40%) while introducing just 2% slowdown.

---
### Biscuit: A Framework for Near-Data Processing of Big Data Workloads.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.23
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551390
* **Key Words**: Programming, Computational modeling, Computers, Bandwidth, Data models, Hardware, Organizations, Big Data, data flow computing, Linux, semiconductor storage, Big Data workloads, Biscuit, near-data processing framework, high-level programming model, data flow, code reuse, Linux OS, high-performance solid-state drive, data filtering, near-data processing, in-storage computing, SSD, 
* **Abstract**: Data-intensive queries are common in business intelligence, data warehousing and analytics applications. Typically, processing a query involves full inspection of large in-storage data sets by CPUs. An intuitive way to speed up such queries is to reduce the volume of data transferred over the storage network to a host system. This can be achieved by filtering out extraneous data within the storage, motivating a form of near-data processing. This work presents Biscuit, a novel near-data processing framework designed for modern solid-state drives. It allows programmers to write a data-intensive application to run on the host system and the storage system in a distributed, yet seamless manner. In order to offer a high-level programming model, Biscuit builds on the concept of data flow. Data processing tasks communicate through typed and data-ordered ports. Biscuit does not distinguish tasks that run on the host system and the storage system. As the result, Biscuit has desirable traits like generality and expressiveness, while promoting code reuse and naturally exposing concurrency. We implement Biscuit on a host system that runs the Linux OS and a high-performance solid-state drive. We demonstrate the effectiveness of our approach and implementation with experimental results. When data filtering is done by hardware in the solid-state drive, the average speed-up obtained for the top five queries of TPC-H is over 15x.

---
### Energy Efficient Architecture for Graph Analytics Accelerators.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.24
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551391
* **Key Words**: Hardware, Instruction sets, Graphics processing units, Data structures, Bandwidth, Convergence, C++ language, computer architecture, data structures, graph grammars, graphics processing units, high level synthesis, microprocessor chips, multiprocessing systems, optimisation, power aware computing, physical-aware logic synthesis, power consumption estimation, graph-parallel applications, RTL generation, cycle-accurate simulator, application-level data structures, SystemC-based template, GPU architectures, multicore CPU, asymmetric convergence, irregular access patterns, iterative vertex-centric graph applications, optimized architecture template, graph analytics applications, computer systems, power efficiency, performance improvement, hardware accelerators, graph analytics accelerators, energy efficient architecture, Hardware accelerators, graph analytics, energy efficient architectures, architectures for emerging applications, 
* **Abstract**: Specialized hardware accelerators can significantly improve the performance and power efficiency of compute systems. In this paper, we focus on hardware accelerators for graph analytics applications and propose a configurable architecture template that is specifically optimized for iterative vertex-centric graph applications with irregular access patterns and asymmetric convergence. The proposed architecture addresses the limitations of the existing multi-core CPU and GPU architectures for these types of applications. The SystemC-based template we provide can be customized easily for different vertex-centric applications by inserting application-level data structures and functions. After that, a cycle-accurate simulator and RTL can be generated to model the target hardware accelerators. In our experiments, we study several graph-parallel applications, and show that the hardware accelerators generated by our template can outperform a 24 core high end server CPU system by up to 3x in terms of performance. We also estimate the area requirement and power consumption of these hardware accelerators through physical-aware logic synthesis, and show up to 65x better power consumption with significantly smaller area.

---
### ASIC Clouds: Specializing the Datacenter.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.25
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551392
* **Key Words**: Online banking, Cloud computing, Cryptography, Servers, Graphics processing units, Neural networks, Random access memory, cloud computing, computer centres, data mining, DRAM chips, field programmable gate arrays, graphics processing units, datacenter, GPU-based clouds, FPGA-based clouds, computing-intensive workloads, total cost of ownership, TCO, Bitcoin mining, YouTube-style video transcoding, Pareto-optimal ASIC cloud servers, place-and-routed circuits, computational fluid dynamic simulations, DRAM subsystem, motherboard, power delivery system, cooling system, operating voltage, case design, ASIC Cloud, Specialization, datacenter, TCO, Dark Silicon, Litecoin, Bitcoin, NRE, Near-threshold, 
* **Abstract**: GPU and FPGA-based clouds have already demonstrated the promise of accelerating computing-intensive workloads with greatly improved power and performance. In this paper, we examine the design of ASIC Clouds, which are purpose-built datacenters comprised of large arrays of ASIC accelerators, whose purpose is to optimize the total cost of ownership (TCO) of large, high-volume chronic computations, which are becoming increasingly common as more and more services are built around the Cloud model. On the surface, the creation of ASIC clouds may seem highlyimprobable due to high NREs and the inflexibility of ASICs. Surprisingly, however, large-scale ASIC Clouds have already been deployed by a large number of commercial entities, to implement the distributed Bitcoin cryptocurrency system. We begin with a case study of Bitcoin mining ASIC Clouds, which are perhaps the largest ASIC Clouds to date. From there, we design three more ASIC Clouds, including a YouTube-style video transcoding ASIC Cloud, a Litecoin ASIC Cloud, and a Convolutional Neural Network ASIC Cloud and show 2-3 orders of magnitude better TCO versus CPU and GPU. Among our contributions, we present a methodology that given an accelerator design, derives Pareto-optimal ASIC Cloud Servers, by extracting data from place-and-routed circuits and computational fluid dynamic simulations, and then employing clever but brute-force search to find the best jointly-optimized ASIC, DRAM subsystem, motherboard, power delivery system, cooling system, operating voltage, and case design. Moreover, we show how data center parameters determine which of the many Pareto-optimal points is TCO-optimal. Finally we examine when it makes sense to build an ASIC Cloud, and examine the impact of ASIC NRE.

---
### APRES: Improving Cache Efficiency by Exploiting Load Characteristics on GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.26
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551393
* **Key Words**: Graphics processing units, Prefetching, Computer architecture, Registers, Scheduling, Bandwidth, cache storage, graphics processing units, reconfigurable architectures, scheduling, storage management, APRES, adaptive prefetching and scheduling, memory latency, GPGPU, memory system pressure, massive thread-level parallelism, GPU cache efficiency, static load instruction, memory address, group prioritization, cache trashing, miss status holding register, warp scheduling, GPGPU, Warp Scheduling, Data Prefetching, 
* **Abstract**: Long memory latency and limited throughput become performance bottlenecks of GPGPU applications. The latency takes hundreds of cycles which is difficult to be hidden by simply interleaving tens of warp execution. While cache hierarchy helps to reduce memory system pressure, massive Thread-Level Parallelism (TLP) often causes excessive cache contention. This paper proposes Adaptive PREfetching and Scheduling (APRES) to improve GPU cache efficiency. APRES relies on the following observations. First, certain static load instructions tend to generate memory addresses having very high locality. Second, although loads have no locality, the access addresses still can show highly strided access pattern. Third, the locality behavior tends to be consistent regardless of warp ID. APRES schedules warps so that as many cache hits generated as possible before any cache misses generated. This is to minimize cache thrashing when many warps are contending for a cache line. However, to realize this operation, it is required to predict which warp will hit the cache in the near future. Without directly predicting future cache hit/miss for each warp, APRES creates a group of warps that will execute the same load instruction in the near future. Based on the third observation, we expect the locality behavior is consistent over all warps in the group. If the first executed warp in the group hits the cache, then the load is considered as a high locality type, and APRES prioritizes all warps in the group. Group prioritization leads to consecutive cache hits, because the grouped warps are likely to access the same cache line. If the first warp missed the cache, then the load is considered as a strided type, and APRES generates prefetch requests for the other warps in the group. After that, APRES prioritizes prefetch targeted warps so that the demand requests are merged to Miss Status Holding Register (MSHR) or prefetched lines can be accessed. On memory-intensive applications, APRES achieves ...

---
### Transparent Offloading and Mapping (TOM): Enabling Programmer-Transparent Near-Data Processing in GPU Systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.27
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551394
* **Key Words**: Graphics processing units, Bandwidth, Memory management, Mathematical model, Three-dimensional displays, Runtime, Message systems, cost-benefit analysis, graphics processing units, program compilers, storage management, transparent offloading and mapping, programmer-transparent near-data processing, compiler-based technique, logic-layer GPU, cost-benefit analysis, software-hardware cooperative mechanism, minimize off-chip bandwidth consumption, TOM, 
* **Abstract**: Main memory bandwidth is a critical bottleneck for modern GPU systems due to limited off-chip pin bandwidth. 3D-stacked memory architectures provide a promising opportunity to significantly alleviate this bottleneck by directly connecting a logic layer to the DRAM layers with high bandwidth connections. Recent work has shown promising potential performance benefits from an architecture that connects multiple such 3D-stacked memories and offloads bandwidth-intensive computations to a GPU in each of the logic layers. An unsolved key challenge in such a system is how to enable computation offloading and data mapping to multiple 3D-stacked memories without burdening the programmer such that any application can transparently benefit from near-data processing capabilities in the logic layer. Our paper develops two new mechanisms to address this key challenge. First, a compiler-based technique that automatically identifies code to offload to a logic-layer GPU based on a simple cost-benefit analysis. Second, a software/hardware cooperative mechanism that predicts which memory pages will be accessed by offloaded code, and places those pages in the memory stack closest to the offloaded code, to minimize off-chip bandwidth consumption. We call the combination of these two programmer-transparent mechanisms TOM: Transparent Offloading and Mapping. Our extensive evaluations across a variety of modern memory-intensive GPU workloads show that, without requiring any program modification, TOM significantly improves performance (by 30% on average, and up to 76%) compared to a baseline GPU system that cannot offload computation to 3D-stacked memories.

---
### Efficient Synonym Filtering and Scalable Delayed Translation for Hybrid Virtual Caching.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.28
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551395
* **Key Words**: Coherence, Memory management, System-on-chip, Power demand, Energy consumption, Protocols, Operating systems, translation, hybrid virtual cache, synonym, 
* **Abstract**: Conventional translation look-aside buffers (TLBs) are required to complete address translation withshort latencies, as the address translation is on the criticalpath of all memory accesses even for L1 cache hits. Such strictTLB latency restrictions limit the TLB capacity, as the latencyincrease with large TLBs may lower the overall performanceeven with potential TLB miss reductions. Furthermore, TLBsconsume a significant amount of energy as they are accessedfor every instruction fetch and data access. To avoid thelatency restriction and reduce the energy consumption, virtualcaching techniques have been proposed to defer translation toafter L1 cache misses. However, an efficient solution for thesynonym problem has been a critical issue hindering the wideadoption of virtual caching.Based on the virtual caching concept, this study proposes ahybrid virtual memory architecture extending virtual cachingto the entire cache hierarchy, aiming to improve both performanceand energy consumption. The hybrid virtual cachinguses virtual addresses augmented with address space identifiers(ASID) in the cache hierarchy for common non-synonymaddresses. For such non-synonyms, the address translationoccurs only after last-level cache (LLC) misses. For uncommonsynonym addresses, the addresses are translated to physicaladdresses with conventional TLBs before L1 cache accesses. Tosupport such hybrid translation, we propose an efficient synonymdetection mechanism based on Bloom filters which canidentify synonym candidates with few false positives. For largememory applications, delayed translation alone cannot solvethe address translation problem, as fixed-granularity delayedTLBs may not scale with the increasing memory requirements.To mitigate the translation scalability problem, this studyproposes a delayed many segment translation designed for thehybrid virtual caching. The experimental results show that ourapproach effectively lowers accesses to the TLBs, leading tosignificant power savi...

---
### Warped-Slicer: Efficient Intra-SM Slicing through Dynamic Resource Partitioning for GPU Multiprogramming.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.29
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551396
* **Key Words**: Kernel, Graphics processing units, Resource management, Registers, Benchmark testing, Instruction sets, Hardware, decision making, graphics processing units, multiprocessing systems, multiprogramming, multi-threading, resource allocation, Warped-Slicer, streaming multiprocessor, intraSM slicing, dynamic resource partitioning, graphics processing unit, GPU, multiprogramming, thread-level parallelism, resource underutilization, Hyper-Q, Kepler architecture, decision making, GPUs, scheduling, multiprogramming, multi-kernel, resource management, 
* **Abstract**: As technology scales, GPUs are forecasted to incorporate an ever-increasing amount of computing resources to support thread-level parallelism. But even with the best effort, exposing massive thread-level parallelism from a single GPU kernel, particularly from general purpose applications, is going to be a difficult challenge. In some cases, even if there is sufficient thread-level parallelism in a kernel, there may not be enough available memory bandwidth to support such massive concurrent thread execution. Hence, GPU resources may be underutilized as more general purpose applications are ported to execute on GPUs. In this paper, we explore multiprogramming GPUs as a way to resolve the resource underutilization issue. There is a growing hardware support for multiprogramming on GPUs. Hyper-Q has been introduced in the Kepler architecture which enables multiple kernels to be invoked via tens of hardware queue streams. Spatial multitasking has been proposed to partition GPU resources across multiple kernels. But the partitioning is done at the coarse granularity of streaming multiprocessors (SMs) where each kernel is assigned to a subset of SMs. In this paper, we advocate for partitioning a single SM across multiple kernels, which we term as intra-SM slicing. We explore various intra-SM slicing strategies that slice resources within each SM to concurrently run multiple kernels on the SM. Our results show that there is not one intra-SM slicing strategy that derives the best performance for all application pairs. We propose Warped-Slicer, a dynamic intra-SM slicing strategy that uses an analytical method for calculating the SM resource partitioning across different kernels that maximizes performance. The model relies on a set of short online profile runs to determine how each kernel's performance varies as more thread blocks from each kernel are assigned to an SM. The model takes into account the interference effect of shared resource usage across multiple kernels. The m...

---
### EIE: Efficient Inference Engine on Compressed Deep Neural Network.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.30
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551397
* **Key Words**: Random access memory, Neural networks, Acceleration, Hardware, System-on-chip, Computational modeling, Sparse matrices, DRAM chips, matrix multiplication, neural nets, sparse matrices, SRAM chips, EIE, energy efficient inference engine, compressed deep neural network, embedded system, DRAM, DNN, AlexNet, VGGNet, onchip SRAM, sparse matrix-vector multiplication, weight sharing, power dissipation, Deep Learning, Model Compression, Hardware Acceleration, Algorithm-Hardware co-Design, ASIC, 
* **Abstract**: State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving, Exploiting sparsity saves 10x, Weight sharing gives 8x, Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88x104 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.

---
### RedEye: Analog ConvNet Image Sensor Architecture for Continuous Mobile Vision.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.31
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551398
* **Key Words**: Complexity theory, Neurons, Image sensors, Mobile communication, Convolution, Arrays, Analog circuits, computer vision, image sensors, neural nets, quantisation (signal), computation-based system energy reduction, cloudlet-based system energy reduction, sensor energy reduction, programmable mechanisms, algorithmic cyclic reuse, physical design reuse, column-parallel design, analog domain, convolutional neural network, vision processing, quantization processing, intensive computation, data traffic, analog readout circuitry, vision feature processing, image frame capturing, continuous mobile vision, ConvNet image sensor architecture, RedEye, continuous mobile vision, programmable analog computing, computer vision, pre-quantization processing, 
* **Abstract**: Continuous mobile vision is limited by the inability to efficiently capture image frames and process vision features. This is largely due to the energy burden of analog readout circuitry, data traffic, and intensive computation. To promote efficiency, we shift early vision processing into the analog domain. This results in RedEye, an analog convolutional image sensor that performs layers of a convolutional neural network in the analog domain before quantization. We design RedEye to mitigate analog design complexity, using a modular column-parallel design to promote physical design reuse and algorithmic cyclic reuse. RedEye uses programmable mechanisms to admit noise for tunable energy reduction. Compared to conventional systems, RedEye reports an 85% reduction in sensor energy, 73% reduction in cloudlet-based system energy, and a 45% reduction in computation-based system energy.

---
### Minerva: Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.32
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551399
* **Key Words**: Optimization, Random access memory, Hardware, Integrated circuit modeling, Circuit faults, Space exploration, Libraries, neural nets, Minerva, deep neural network accelerators, deep neural networks, classification tasks, specialized hardware, magnitude improvement, general-purpose hardware, automated codesign, DNN hardware accelerators, fixed-point accelerator baseline, heterogeneous datatype optimization, inline predication, small activity values, active hardware fault detection, domain-aware error mitigation, SRAM voltages, DNN model accuracy, ultra-low power DNN accelerators, power-constrained IoT, mobile devices, 
* **Abstract**: The continued success of Deep Neural Networks (DNNs) in classification tasks has sparked a trend of accelerating their execution with specialized hardware. While published designs easily give an order of magnitude improvement over general-purpose hardware, few look beyond an initial implementation. This paper presents Minerva, a highly automated co-design approach across the algorithm, architecture, and circuit levels to optimize DNN hardware accelerators. Compared to an established fixed-point accelerator baseline, we show that fine-grained, heterogeneous datatype optimization reduces power by 1.5×; aggressive, inline predication and pruning of small activity values further reduces power by 2.0×; and active hardware fault detection coupled with domain-aware error mitigation eliminates an additional 2.7× through lowering SRAM voltages. Across five datasets, these optimizations provide a collective average of 8.1× power reduction over an accelerator baseline without compromising DNN model accuracy. Minerva enables highly accurate, ultra-low power DNN accelerators (in the range of tens of milliwatts), making it feasible to deploy DNNs in power-constrained IoT and mobile devices.

---
### Opportunistic Competition Overhead Reduction for Expediting Critical Section in NoC Based CMPs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.33
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551400
* **Key Words**: Nickel, Instruction sets, Spinning, Benchmark testing, Hardware, Operating systems, Queueing analysis, multiprocessing systems, network-on-chip, operating systems (computers), NoC based CMPs, multithreaded shared variable performance, serialized critical section execution, serialized competition overhead, operating systems, queue spinlock, low-overhead spinning phase, high-overhead sleeping phase, software-hardware cooperative mechanism, remaining times of retry, RTR, high-overhead sleep mode, opportunistic competition overhead reduction technique, cycle-accurate full-system simulations, GEM5, PARSEC, SPEC OMP2012 benchmarks, region-of-interest, Critical Section, CMP, NoC, OS, 
* **Abstract**: With the degree of parallelism increasing, performance of multi-threaded shared variable applications is not only limited by serialized critical section execution, but also by the serialized competition overhead for threads to get access to critical section. As the number of concurrent threads grows, such competition overhead may exceed the time spent in critical section itself, and become the dominating factor limiting the performance of parallel applications. In modern operating systems, queue spinlock, which comprises a low-overhead spinning phase and a high-overhead sleeping phase, is often used to lock critical sections. In the paper, we show that this advanced locking solution may create very high competition overhead for multithreaded applications executing in NoC-based CMPs. Then we propose a software-hardware cooperative mechanism that can opportunistically maximize the chance that a thread wins the critical section access in the low-overhead spinning phase, thereby reducing the competition overhead. At the OS primitives level, we monitor the remaining times of retry (RTR) in a thread's spinning phase, which reflects in how long the thread must enter into the high-overhead sleep mode. At the hardware level, we integrate the RTR information into the packets of locking requests, and let the NoC prioritize locking request packets according to the RTR information. The principle is that the smaller RTR a locking request packet carries, the higher priority it gets and thus quicker delivery. We evaluate our opportunistic competition overhead reduction technique with cycle-accurate full-system simulations in GEM5 using PARSEC (11 programs) and SPEC OMP2012 (14 programs) benchmarks. Compared to the original queue spinlock implementation, experimental results show that our method can effectively increase the opportunity of threads entering the critical section in low-overhead spinning phase, reducing the competition overhead averagely by 39.9% (maximally by 61.8%) an...

---
### Short-Circuit Dispatch: Accelerating Virtual Machine Interpreters on Embedded Processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.34
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551401
* **Key Words**: Program processors, Hardware, Pipelines, Virtual machining, Switches, Libraries, authoring languages, embedded systems, parallel processing, pipeline processing, program compilers, virtual machines, short-circuit dispatch, virtual machine interpreters, embedded processors, VM, resource-constrained embedded platforms, scripting languages, compilers, bytecode dispatch loop, shallow pipelines, low IPC, software-created bytecode jump table, branch target buffer, BTB, RISC-V embedded processor, Microarchitecture, Pipeline, Scripting Languages, Bytecodes, Interpreters, Dispatch, JavaScript, Lua, 
* **Abstract**: Interpreters are widely used to implement high-level language virtual machines (VMs), especially on resource-constrained embedded platforms. Many scripting languages employ interpreter-based VMs for their advantages over native code compilers, such as portability, smaller resource footprint, and compact codes. For efficient interpretation a script (program) is first compiled into an intermediate representation, or bytecodes. The canonical interpreter then runs an infinite loop that fetches, decodes, and executes one bytecode at a time. This bytecode dispatch loop is a well-known source of inefficiency, typically featuring a large jump table with a hard-to-predict indirect jump. Most existing techniques to optimize this loop focus on reducing the misprediction rate of this indirect jump in both hardware and software. However, these techniques are much less effective on embedded processors with shallow pipelines and low IPCs. Instead, we tackle another source of inefficiency more prominent on embedded platforms - redundant computation in the dispatch loop. To this end, we propose Short-Circuit Dispatch (SCD), a low cost architectural extension that enables fast, hardware-based bytecode dispatch with fewer instructions. The key idea of SCD is to overlay the software-created bytecode jump table on a branch target buffer (BTB). Once a bytecode is fetched, the BTB is looked up using the bytecode, instead of PC, as key. If it hits, the interpreter directly jumps to the target address retrieved from the BTB, otherwise, it goes through the original dispatch path. This effectively eliminates redundant computation in the dispatcher code for decode, bound check, and target address calculation, thus significantly reducing total instruction count. Our simulation results demonstrate that SCD achieves geomean speedups of 19.9% and 14.1% for two production-grade script interpreters for Lua and JavaScript, respectively. Moreover, our fully synthesizable RTL design based on a RISC-V e...

---
### ARM Virtualization: Performance and Architectural Implications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.35
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551402
* **Key Words**: Virtual machine monitors, Hardware, Virtualization, Computer architecture, Kernel, Servers, computer architecture, microcontrollers, multiprocessing systems, virtualisation, ARM servers, ARM virtualization performance, multicore measurements, x86 hypervisors design, KVM, Xen, hypervisor software design, VM-to-hypervisor transition mechanism, computer architecture, hypervisors, operating systems, virtualization, multi-core, performance, ARM, x86, 
* **Abstract**: ARM servers are becoming increasingly common, making server technologies such as virtualization for ARM of growing importance. We present the first study of ARM virtualization performance on server hardware, including multi-core measurements of two popular ARM and x86 hypervisors, KVM and Xen. We show how ARM hardware support for virtualization can enable much faster transitions between VMs and the hypervisor, a key hypervisor operation. However, current hypervisor designs, including both Type 1 hypervisors such as Xen and Type 2 hypervisors such as KVM, are not able to leverage this performance benefit for real application workloads. We discuss the reasons why and show that other factors related to hypervisor software design and implementation have a larger role in overall performance. Based on our measurements, we discuss changes to ARM's hardware virtualization support that can potentially bridge the gap to bring its faster VM-to-hypervisor transition mechanism to modern Type 2 hypervisors running real applications. These changes have been incorporated into the latest ARM architecture.

---
### Base-Victim Compression: An Opportunistic Cache Compression Architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.36
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551403
* **Key Words**: Arrays, Performance gain, Complexity theory, Proposals, Prefetching, Energy efficiency, cache storage, memory architecture, multi-threading, performance evaluation, Base-Victim compression, opportunistic cache compression architecture, memory wall, cache management policies, cache capacity, conflict misses, capacity miss reduction, cache compression implementation complexity, cache access latency, cache power, performance gains, suboptimal replacement policies, cache design, performance improvement, advanced cache replacement policies, cache-sensitive applications, single-threaded workloads, four-thread multiprogram workload mixes, cache compression, cache replacement policies, 
* **Abstract**: The memory wall has motivated many enhancements to cache management policies aimed at reducing misses. Cache compression has been proposed to increase effective cache capacity, which potentially reduces capacity and conflict misses. However, complexity in cache compression implementations could increase cache power and access latency. On the other hand, advanced cache replacement mechanisms use heuristics to reduce misses, leading to significant performance gains. Both cache compression and replacement policies should collaborate to improve performance. In this paper, we demonstrate that cache compression and replacement policies can interact negatively. In many workloads, performance gains from replacement policies are lost due to the need to alter the replacement policy to accommodate compression. This leads to sub-optimal replacement policies that could lose performance compared to an uncompressed cache. We introduce a novel, opportunistic cache compression mechanism, Base-Victim, based on an efficient cache design. Our compression architecture improves performance on top of advanced cache replacement policies, and guarantees a hit rate at least as high as that of an uncompressed cache. For cache-sensitive applications, Base-Victim achieves an average 7.3% performance gain for single-threaded workloads, and 8.7% gain for four-thread multi-program workload mixes.

---
### Bit-Plane Compression: Transforming Data for Better Compression in Many-Core Architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.37
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551404
* **Key Words**: Arrays, Bandwidth, Dictionaries, Benchmark testing, Encoding, Random access memory, computer architecture, multiprocessing systems, storage management, bit-plane compression, many-core architecture, lightweight compression algorithm, memory bandwidth, homogeneously-typed memory block, smart data transformation, data compressibility, data compression, main memory, DRAM, 
* **Abstract**: As key applications become more data-intensive and the computational throughput of processors increases, the amount of data to be transferred in modern memory subsystems grows. Increasing physical bandwidth to keep up with the demand growth is challenging, however, due to strict area and energy limitations. This paper presents a novel and lightweight compression algorithm, Bit-Plane Compression (BPC), to increase the effective memory bandwidth. BPC aims at homogeneously-typed memory blocks, which are prevalent in many-core architectures, and applies a smart data transformation to both improve the inherent data compressibility and to reduce the complexity of compression hardware. We demonstrate that BPC provides superior compression ratios of 4.1:1 for integer benchmarks and reduces memory bandwidth requirements significantly.

---
### XED: Exposing On-Die Error Detection Information for Strong Memory Reliability.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.38
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551405
* **Key Words**: Error correction codes, Reliability, DRAM chips, Runtime, Standards, Memory management, circuit reliability, DRAM chips, storage management chips, XED, On-Die error detection information, strong memory reliability, large-granularity memory failures, DRAM scales, system reliability, DRAM chips, memory vendors, On-Die ECC, memory controller, reliability memory systems, 9-chip ECC-DIMM, SECDED, 8-chip nonECC DIMM, exposed On-Die error detection, memory standards, faulty chip, RAID-3, Chipkill systems, Double-Chipkill level reliability, On-Die ECC, Chipkill, Double-Chipkill, RAID-3, 
* **Abstract**: Large-granularity memory failures continue to be a critical impediment to system reliability. To make matters worse, as DRAM scales to smaller nodes, the frequency of unreliable bits in DRAM chips continues to increase. To mitigate such scaling-related failures, memory vendors are planning to equip existing DRAM chips with On-Die ECC. For maintaining compatibility with memory standards, On-Die ECC is kept invisible from the memory controller. This paper explores how to design high reliability memory systems in presence of On-Die ECC. We show that if On-Die ECC is not exposed to the memory system, having a 9-chip ECC-DIMM (implementing SECDED) provides almost no reliability benefits compared to an 8-chip non-ECC DIMM. We also show that if the error detection of On-Die ECC can be exposed to the memory controller, then Chipkill-level reliability can be achieved even with a 9-chip ECC-DIMM. To this end, we propose eXposed On-Die Error Detection (XED), which exposes the On-Die error detection information without requiring changes to the memory standards or consuming bandwidth overheads. When the On-Die ECC detects an error, XED transmits a pre-defined “catch-word” instead of the corrected data value. On receiving the catch-word, the memory controller uses the parity stored in the 9-chip of the ECC-DIMM to correct the faulty chip (similar to RAID-3). Our studies show that XED provides Chipkill-level reliability (172× higher than SECDED), while incurring negligible overheads, with a 21% lower execution time than Chipkill. We also show that XED can enable Chipkill systems to provide Double-Chipkill level reliability while avoiding the associated storage, performance, and power overheads.

---
### Production-Run Software Failure Diagnosis via Adaptive Communication Tracking.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.39
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551406
* **Key Words**: Computer bugs, Neurons, Hardware, Software, Concurrent computing, Biological neural networks, Training, neural nets, program debugging, program diagnostics, software fault tolerance, production-run software failure diagnosis, adaptive communication tracking, production-run time, bug detection algorithms, ACT, machine intelligence, neural hardware, data communication invariants, sequential bugs, concurrency bugs, multiprocessor system, three stage pipeline, hidden layer neural networks, open source programs, Concurrency bugs, Sequential bugs, Failures, Dependence, Neural hardware, 
* **Abstract**: Software failure diagnosis techniques work either by sampling some events at production-run time or by using some bug detection algorithms. Some of the techniques require the failure to be reproduced multiple times. The ones that do not require such, are not adaptive enough when the execution platform, environment or code changes. We propose ACT, a diagnosis technique for production-run failures, that uses the machine intelligence of neural hardware. ACT learns some invariants (e.g., data communication invariants) on-the-fly using the neural hardware and records any potential violation of them. Since ACT can learn invariants on-the-fly, it can adapt to any change in execution setting or code. Since it records only the potentially violated invariants, the postprocessing phase can pinpoint the root cause fairly accurately without requiring to observe the failure again. ACT works seamlessly for many sequential and concurrency bugs. The paper provides a detailed design and implementation of ACT in a typical multiprocessor system. It uses a three stage pipeline for partially configurable one hidden layer neural networks. We have evaluated ACT on a variety of programs from popular benchmarks as well as open source programs. ACT diagnoses failures caused by 16 bugs from these programs with accurate ranking. Compared to existing learning and sampling based approaches, ACT has better diagnostic ability. For the default configuration, ACT has an average execution overhead of 8.2%.

---
### Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.40
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551407
* **Key Words**: Throughput, Random access memory, Radio frequency, Parallel processing, Shape, Arrays, computer architecture, convolution, data flow computing, neural nets, power aware computing, Eyeriss, spatial architecture, energy-efficient dataflow, deep convolutional neural networks, deep CNNs, computational complexity, high-dimensional convolutions, parallel processing, energy-efficient CNN processing, row-stationary dataflow, RS dataflow, data movement energy consumption minimization, local data reuse, feature map pixels, partial sum accumulations, processing engine local storage, PE local storage, direct interPE communication, spatial parallelism, AlexNet CNN configurations, Spatial Architecture, Convolutional Neural Networks, Dataflow, Energy Efficiency, 
* **Abstract**: Deep convolutional neural networks (CNNs) are widely used in modern AI systems for their superior accuracy but at the cost of high computational complexity. The complexity comes from the need to simultaneously process hundreds of filters and channels in the high-dimensional convolutions, which involve a significant amount of data movement. Although highly-parallel compute paradigms, such as SIMD/SIMT, effectively address the computation requirement to achieve high throughput, energy consumption still remains high as data movement can be more expensive than computation. Accordingly, finding a dataflow that supports parallel processing with minimal data movement cost is crucial to achieving energy-efficient CNN processing without compromising accuracy. In this paper, we present a novel dataflow, called row-stationary (RS), that minimizes data movement energy consumption on a spatial architecture. This is realized by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations. Unlike dataflows used in existing designs, which only reduce certain types of data movement, the proposed RS dataflow can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism. To evaluate the energy efficiency of the different dataflows, we propose an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints. Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4× to 2.5×) and fully-connected layers (at least 1.3× for batch size larger than 16). The RS dataflow has also been demonstrated on a fabricated chip, which verifies our energy analysis.

---
### Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.41
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551408
* **Key Words**: Neurons, Random access memory, Computer architecture, Three-dimensional displays, Biological neural networks, Artificial neural networks, logic circuits, memory architecture, neural nets, storage management chips, Neurocube, programmable digital neuromorphic architecture, high-density 3D memory, logic tier, neural computing, memory centric computing, HMC, convolutional neural network, Neural nets, Neurocomputers, Neuromorphic computing, 
* **Abstract**: This paper presents a programmable and scalable digital neuromorphic architecture based on 3D high-density memory integrated with logic tier for efficient neural computing. The proposed architecture consists of clusters of processing engines, connected by 2D mesh network as a processing tier, which is integrated in 3D with multiple tiers of DRAM. The PE clusters access multiple memory channels (vaults) in parallel. The operating principle, referred to as the memory centric computing, embeds specialized state-machines within the vault controllers of HMC to drive data into the PE clusters. The paper presents the basic architecture of the Neurocube and an analysis of the logic tier synthesized in 28nm and 15nm process technologies. The performance of the Neurocube is evaluated and illustrated through the mapping of a Convolutional Neural Network and estimating the subsequent power and performance for both training and inference.

---
### Cambricon: An Instruction Set Architecture for Neural Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.42
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551409
* **Key Words**: Artificial neural networks, Registers, System-on-chip, Computer architecture, Data transfer, Ground penetrating radar, Libraries, instruction sets, neural net architecture, power aware computing, Cambricon, neural networks, general-purpose processors, application-specific hardware accelerators, energy-efficiency, NN high-level functional blocks, domain-specific instruction set architecture, domain-specific ISA, load-store architecture, scalar instructions, vector instructions, matrix instructions, logical instructions, data transfer instructions, control instructions, TSMC 65nm technology, 
* **Abstract**: Neural Networks (NN) are a family of models for a broad range of emerging machine learning and pattern recondition applications. NN techniques are conventionally executed on general-purpose processors (such as CPU and GPGPU), which are usually not energy-efficient since they invest excessive hardware resources to flexibly support various workloads. Consequently, application-specific hardware accelerators for neural networks have been proposed recently to improve the energy-efficiency. However, such accelerators were designed for a small set of NN techniques sharing similar computational patterns, and they adopt complex and informative instructions (control signals) directly corresponding to high-level functional blocks of an NN (such as layers), or even an NN as a whole. Although straightforward and easy-to-implement for a limited set of similar NN techniques, the lack of agility in the instruction set prevents such accelerator designs from supporting a variety of different NN techniques with sufficient flexibility and efficiency. In this paper, we propose a novel domain-specific Instruction Set Architecture (ISA) for NN accelerators, called Cambricon, which is a load-store architecture that integrates scalar, vector, matrix, logical, data transfer, and control instructions, based on a comprehensive analysis of existing NN techniques. Our evaluation over a total of ten representative yet distinct NN techniques have demonstrated that Cambricon exhibits strong descriptive capacity over a broad range of NN techniques, and provides higher code density than general-purpose ISAs such as ×86, MIPS, and GPGPU. Compared to the latest state-of-the-art NN accelerator design DaDianNao [5] (which can only accommodate 3 types of NN techniques), our Cambricon-based accelerator prototype implemented in TSMC 65nm technology incurs only negligible latency/power/area overheads, with a versatile coverage of 10 different NN benchmarks.

---
### Decoupling Loads for Nano-Instruction Set Computers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.43
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551410
* **Key Words**: Registers, Schedules, Microarchitecture, Hardware, Dynamic scheduling, Reduced instruction set computing, Semantics, instruction sets, program compilers, reduced instruction set computing, scheduling, RISC, may-alias stores, decoupled load data access, static instruction schedules, compilers, load instruction, register write operations, ISA extension, nanoinstruction set computers, 
* **Abstract**: We propose an ISA extension that decouples the data access and register write operations in a load instruction. We describe system and hardware support for decoupled loads. Furthermore, we show how compilers can generate better static instruction schedules by hoisting a decoupled load's data access above may-alias stores and branches. We find that decoupled loads improve performance with geometric mean speedups of 8.4%.

---
### Future Vector Microprocessor Extensions for Data Aggregations.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.44
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551411
* **Key Words**: Registers, Parallel processing, Microprocessors, Support vector machines, Instruction sets, Data models, data aggregation, microprocessor chips, parallel processing, vectors, future vector microprocessor extensions, data aggregations, annual data generation, Dennard scaling, single instruction-multiple data, SIMD instruction sets, data-level parallelism, DLP, simd, vector, aggregation, dlp, 
* **Abstract**: As the rate of annual data generation grows exponentially, there is a demand to aggregate and summarise vast amounts of information quickly. In the past, frequency scaling was relied upon to push application throughput. Today, Dennard scaling has ceased and further performance must come from exploiting parallelism. Single instruction-multiple data (SIMD) instruction sets offer a highly efficient and scalable way of exploiting data-level parallelism (DLP). While microprocessors originally offered very simple SIMD support targeted at multimedia applications, these extensions have been growing both in width and functionality. Observing this trend, we use a simulation framework to model future SIMD support and then propose and evaluate five different ways of vectorising data aggregation. We find that although data aggregation is abundant in DLP, it is often too irregular to be expressed efficiently using typical SIMD instructions. Based on this observation, we propose a set of novel algorithms and SIMD instructions to better capture this irregular DLP. Furthermore, we discover that the best algorithm is highly dependent on the characteristics of the input. Our proposed solution can dynamically choose the optimal algorithm in the majority of cases and achieves speedups between 2.7x and 7.6x over a scalar baseline.

---
### Efficiently Scaling Out-of-Order Cores for Simultaneous Multithreading.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.45
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551412
* **Key Words**: Out of order, Microarchitecture, Registers, Hardware, Throughput, Schedules, Delays, multiprocessing systems, multi-threading, processor scheduling, simultaneous multithreading, SMT out-of-order cores, structural out-of-order core resources, thread interleaving, false dependences, in-sequence instructions, reordered instructions, instruction window, in-flight instructions, hybrid out-of-order/in-order microarchitecture, in-order scheduling, FIFO issue queue, shelf, instruction-by-instruction basis, reorder buffer, physical registers, load-store queues, 4-threaded cores, energy-delay product, microarchitecture, in-sequence, reorder, 
* **Abstract**: Simultaneous multithreading (SMT) out-of-order cores waste a significant portion of structural out-of-order core resources on instructions that do not need them. These resources eliminate false ordering dependences. However, because thread interleaving spreads dependent instructions, nearly half of instructions dynamically issue in program order after all false dependences have resolved. These in-sequence instructions interleave with other reordered instructions at a fine granularity within the instruction window. We develop a technique to efficiently scale in-flight instructions through a hybrid out-of-order/in-order microarchitecture, which can dispatch instructions to efficient in-order scheduling mechanisms -- using a FIFO issue queue called the shelf -- on an instruction-by-instruction basis. Instructions dispatched to the shelf do not allocate out-of-order core resources in the reorder buffer, issue queue, physical registers, or load-store queues. We measure opportunity for such hybrid microarchitectures and design and evaluate a practical dispatch mechanism targeted at 4-threaded cores. Adding a shelf to a baseline 4-thread system with 64- entry ROB improves normalized system throughput by 11.5% (up to 19.2% at best) and energy-delay product by 10.9% (up to 17.5% at best).

---
### Accelerating Dependent Cache Misses with an Enhanced Memory Controller.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.46
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551413
* **Key Words**: Prefetching, Electromagnetic compatibility, System-on-chip, Random access memory, Benchmark testing, Correlation, Delays, cache storage, DRAM chips, multiprocessing systems, dependent cache misses, on-chip contention, memory access latency, multicore processors, latency-critical memory operations, cache miss latency, enhanced memory controller, DRAM, memory requests, EMC, memory intensive quad-core workloads, system performance, energy consumption, global history buffer prefetcher, 
* **Abstract**: On-chip contention increases memory access latency for multi-core processors. We identify that this additional latency has a substantial effect on performance for an important class of latency-critical memory operations: those that result in a cache miss and are dependent on data from a prior cache miss. We observe that the number of instructions between the first cache miss and its dependent cache miss is usually small. To minimize dependent cache miss latency, we propose adding just enough functionality to dynamically identify these instructions at the core and migrate them to the memory controller for execution as soon as source data arrives from DRAM. This migration allows memory requests issued by our new Enhanced Memory Controller (EMC) to experience a 20% lower latency than if issued by the core. On a set of memory intensive quad-core workloads, the EMC results in a 13% improvement in system performance and a 5% reduction in energy consumption over a system with a Global History Buffer prefetcher, the highest performing prefetcher in our evaluation.

---
### Treadmill: Attributing the Source of Tail Latency through Precise Load Testing and Statistical Inference.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.47
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551414
* **Key Words**: Servers, Hardware, Testing, Histograms, Web and internet services, Production, computer centres, computer network performance evaluation, Internet, regression analysis, Treadmill, precise load testing, statistical inference, request tail latency management, large-scale Internet services, data centers, service operators, production hardware configurations, software hardware configurations, performance evaluation, performance factor attribution, server workloads, server load tester, modular load tester platform, statistically-sound performance evaluation, quantile regression, server systems, server hardware features, Facebook production workloads, counter-intuitive performance behaviors, tail latency, load testing, data center, 
* **Abstract**: Managing tail latency of requests has become one of the primary challenges for large-scale Internet services. Data centers are quickly evolving and service operators frequently desire to make changes to the deployed software and production hardware configurations. Such changes demand a confident understanding of the impact on one's service, in particular its effect on tail latency (e.g., 95th-or 99th-percentile response latency of the service). Evaluating the impact on the tail is challenging because of its inherent variability. Existing tools and methodologies for measuring these effects suffer from a number of deficiencies including poor load tester design, statistically inaccurate aggregation, and improper attribution of effects. As shown in the paper, these pitfalls can often result in misleading conclusions. In this paper, we develop a methodology for statistically rigorous performance evaluation and performance factor attribution for server workloads. First, we find that careful design of the server load tester can ensure high quality performance evaluation, and empirically demonstrate the inaccuracy of load testers in previous work. Learning from the design flaws in prior work, we design and develop a modular load tester platform, Treadmill, that overcomes pitfalls of existing tools. Next, utilizing Treadmill, we construct measurement and analysis procedures that can properly attribute performance factors. We rely on statistically-sound performance evaluation and quantile regression, extending it to accommodate the idiosyncrasies of server systems. Finally, we use our augmented methodology to evaluate the impact of common server hardware features with Facebook production workloads on production hardware. We decompose the effects of these features on request tail latency and demonstrate that our evaluation methodology provides superior results, particularly in capturing complicated and counter-intuitive performance behaviors. By tuning the hardware features as...

---
### Dynamo: Facebook's Data Center-Wide Power Management System.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.48
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551415
* **Key Words**: Servers, Generators, Facebook, Power measurement, Monitoring, Power demand, Production, computer centres, decision making, power aware computing, resource allocation, social networking (online), Dynamo, Facebook, data center power management system, data center power delivery hierarchy overloading circuit breaker tripping, data center power utilization, decision making, data center, power, management, 
* **Abstract**: Data center power is a scarce resource that often goes underutilized due to conservative planning. This is because the penalty for overloading the data center power delivery hierarchy and tripping a circuit breaker is very high, potentially causing long service outages. Recently, dynamic server power capping, which limits the amount of power consumed by a server, has been proposed and studied as a way to reduce this penalty, enabling more aggressive utilization of provisioned data center power. However, no real at-scale solution for data center-wide power monitoring and control has been presented in the literature. In this paper, we describe Dynamo -- a data center-wide power management system that monitors the entire power hierarchy and makes coordinated control decisions to safely and efficiently use provisioned data center power. Dynamo has been developed and deployed across all of Facebook's data centers for the past three years. Our key insight is that in real-world data centers, different power and performance constraints at different levels in the power hierarchy necessitate coordinated data center-wide power management. We make three main contributions. First, to understand the design space of Dynamo, we provide a characterization of power variation in data centers running a diverse set of modern workloads. This characterization uses fine-grained power samples from tens of thousands of servers and spanning a period of over six months. Second, we present the detailed design of Dynamo. Our design addresses several key issues not addressed by previous simulation-based studies. Third, the proposed techniques and design have been deployed and evaluated in large scale data centers serving billions of users. We present production results showing that Dynamo has prevented 18 potential power outages in the past 6 months due to unexpected power surges, that Dynamo enables optimizations leading to a 13% performance boost for a production Hadoop cluster and a nearly 40%...

---
### Peak Efficiency Aware Scheduling for Highly Energy Proportional Servers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.49
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551416
* **Key Words**: Servers, Processor scheduling, Dynamic scheduling, Dynamic range, Sensors, Electric breakdown, Current measurement, computer centres, file servers, power aware computing, scheduling, peak efficiency aware scheduling, PEAS, better-than-ideal energy proportionality, data center sever, data center scheduling, servers, energy efficiency, scheduling, 
* **Abstract**: Energy proportionality of data center severs have improved drastically over the past decade to the point where near ideal energy proportional servers are now common. These highly energy proportional servers exhibit the unique property where peak efficiency no longer coincides with peak utilization. In this paper, we explore the implications of this property on data center scheduling. We identified that current state of the art data center schedulers does not efficiently leverage these properties, leading to inefficient scheduling decisions. We propose Peak Efficiency Aware Scheduling (PEAS) which can achieve better-than-ideal energy proportionality at the data center level. We demonstrate that PEAS can reduce average power by 25.5% with 3.0% improvement to TCO compared to state-of-the-art scheduling policies.

---
### Power Attack Defense: Securing Battery-Backed Data Centers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.50
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551417
* **Key Words**: Batteries, Servers, Uninterruptible power systems, Security, Software, Power system faults, computer centres, emergency power supply, energy storage, power aware computing, security of data, power attack defense, battery-backed data center, battery systems, secure energy backup, cyber criminals, backup energy resources, distributed energy storage architecture, power-constrained system crash, Power Virus, benign loads, energy management patch, lightweight software, Google cluster traces, data center, battery, power attack, defense, 
* **Abstract**: Battery systems are crucial components for mission-critical data centers. Without secure energy backup, existing under-provisioned data centers are largely unguarded targets for cyber criminals. Particularly for today's scale-out servers, power oversubscription unavoidably taxes a data center's backup energy resources, leaving very little room for dealing with emergency. Besides, the emerging trend towards deploying distributed energy storage architecture causes the associated energy backup of each rack to shrink, making servers vulnerable to power anomalies. As a result, an attacker can generate power peaks to easily crash or disrupt a power-constrained system. This study aims at securing data centers from malicious loads that seek to drain their precious energy storage and overload server racks without prior detection. We term such load as Power Virus (PV) and demonstrate its basic two-phase attacking model and characterize its behaviors on real systems. The PV can learn the victim rack's battery characteristics by disguising as benign loads. Once gaining enough information, the PV can be mutated to generate hidden power spikes that have a high chance to overload the system. To defend against PV, we propose power attack defense (PAD), a novel energy management patch built on lightweight software and hardware mechanisms. PAD not only increases the attacking cost considerably by hiding vulnerable racks from visible spikes, it also strengthens the last line of defense against hidden spikes. Using Google cluster traces we show that PAD can effectively raise the bar of a successful power attack: compared to prior arts, it increases the data center survival time by 1.6~11X and provides better performance guarantee. It enables modern data centers to safely exploit the benefits that power oversubscription may provide, with the slightest cost overhead.

---
### DRAF: A Low-Power DRAM-Based Reconfigurable Acceleration Fabric.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.51
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551418
* **Key Words**: Field programmable gate arrays, Random access memory, Table lookup, Fabrics, Servers, Power demand, Switches, computer centres, DRAM chips, field programmable gate arrays, DRAF, low-power DRAM-based reconfigurable acceleration fabric, application-specific accelerators, energy efficiency, FPGA lookup tables, power overheads, FPGA devices, power constraints, datacenter servers, diverse accelerators, datacenter applications, bit-level reconfigurable logic, DRAM subarrays, dense lookup tables, DRAM operations, bitline precharge, charge restoration, reconfigurable routing, DRAM latency, configuration contexts, Xeon core, datacenter tasks, interactive services, speech recognition, DRAM, reconfigurable logic, FPGA, low-power, 
* **Abstract**: FPGAs are a popular target for application-specific accelerators because they lead to a good balance between flexibility and energy efficiency. However, FPGA lookup tables introduce significant area and power overheads, making it difficult to use FPGA devices in environments with tight cost and power constraints. This is the case for datacenter servers, where a modestly-sized FPGA cannot accommodate the large number of diverse accelerators that datacenter applications need. This paper introduces DRAF, an architecture for bit-level reconfigurable logic that uses DRAM subarrays to implement dense lookup tables. DRAF overlaps DRAM operations like bitline precharge and charge restoration with routing within the reconfigurable routing fabric to minimize the impact of DRAM latency. It also supports multiple configuration contexts that can be used to quickly switch between different accelerators with minimal latency. Overall, DRAF trades off some of the performance of FPGAs for significant gains in area and power. DRAF improves area density by 10x over FPGAs and power consumption by more than 3x, enabling DRAF to satisfy demanding applications within strict power and cost constraints. While accelerators mapped to DRAF are 2-3x slower than those in FPGAs, they still deliver a 13x speedup and an 11x reduction in power consumption over a Xeon core for a wide range of datacenter tasks, including analytics and interactive services like speech recognition.

---
### Mellow Writes: Extending Lifetime in Resistive Memories through Selective Slow Write Backs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.52
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551419
* **Key Words**: Limiting, Phase change random access memory, Nonvolatile memory, Performance evaluation, Memory management, Analytical models, DRAM chips, resistive RAM, sensitivity analysis, sensitivity analysis, cell endurance, wear quota, eager mellow writes, bank aware mellow writes, microarchitectural mechanisms, write operation, nonvolatile memories, wear limiting technique, wear leveling, limited write endurance, standby power, scalability, DRAM-based main memory, resistive memory technologies, selective slow write backs, resistive memory lifetime, non-volatile memory, endurance, write latency, 
* **Abstract**: Emerging resistive memory technologies, such as PCRAM and ReRAM, have been proposed as promising replacements for DRAM-based main memory, due to their better scalability, low standby power, and non-volatility. However, limited write endurance is a major drawback for such resistive memory technologies. Wear leveling (balancing the distribution of writes) and wear limiting (reducing the number of writes) have been proposed to mitigate this disadvantage, but both techniques only manage a fixed budget of writes to a memory system rather than increase the number available. In this paper, we propose a new type of wear limiting technique, Mellow Writes, which reduces the wearout of individual writes rather than reducing the number of writes. Mellow Writes is based on the fact that slow writes performed with lower dissipated power can lead to longer endurance (and therefore longer lifetimes). For non-volatile memories, an N 1 to N 3 times endurance can be achieved if the write operation is slowed down by N times. We present three microarchitectural mechanisms (BankAware Mellow Writes, Eager Mellow Writes, and Wear Quota) that selectively perform slow writes to increase memory lifetime while minimizing performance impact. Assuming a factor N 2 advantage in cell endurance for a factor N slower write, our best Mellow Writes mechanism can achieve 2.58× lifetime and 1.06× performance of the baseline system. In addition, its performance is almost the same as a system aggressively optimized for performance (at the expense of endurance). Finally, Wear Quota guarantees a minimal lifetime (e.g., 8 years) by forcing more slow writes in presence of heavy workloads. We also perform sensitivity analysis on the endurance advantage factor for slow writes, from N 1 to N 3 , and find that our technique is still useful for factors as low as N 1 .

---
### MITTS: Memory Inter-arrival Time Traffic Shaping.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.53
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551420
* **Key Words**: Bandwidth, Memory management, Cloud computing, Throughput, Multicore processing, Instruction sets, Hardware, bandwidth allocation, cloud computing, multiprocessing systems, scheduling, storage management, MITTS, memory interarrival time traffic shaping, memory bandwidth allocation, multicore system, memory channel, scheduling, infrastructure as a service, IaaS cloud system, distributed hardware mechanism, Memory Architecture, Memory Management, Multicore, IaaS Clouds, 
* **Abstract**: Memory bandwidth severely limits the scalability and performance of multicore and manycore systems. Application performance can be very sensitive to both the delivered memory bandwidth and latency. In multicore systems, a memory channel is usually shared by multiple cores. Having the ability to precisely provision, schedule, and isolate memory bandwidth and latency on a per-core basis is particularly important when different memory guarantees are needed on a per-customer, per-application, or per-core basis. Infrastructure as a Service (IaaS) Cloud systems, and even general purpose multicores optimized for application throughput or fairness all benefit from the ability to control and schedule memory access on a fine-grain basis. In this paper, we propose MITTS (Memory Inter-arrival Time Traffic Shaping), a simple, distributed hardware mechanism which limits memory traffic at the source (Core or LLC). MITTS shapes memory traffic based on memory request inter-arrival time, enabling fine-grain bandwidth allocation. In an IaaS system, MITTS enables Cloud customers to express their memory distribution needs and pay commensurately. For instance, MITTS enables charging customers that have bursty memory traffic more than customers with uniform memory traffic for the same aggregate bandwidth. Beyond IaaS systems, MITTS can also be used to optimize for throughput or fairness in a general purpose multi-program workload. MITTS uses an online genetic algorithm to configure hardware bins, which can adapt for program phases and variable input sets. We have implemented MITTS in Verilog and have taped-out the design in a 25-core 32nm processor and find that MITTS requires less than 0.9% of core area. We evaluate across SPECint, PARSEC, Apache, and bhm Mail Server workloads, and find that MITTS achieves an average 1.18× performance gain compared to the best static bandwidth allocation, a 2.69× average performance/cost advantage in an IaaS setting, and up to 1.17x better throughput and...

---
### The Anytime Automaton.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.54
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551421
* **Key Words**: Automata, Approximate computing, Computational modeling, Approximation algorithms, Pipelines, Runtime, Real-time systems, approximation theory, automata theory, parallel processing, Anytime Automaton, approximate computing, parallel pipeline, anytime approximation, 
* **Abstract**: Approximate computing is an emerging paradigm enabling tradeoffs between accuracy and efficiency. However, a fundamental challenge persists: state-of-the-art techniques lack the ability to enforce runtime guarantees on accuracy. The convention is to 1) employ offline or online accuracy models, or 2) present experimental results that demonstrate empirically low error. Unfortunately, these approaches are still unable to guarantee acceptability of all application outputs at runtime. We offer a solution that revisits concepts from anytime algorithms. Originally explored for real-time decision problems, anytime algorithms have the property of producing results with increasing accuracy over time. We propose the Anytime Automaton, a new computation model that executes applications as a parallel pipeline of anytime approximations. An automaton produces approximate versions of the application output with increasing accuracy, guaranteeing that the final precise version is eventually reached. The automaton can be stopped whenever the output is deemed acceptable, otherwise, it is a simple matter of letting it run longer. We present an in-depth analysis of the model and demonstrate attractive runtime-accuracy profiles on various applications. Our anytime automaton is the first step towards systems where the acceptability of an application's output directly governs the amount of time and energy expended.

---
### Accelerating Markov Random Field Inference Using Molecular Optical Gibbs Sampling Units.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.55
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551422
* **Key Words**: Computer architecture, Bayes methods, graphics processing units, inference mechanisms, iterative methods, learning (artificial intelligence), Markov processes, Monte Carlo methods, random processes, statistical distributions, Markov random field inference, molecular optical Gibbs sampling units, statistics, data analytics, probabilistic machine learning algorithms, Markov Chain Monte Carlo sampling, MCMC sampling, Bayesian inference, probabilistic iterative algorithm, parameterized probability distributions, resonance energy transfer networks, RET networks, RET-based sampling units, RSU-G, discrete accelerator, emulation-based evaluation, computer vision applications, HD images, RSU augmented GPU, probabilistic computing, emerging technology, resonance energy transfer, nanophotonics, 
* **Abstract**: The increasing use of probabilistic algorithms from statistics and machine learning for data analytics presents new challenges and opportunities for the design of computing systems. One important class of probabilistic machine learning algorithms is Markov Chain Monte Carlo (MCMC) sampling, which can be used on a wide variety of applications in Bayesian Inference. However, this probabilistic iterative algorithm can be inefficient in practice on today's processors, especially for problems with high dimensionality and complex structure. The source of inefficiency is generating samples from parameterized probability distributions. This paper seeks to address this sampling inefficiency and presents a new approach to support probabilistic computing that leverages the native randomness of Resonance Energy Transfer (RET) networks to construct RET-based sampling units (RSU). Although RSUs can be designed for a variety of applications, we focus on the specific class of probabilistic problems described as Markov Random Field Inference. Our proposed RSU uses a RET network to implement a molecular-scale optical Gibbs sampling unit (RSU-G) that can be integrated into a processor / GPU as specialized functional units or organized as a discrete accelerator. We experimentally demonstrate the fundamental operation of an RSU using a macro-scale hardware prototype. Emulation-based evaluation of two computer vision applications for HD images reveal that an RSU augmented GPU provides speedups over a GPU of 3 and 16. Analytic evaluation shows a discrete accelerator that is limited by 336 GB/s DRAM produces speedups of 21 and 54 versus the GPU implementations.

---
### Evaluation of an Analog Accelerator for Linear Algebra.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.56
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551423
* **Key Words**: Computer architecture, Acceleration, Mathematical model, Analog computers, Silicon, Differential equations, Embedded systems, analogue integrated circuits, digital computers, linear algebra, power aware computing, analog accelerator, linear algebra, supply voltage scaling, modern integrated circuits, linear equations, analog computing, digital computers, accelerator architectures, analog-digital integrated circuits, analog computers, linear algebra, 
* **Abstract**: Due to the end of supply voltage scaling and the increasing percentage of dark silicon in modern integrated circuits, researchers are looking for new scalable ways to get useful computation from existing silicon technology. In this paper we present a reconfigurable analog accelerator for solving systems of linear equations. Commonly perceived downsides of analog computing, such as low precision and accuracy, limited problem sizes, and difficulty in programming are all compensated for using methods we discuss. Based on a prototyped analog accelerator chip we compare the performance and energy consumption of the analog solver against an efficient digital algorithm running on a CPU, and find that the analog accelerator approach may be an order of magnitude faster and provide one third energy savings, depending on the accelerator design. Due to the speed and efficiency of linear algebra algorithms running on digital computers, an analog accelerator that matches digital performance needs a large silicon footprint. Finally, we conclude that problem classes outside of systems of linear equations may hold more promise for analog acceleration.

---
### LaPerm: Locality Aware Scheduler for Dynamic Parallelism on GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.57
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551424
* **Key Words**: Kernel, Graphics processing units, Parallel processing, Dynamic scheduling, Computer architecture, Instruction sets, Vehicle dynamics, graphics processing units, multiprocessing systems, parallel architectures, LaPerm, locality aware scheduler, dynamic parallelism, GPU execution model, GPU architecture, bulk synchronous parallel model, GPU memory hierarchy, dynamic nested kernel, hierarchical reference locality, locality-aware TB scheduler, parent-child locality, stream multiprocessor, CUDA, child thread block, GPU, dynamic parallelism, irregular applications, thread block scheduler, memory locality, 
* **Abstract**: Recent developments in GPU execution models and architectures have introduced dynamic parallelism to facilitate the execution of irregular applications where control flow and memory behavior can be unstructured, time-varying, and hierarchical. The changes brought about by this extension to the traditional bulk synchronous parallel (BSP) model also creates new challenges in exploiting the current GPU memory hierarchy. One of the major challenges is that the reference locality that exists between the parent and child thread blocks (TBs) created during dynamic nested kernel and thread block launches cannot be fully leveraged using the current TB scheduling strategies. These strategies were designed for the current implementations of the BSP model but fall short when dynamic parallelism is introduced since they are oblivious to the hierarchical reference locality. We propose LaPerm, a new locality-aware TB scheduler that exploits such parent-child locality, both spatial and temporal. LaPerm adopts three different scheduling decisions to i) prioritize the execution of the child TBs, ii) bind them to the stream multiprocessors (SMXs) occupied by their parents TBs, and iii) maintain workload balance across compute units. Experiments with a set of irregular CUDA applications executed on a cycle-level simulator employing dynamic parallelism demonstrate that LaPerm is able to achieve an average of 27% performance improvement over the baseline round-robin TB scheduler commonly used in modern GPUs.

---
### ActivePointers: A Case for Software Address Translation on GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.58
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551425
* **Key Words**: Graphics processing units, Hardware, Instruction sets, Memory management, Context, graphics processing units, parallel architectures, ActivePointers, software address translation layer, paging system, page faults, virtual address space management, GPU programs, fully functional memory mapped files, GPU memory, GPU page cache, trigger page faults, translation cache, hardware registers, translation aggregation, deadlock-free page fault handling, NVIDIA GPU, AVX vector instructions, Operating systems, Parallel architectures, Memory management, 
* **Abstract**: Modern discrete GPUs have been the processors of choice for accelerating compute-intensive applications, but using them in large-scale data processing is extremely challenging. Unfortunately, they do not provide important I/O abstractions long established in the CPU context, such as memory mapped files, which shield programmers from the complexity of buffer and I/O device management. However, implementing these abstractions on GPUs poses a problem: the limited GPU virtual memory system provides no address space management and page fault handling mechanisms to GPU developers, and does not allow modifications to memory mappings for running GPU programs. We implement ActivePointers, a software address translation layer and paging system that introduces native support for page faults and virtual address space management to GPU programs, and enables the implementation of fully functional memory mapped files on commodity GPUs. Files mapped into GPU memory are accessed using active pointers, which behave like regular pointers but access the GPU page cache under the hood, and trigger page faults which are handled on the GPU. We design and evaluate a number of novel mechanisms, including a translation cache in hardware registers and translation aggregation for deadlock-free page fault handling of threads in a single warp. We extensively evaluate ActivePointers on commodity NVIDIA GPUs using microbenchmarks, and also implement a complex image processing application that constructs a photo collage from a subset of 10 million images stored in a 40GB file. The GPU implementation maps the entire file into GPU memory and accesses it via active pointers. The use of active pointers adds only up to 1% to the application's runtime, while enabling speedups of up to 3.9× over a combined CPU+GPU implementation and 2.6× over a 12-core CPU-only implementation which uses AVX vector instructions.

---
### Virtual Thread: Maximizing Thread-Level Parallelism beyond GPU Scheduling Limit.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.59
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551426
* **Key Words**: Instruction sets, Registers, Graphics processing units, Scheduling, Memory management, Context, Complexity theory, formal logic, graphics processing units, parallel processing, scheduling, software architecture, virtualisation, virtual thread, thread-level parallelism, GPU scheduling limit, concurrent threads, thread scheduling structures, on-chip memory, VT architecture, cooperative thread arrays, CTA, logic complexity, GPU, GPGPU, Warp Scheduling, Virtual Thread (VT), Capacity Limit, Scheduling Limit, 
* **Abstract**: Modern GPUs require tens of thousands of concurrent threads to fully utilize the massive amount of processing resources. However, thread concurrency in GPUs can be diminished either due to shortage of thread scheduling structures (scheduling limit), such as available program counters and single instruction multiple thread stacks, or due to shortage of on-chip memory (capacity limit), such as register file and shared memory. Our evaluations show that in practice concurrency in many general purpose applications running on GPUs is curtailed by the scheduling limit rather than the capacity limit. Maximizing the utilization of on-chip memory resources without unduly increasing the scheduling complexity is a key goal of this paper. This paper proposes a Virtual Thread (VT) architecture which assigns Cooperative Thread Arrays (CTAs) up to the capacity limit, while ignoring the scheduling limit. However, to reduce the logic complexity of managing more threads concurrently, we propose to place CTAs into active and inactive states, such that the number of active CTAs still respects the scheduling limit. When all the warps in an active CTA hit a long latency stall, the active CTA is context switched out and the next ready CTA takes its place. We exploit the fact that both active and inactive CTAs still fit within the capacity limit which obviates the need to save and restore large amounts of CTA state. Thus VT significantly reduces performance penalties of CTA swapping. By swapping between active and inactive states, VT can exploit higher degree of thread level parallelism without increasing logic complexity. Our simulation results show that VT improves performance by 23.9% on average.

---
### All-Inclusive ECC: Thorough End-to-End Protection for Reliable Computer Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.60
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551427
* **Key Words**: Error correction codes, Pins, Reliability, Bandwidth, Clocks, DRAM chips, circuit reliability, data protection, DRAM chips, error correction codes, security of data, all-inclusive error checking and correcting code, AIECC code, end-to-end, computer memory reliability, clock control command and address, CCCA signal, memory protection scheme, DRAM data protection, Reliability, ECC, main memory, DRAM, 
* **Abstract**: Increasing transfer rates and decreasing I/O voltage levels make signals more vulnerable to transmission errors. While the data in computer memory are well-protected by modern error checking and correcting (ECC) codes, the clock, control, command, and address (CCCA) signals are weakly protected or even unprotected such that transmission errors leave serious gaps in data-only protection. This paper presents All-Inclusive ECC (AIECC), a memory protection scheme that leverages and augments data ECC to also thoroughly protect CCCA signals. AIECC provides strong end-to-end protection of memory, detecting nearly 100% of CCCA errors and also preventing transmission errors from causing latent memory data corruption. AIECC provides these system-level benefits without requiring extra storage and transfer overheads and without degrading the effective level of data protection.

---
### Rescuing Uncorrectable Fault Patterns in On-Chip Memories through Error Pattern Transformation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.61
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551428
* **Key Words**: Error correction codes, System-on-chip, Reliability, SRAM cells, Error correction, Voltage control, Circuit faults, built-in self test, error correction codes, fault tolerance, power aware computing, SRAM chips, uncorrectable fault patterns, error pattern transformation, voltage scaling, processor power reduction, SRAM cells, error correcting code, ECC, low-latency error correction, logical bit, BIST-detectable fault pattern, on-chip memory reliability, ARM Cortex-A7-like core, 
* **Abstract**: Voltage scaling can effectively reduce processor power, but also reduces the reliability of the SRAM cells in on-chip memories. Therefore, it is often accompanied by the use of an error correcting code (ECC). To enable reliable and efficient memory operation at low voltages, ECCs for on-chip memories must provide both high error coverage and low correction latency. In this paper, we propose error pattern transformation, a novel low-latency error correction technique that allows on-chip memories to be scaled to voltages lower than what has been previously possible. Our technique relies on the observation that the number of on-chip memory errors that many ECCs can correct differs widely depending on the error patterns in the logical words they protect. We propose adaptively rearranging the logical bit to physical bit mapping per word according to the BIST-detectable fault pattern in the physical word. The adaptive logical bit to physical bit mapping transforms many uncorrectable error patterns in the logical words into correctable error patterns and, therefore, improving on-chip memory reliability. This reduces the minimum voltage at which on-chip memory can run by 70mV over the best low-latency ECC baseline, leading to a 25.7% core-wide power reduction for an ARM Cortex-A7-like core. Energy per instruction is reduced by 15.7% compared to the best baseline.

---
### RelaxFault Memory Repair.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.62
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551429
* **Key Words**: Random access memory, Maintenance engineering, Circuit faults, Memory management, Reliability, Error correction codes, Large-scale systems, cache storage, fault tolerance, memory architecture, RelaxFault memory repair, memory system reliability, fault tolerance, low-cost resilience mechanism, fault-aware resilience mechanism, hardware-only resilience mechanism, faulty memory locations remapping, LLC capacity, last level cache, ECC, memory system architecture, memory fault model, Memory, DRAM, Reliability, Dependable Architecture, Data-center scale computing, High Performance Computing, Microarchitecture, 
* **Abstract**: Memory system reliability is a serious concern in many systems today, and is becoming more worrisome as technology scales and system size grows. Stronger fault tolerance capability is therefore desirable, but often comes at high cost. In this paper, we propose a low-cost, fault-aware, hardware-only resilience mechanism, RelaxFault, that repairs the vast majority of memory faults using a small amount of the LLC to remap faulty memory locations. RelaxFault requires less than 100KiB of LLC capacity, has near-zero impact on performance and power. By repairing faults, RelaxFault relaxes the requirement for high fault tolerance of other mechanisms, such as ECC. A better tradeoff between resilience and overhead is made by exploiting an understanding of memory system architecture and fault characteristics. We show that RelaxFault provides better repair capability than prior work of similar cost, improves memory reliability to a greater extent, and significantly reduces the number of maintenance events and memory module replacements. We also propose a more refined memory fault model than prior work and demonstrate its importance.

---
### Using Multiple Input, Multiple Output Formal Control to Maximize Resource Efficiency in Architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.63
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551430
* **Key Words**: Program processors, MIMO, Control theory, Multicore processing, Frequency control, Control systems, Uncertainty, computer architecture, MIMO systems, power consumption, robust control, multiple input multiple output formal control, resource efficiency, power consumption, average utilization, robust control, heuristic-based controllers, MIMO controllers, future processors, Architectural control, Control theory, Tuning, 
* **Abstract**: As processors seek more resource efficiency, they increasingly need to target multiple goals at the same time, such as a level of performance, power consumption, and average utilization. Robust control solutions cannot come from heuristic-based controllers or even from formal approaches that combine multiple single-parameter controllers. Such controllers may end-up working against each other. What is needed is control-theoretical MIMO (multiple input, multiple output) controllers, which actuate on multiple inputs and control multiple outputs in a coordinated manner. In this paper, we use MIMO control-theory techniques to develop controllers to dynamically tune architectural parameters in processors. To our knowledge, this is the first work in this area. We discuss three ways in which a MIMO controller can be used. We develop an example of MIMO controller and show that it is substantially more effective than controllers based on heuristics or built by combining single-parameter formal controllers. The general approach discussed here is likely to be increasingly relevant as future processors become more resource-constrained and adaptive.

---
### Exploiting Dynamic Timing Slack for Energy Efficiency in Ultra-Low-Power Embedded Systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.64
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551431
* **Key Words**: Clocks, Embedded systems, Microcontrollers, Embedded software, Delays, embedded systems, microcontrollers, power aware computing, dynamic timing slack, DTS, energy efficiency, ultralow-power embedded system, embedded software application, voltage scaling, frequency scaling, microprocessor, microcontroller, Timing Slack, Energy Efficiency, Ultra-low-power, Embedded Systems, 
* **Abstract**: Many emerging applications such as the internet of things, wearables, and sensor networks have ultra-low-power requirements. At the same time, cost and programmability considerations dictate that many of these applications will be powered by general purpose embedded microprocessors and microcontrollers, not ASICs. In this paper, we exploit a new opportunity for improving energy efficiency in ultralow-power processors expected to drive these applications -- dynamic timing slack. Dynamic timing slack exists when an embedded software application executed on a processor does not exercise the processor's static critical paths. In such scenarios, the longest path exercised by the application has additional timing slack which can be exploited for power savings at no performance cost by scaling down the processor's voltage at the same frequency until the longest exercised paths just meet timing constraints. Paths that cannot be exercised by an application can safely be allowed to violate timing constraints. We show that dynamic timing slack exists for many ultra-low-power applications and that exploiting dynamic timing slack can result in significant power savings for any ultra-low-power processors. We also present an automated methodology for identifying dynamic timing slack and selecting a safe operating point for a processor and a particular embedded software. Our approach for identifying and exploiting dynamic timing slack is non-speculative, requires no programmer intervention and little or no hardware support, and demonstrates potential power savings of up to 32%, 25% on average, over a range of embedded applications running on a common ultra-low-power processor, at no performance cost.

---
### CASH: Supporting IaaS Customers with a Sub-core Configurable Architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.65
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551432
* **Key Words**: Quality of service, Runtime, Multicore processing, Optimization, Resource management, Convex functions, cloud computing, concave programming, learning (artificial intelligence), reconfigurable architectures, resource allocation, CASH, IaaS customers, subcore configurable architecture, infrastructure as a service, IaaS clouds, low-level resource usage, fine-grain configurable resources, quality-of-service requirements, QoS requirements, cost minimization, fine-grain configurable architecture, cost-optimizing runtime system, hardware architecture, ALU, L2 cache banks, control theory, machine learning, fine-grain configurability, nonconvex optimization, cost savings, Configurable Architectures, Manycore Architectures, Control System, Machine Learning, 
* **Abstract**: Infrastructure as a Service (IaaS) Clouds have grown increasingly important. Recent architecture designs support IaaS providers through fine-grain configurability, allowing providers to orchestrate low-level resource usage. Little work, however, has been devoted to supporting IaaS customers who must determine how to use such fine-grain configurable resources to meet quality-of-service (QoS) requirements while minimizing cost. This is a difficult problem because the multiplicity of configurations creates a non-convex optimization space. In addition, this optimization space may change as customer applications enter and exit distinct processing phases. In this paper, we overcome these issues by proposing CASH: a fine-grain configurable architecture co-designed with a cost-optimizing runtime system. The hardware architecture enables configurability at the granularity of individual ALUs and L2 cache banks and provides unique interfaces to support low-overhead, dynamic configuration and monitoring. The runtime uses a combination of control theory and machine learning to configure the architecture such that QoS requirements are met and cost is minimized. Our results demonstrate that the combination of fine-grain configurability and non-convex optimization provides tremendous cost savings (70% savings) compared to coarse-grain heterogeneity and heuristic optimization. In addition, the system is able to customize configurations to particular applications, respond to application phases, and provide near optimal cost for QoS targets.

---
### Boosting Access Parallelism to PCM-Based Main Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.66
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551433
* **Key Words**: Phase change materials, Parallel processing, Random access memory, Throughput, Memory architecture, Boosting, Servers, DRAM chips, error correction codes, error detection codes, multiprogramming, multi-threading, phase change memories, PCM-based main memory, DRAM, main memory replacement, phase change memory, error detection/correction bits, reads with an ongoing write, RoW, write with an ongoing write, WoW, PCM access parallelism, PCMap, multiprogrammed workloads, multithreaded workloads, Phase Change Memory, Write performance, 
* **Abstract**: Despite its promise as a DRAM main memory replacement, Phase Change Memory (PCM) has high write latencies which can be a serious detriment to its widespread adoption. Apart from slowing down a write request, the consequent high latency can also keep other chips of the same rank, that are not involved in this write, idle for long times. There are several practical considerations that make it difficult to allow subsequent reads and/or writes to be served concurrently from the same chips during the long latency write. This paper proposes and evaluates several novel mechanisms - re-constructing data from error correction bits instead of waiting for chips currently busy to serve a read, rotating word mappings across chips of a PCM rank, and rotating the mapping of error detection/correction bits across these chips - to overlap several reads with an ongoing write (RoW) and even a write with an ongoing write (WoW). The paper also presents the necessary micro-architectural enhancements needed to implement these mechanisms, without significantly changing the current interfaces. The resulting PCM access parallelism (PCMap) system incorporating these enhancements, boosts the intra-rank-level parallelism during such writes from a very low baseline value of 2.4 to an average and maximum values of 4.5 and 7.4, respectively (out of a maximum of 8.0), across a wide spectrum of both multiprogrammed and multithreaded workloads. This boost in parallelism results in an average IPC improvement of 15.6% and 16.7% for the multi-programmed and multi-threaded workloads, respectively.

---
### Agile Paging: Exceeding the Best of Nested and Shadow Paging.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.67
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551434
* **Key Words**: Hardware, Virtual machine monitors, Software, Switches, Memory management, Virtualization, Virtual machining, paged storage, virtual machines, virtualisation, agile paging, nested paging, shadow paging, virtualizing memory, virtual machine monitor, VMM, virtualized page walk, virtual memory, virtual machines, virtualization, translation lookaside buffer, nested paging, shadow paging, 
* **Abstract**: Virtualization provides benefits for many workloads, but the overheads of virtualizing memory are not universally low. The cost comes from managing two levels of address translation - one in the guest virtual machine (VM) and the other in the host virtual machine monitor (VMM) - with either nested or shadow paging. Nested paging directly performs a two-level page walk that makes TLB misses slower than unvirtualized native, but enables fast page tables changes. Alternatively, shadow paging restores native TLB miss speeds, but requires costly VMM intervention on page table updates. This paper proposes agile paging that combines both techniques and exceeds the best of both. A virtualized page walk starts with shadow paging and optionally switches in the same page walk to nested paging where frequent page table updates would cause costly VMM interventions. Agile paging enables most TLB misses to be handled as fast as native while most page table changes avoid VMM intervention. It requires modest changes to hardware (e.g., demark when to switch) and VMM policies (e.g., predict good switching opportunities). We emulate the proposed hardware and prototype the software in Linux with KVM on x86-64. Agile paging performs more than 12% better than the best of the two techniques and comes within 4% of native execution for all workloads.

---
### Energy Efficient Data Encoding in DRAM Channels Exploiting Data Value Similarity.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/ISCA.2016.68
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551435
* **Key Words**: Random access memory, Switches, Hamming weight, Encoding, Resistors, Pins, Prefetching, DRAM chips, encoding, energy conservation, energy consumption, low-power electronics, energy efficient data encoding, DRAM channels, data value similarity, DRAM data bandwidth, energy dissipation, DRAM data bus, energy consumption reduction, DRAM interfaces, symmetric termination, pseudoopen drain, POD, low voltage swing terminated logic, LVSTL, termination energy, data words, bitwise difference encoding, BD-encoding, hamming weight, switching energy, power noise, memory controller, BD-coder, DRAM, interface, energy, data locality, daa similarity, POD, LVSTL, termination, switching activity, 
* **Abstract**: As DRAM data bandwidth increases, tremendous energy is dissipated in the DRAM data bus. To reduce the energy consumed in the data bus, DRAM interfaces with symmetric termination, such as Pseudo Open Drain (POD) and Low Voltage Swing Terminated Logic (LVSTL), have been adopted in modern DRAMs. In interfaces using asymmetric termination, the amount of termination energy is proportional to the hamming weight of the data words. In this work, we propose Bitwise Difference Encoding (BD-Encoding), which decreases the hamming weight of data words, leading to a reduction in energy consumption in the modern DRAM data bus. Since smaller hamming weight of the data words also reduces switching activity, switching energy and power noise are also both reduced. BD-Encoding exploits the similarity in data words in the DRAM data bus. We observed that similar data words (i.e. data words whose hamming distance is small) are highly likely to be sent over at similar times. Based on this observation, BD-coder stores the data recently sent over in both the memory controller and DRAMs. Then, BD-coder transfers the bitwise difference between the current data and the most similar data. In an evaluation using SPEC 2006, BD-Encoding using 64 recent data reduced termination energy by 58.3% and switching energy by 45.3%. In addition, 55% of the LdI/dt noise was decreased with BD-Encoding.
