### isca 2017 | 54 papers.
---
### In-Datacenter Performance Analysis of a Tensor Processing Unit.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080246
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080246?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Neural networks, 
* **Abstract**: Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU) --- deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X -- 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X -- 80X higher. Moreover, using the CPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.

---
### ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080244
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080244?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Neural networks, 
* **Abstract**: Deep Neural Networks (DNNs) have demonstrated state-of-the-art performance on a broad range of tasks involving natural language, speech, image, and video processing, and are deployed in many real world applications. However, DNNs impose significant computational challenges owing to the complexity of the networks and the amount of data they process, both of which are projected to grow in the future. To improve the efficiency of DNNs, we propose ScaleDeep, a dense, scalable server architecture, whose processing, memory and interconnect subsystems are specialized to leverage the compute and communication characteristics of DNNs. While several DNN accelerator designs have been proposed in recent years, the key difference is that ScaleDeep primarily targets DNN training, as opposed to only inference or evaluation. The key architectural features from which ScaleDeep derives its efficiency are: (i) heterogeneous processing tiles and chips to match the wide diversity in computational characteristics (FLOPs and Bytes/FLOP ratio) that manifest at different levels of granularity in DNNs, (ii) a memory hierarchy and 3-tiered interconnect topology that is suited to the memory access and communication patterns in DNNs, (iii) a low-overhead synchronization mechanism based on hardware data-flow trackers, and (iv) methods to map DNNs to the proposed architecture that minimize data movement and improve core utilization through nested pipelining. We have developed a compiler to allow any DNN topology to be programmed onto ScaleDeep, and a detailed architectural simulator to estimate performance and energy. The simulator incorporates timing and power models of ScaleDeep's components based on synthesis to Intel's 14nm technology. We evaluate an embodiment of ScaleDeep with 7032 processing tiles that operates at 600 MHz and has a peak performance of 680 TFLOPs (single precision) and 1.35 PFLOPs (half-precision) at 1.4KW. Across 11 state-of-the-art DNNs containing 0.65M-14.9M neurons and 6.8M-145.9M weights, including winners from 5 years of the ImageNet competition, ScaleDeep demonstrates 6x-28x speedup at iso-power over the state-of-the-art performance on GPUs.

---
### SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080254
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080254?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Special purpose systems, Parallel architectures, 
* **Abstract**: Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs, especially in mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to a multiplier array, where they are extensively reused; product accumulation is performed in a novel accumulator array. On contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7x and 2.3x, respectively, over a comparably provisioned dense CNN accelerator.

---
### Bespoke Processors for Applications with Ultra-low Area and Power Constraints.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080247
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080247?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Special purpose systems, Embedded and cyber-physical systems, Embedded systems, Hardware, Very large scale integration design, Application-specific VLSI designs, Application specific processors, 
* **Abstract**: A large number of emerging applications such as implantables, wearables, printed electronics, and IoT have ultra-low area and power constraints. These applications rely on ultra-low-power general purpose microcontrollers and microprocessors, making them the most abundant type of processor produced and used today. While general purpose processors have several advantages, such as amortized development cost across many applications, they are significantly over-provisioned for many area- and power-constrained systems, which tend to run only one or a small number of applications over their lifetime. In this paper, we make a case for bespoke processor design, an automated approach that tailors a general purpose processor IP to a target application by removing all gates from the design that can never be used by the application. Since removed gates are never used by an application, bespoke processors can achieve significantly lower area and power than their general purpose counterparts without any performance degradation. Also, gate removal can expose additional timing slack that can be exploited to increase area and power savings or performance of a bespoke design. Bespoke processor design reduces area and power by 62% and 50%, on average, while exploiting exposed timing slack improves average power savings to 65%.

---
### A Programmable Galois Field Processor for the Internet of Things.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080227
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080227?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Special purpose systems, 
* **Abstract**: This paper investigates the feasibility of a unified processor architecture to enable error coding flexibility and secure communication in low power Internet of Things (IoT) wireless networks. Error coding flexibility for wireless communication allows IoT applications to exploit the large tradeoff space in data rate, link distance and energy-efficiency. As a solution, we present a light-weight Galois Field (GF) processor to enable energy-efficient block coding and symmetric/asymmetric cryptography kernel processing for a wide range of GF sizes (2m, m = 2, 3, ..., 233) and arbitrary irreducible polynomials. Program directed connections among primitive GF arithmetic units enable dynamically configured parallelism to efficiently perform either four-way SIMD 5- to 8-bit GF operations, including multiplicative inverse, or a wide bit-width (e.g., 32-bit) GF product in a single cycle. To illustrate our ideas, we synthesized our GF processor in a 28nm technology. Compared to a baseline software implementation optimized for a general purpose ARM M0+ processor, our processor exhibits a 5-20 x speedup for a range of error correction codes and symmetric/asymmetric cryptography applications. Additionally, our proposed GF processor consumes 431Î¼W at 0.9V and 100MHz, and achieves 35.5pJ/b energy efficiency while executing AES operations at 12.2Mbps. We achieve this within an area of 0.01mm2.

---
### XPro: A Cross-End Processing Architecture for Data Analytics in Wearables.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080219
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080219?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Heterogeneous (hybrid) systems, 
* **Abstract**: Wearable computing systems have spurred many opportunities to continuously monitor human bodies with sensors worn on or implanted in the body. These emerging platforms have started to revolutionize many fields, including healthcare and wellness applications, particularly when integrated with intelligent analytic capabilities. However, a significant challenge that computer architects are facing is how to embed sophisticated analytic capabilities in wearable computers in an energy-efficient way while not compromising system performance. In this paper, we present XPro, a novel cross-end analytic engine architecture for wearable computing systems. The proposed cross-end architecture is able to realize a generic classification design across wearable sensors and a data aggregator with high energy-efficiency. To facilitate the practical use of XPro, we also develop an Automatic XPro Generator that formally generates XPro instances according to specific design constraints. As a proof of concept, we study the design and implementation of XPro with six different health applications. Evaluation results show that, compared with state-of-the-art methods, XPro can increase the battery life of the sensor node by 1.6-2.4X while at the same time reducing system delay by 15.6-60.8% for wearable computing systems.

---
### Regaining Lost Cycles with HotCalls: A Fast Interface for SGX Secure Enclaves.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080208
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080208?download=true
* **Key Words**: Security and privacy, Security in hardware, Software and application security, Software security engineering, Systems security, 
* **Abstract**: Intel's SGX secure execution technology allows running computations on secret data using untrusted servers. While recent work showed how to port applications and large-scale computations to run under SGX, the performance implications of using the technology remains an open question. We present the first comprehensive quantitative study to evaluate the performance of SGX. We show that straightforward use of SGX library primitives for calling functions add between 8,200 - 17,000 cycles overhead, compared to 150 cycles of a typical system call. We quantify the performance impact of these library calls and show that in applications with high system calls frequency, such as memcached, openVPN, and lighttpd, which all have high bandwidth network requirements, the performance degradation may be as high as 79%. We investigate the sources of this performance degradation by leveraging a new set of microbenchmarks for SGX-specific operations such as enclave entry-calls and out-calls, and encrypted memory I/O accesses. We leverage the insights we gain from these analyses to design a new SGX interface framework HotCalls. HotCalls are based on a synchronization spin-lock mechanism and provide a 13-27x speedup over the default interface. It can easily be integrated into existing code, making it a practical solution. Compared to a baseline SGX implementation of memcached, openVPN, and lighttpd - we show that using the new interface boosts the throughput by 2.6-3.7x, and reduces application latency by 62-74%.

---
### InvisiMem: Smart Memory Defenses for Memory Bus Side Channel.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080232
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080232?download=true
* **Key Words**: Hardware, Integrated circuits, 3D integrated circuits, Security and privacy, Security in hardware, Hardware security implementation, Hardware-based security protocols, 
* **Abstract**: A practically feasible low-overhead hardware design that provides strong defenses against memory bus side channel remains elusive. This paper observes that smart memory, memory with compute capability and a packetized interface, can dramatically simplify this problem. InvisiMem expands the trust base to include the logic layer in the smart memory to implement cryptographic primitives, which aid in addressing several memory bus side channel vulnerabilities efficiently. This allows the secure host processor to send encrypted addresses over the untrusted memory bus, and thereby eliminates the need for expensive address obfuscation techniques based on Oblivious RAM (ORAM). In addition, smart memory enables efficient solutions for ensuring freshness without using expensive Merkle trees, and mitigates memory bus timing channel using constant heart-beat packets. We demonstrate that InvisiMem designs have one to two orders of magnitude of lower overheads for performance, space, energy, and memory bandwidth, compared to prior solutions.

---
### ObfusMem: A Low-Overhead Access Obfuscation for Trusted Memories.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080230
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080230?download=true
* **Key Words**: Security and privacy, Security in hardware, Hardware security implementation, Hardware-based security protocols, 
* **Abstract**: Trustworthy software requires strong privacy and security guarantees from a secure trust base in hardware. While chipmakers provide hardware support for basic security and privacy primitives such as enclaves and memory encryption. these primitives do not address hiding of the memory access pattern, information about which may enable attacks on the system or reveal characteristics of sensitive user data. State-of-the-art approaches to protecting the access pattern are largely based on Oblivious RAM (ORAM). Unfortunately, current ORAM implementations suffer from very significant practicality and overhead concerns, including roughly an order of magnitude slowdown, more than 100% memory capacity overheads, and the potential for system deadlock.

---
### ThermoGater: Thermally-Aware On-Chip Voltage Regulation.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080250
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080250?download=true
* **Key Words**: Hardware, Power and energy, Thermal issues, 
* **Abstract**: Tailoring the operating voltage to fine-grain temporal changes in the power and performance needs of the workload can effectively enhance power efficiency. Therefore, power-limited computing platforms of today widely deploy integrated (i.e., on-chip) voltage regulation which enables fast fine-grain voltage control. Voltage regulators convert and distribute power from an external energy source to the processor. Unfortunately, power conversion loss is inevitable and projected integrated regulator designs are unlikely to eliminate this loss even asymptotically. Reconfigurable power delivery by selective shut-down, i.e., gating, of distributed on-chip regulators in response to spatio-temporal changes in power demand can sustain operation at the minimum conversion loss. However, even the minimum conversion loss is sizable, and as conversion loss gets dissipated as heat, on-chip regulators can easily cause thermal emergencies due to their small footprint.

---
### PowerChief: Intelligent Power Allocation for Multi-Stage Applications to Improve Responsiveness on Power Constrained CMP.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080224
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080224?download=true
* **Key Words**: Computer systems organization, Architectures, Distributed architectures, Cloud computing, Hardware, Power and energy, 
* **Abstract**: Modern user facing applications consist of multiple processing stages with a number of service instances in each stage. The latency profile of these multi-stage applications is intrinsically variable, making it challenging to provide satisfactory responsiveness. Given a limited power budget, improving the end-to-end latency requires intelligently boosting the bottleneck service across stages using multiple boosting techniques. However, prior work fail to acknowledge the multi-stage nature of user-facing applications and perform poorly in improving responsiveness on power constrained CMP, as they are unable to accurately identify bottleneck service and apply the boosting techniques adaptively.

---
### CHARSTAR: Clock Hierarchy Aware Resource Scaling in Tiled ARchitectures.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080212
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080212?download=true
* **Key Words**: Hardware, Very large scale integration design, On-chip resource management, 
* **Abstract**: High-performance architectures are over-provisioned with resources to extract the maximum achievable performance out of applications. Two sources of avoidable power dissipation are the leakage power from underutilized resources, along with clock power from the clock hierarchy that feeds these resources. Most reconfiguration mechanisms either focus solely on power gating execution resources alone or in addition, simply turn off the immediate clock tree segment which supplied the clock to those resources. These proposals neither attempt to gate further up the clock hierarchy nor do they involve the clock hierarchy in influencing the reconfiguration decisions. The primary contribution of CHARSTAR is optimizing reconfiguration mechanisms to become clock hierarchy aware. Resource gating decisions are cognizant of the power consumed by each node in the clock hierarchy and additionally, entire branches of the clock tree are greedily shut down whenever possible.

---
### Chasing Away RAts: Semantics and Evaluation for Relaxed Atomics on Heterogeneous Systems.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080206
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080206?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Single instruction, multiple data, Computing methodologies, Parallel computing methodologies, Parallel algorithms, Shared memory algorithms, Hardware, Communication hardware, interfaces and storage, Software and its engineering, Software organization and properties, Software functional properties, Correctness, Consistency, 
* **Abstract**: An unambiguous and easy-to-understand memory consistency model is crucial for ensuring correct synchronization and guiding future design of heterogeneous systems. In a widely adopted approach, the memory model guarantees sequential consistency (SC) as long as programmers obey certain rules. The popular data-race-free-0 (DRF0) model exemplifies this SC-centric approach by requiring programmers to avoid data races. Recent industry models, however, have extended such SC-centric models to incorporate relaxed atomics. These extensions can improve performance, but are difficult to specify formally and use correctly. This work addresses the impact of relaxed atomics on consistency models for heterogeneous systems in two ways. First, we introduce a new model, Data-Race-Free-Relaxed (DRFrlx), that extends DRF0 to provide SC-centric semantics for the common use cases of relaxed atomics. Second, we evaluate the performance of relaxed atomics in CPU-GPU systems for these use cases. We find mixed results -- for most cases, relaxed atomics provide only a small benefit in execution time, but for some cases, they help significantly (e.g., up to 51% for DRFrlx over DRF0).

---
### Hiding the Long Latency of Persist Barriers Using Speculative Execution.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080240
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080240?download=true
* **Key Words**: Computer systems organization, Architectures, Hardware, Emerging technologies, Memory and dense storage, 
* **Abstract**: Byte-addressable non-volatile memory technology is emerging as an alternative for DRAM for main memory. This new Non-Volatile Main Memory (NVMM) allows programmers to store important data in data structures in memory instead of serializing it to the file system, thereby providing a substantial performance boost. However, modern systems reorder memory operations and utilize volatile caches for better performance, making it difficult to ensure a consistent state in NVMM. Intel recently announced a new set of persistence instructions, clflushopt, clwb, and pcommit. These new instructions make it possible to implement fail-safe code on NVMM, but few workloads have been written or characterized using these new instructions.

---
### Non-Speculative Load-Load Reordering in TSO.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080220
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080220?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, 
* **Abstract**: In Total Store Order memory consistency (TSO), loads can be speculatively reordered to improve performance. If a load-load reordering is seen by other cores, speculative loads must be squashed and re-executed. In architectures with an unordered interconnection network and directory coherence, this has been the established view for decades. We show, for the first time, that it is not necessary to squash and re-execute speculatively reordered loads in TSO when their reordering is seen. Instead, the reordering can be hidden form other cores by the coherence protocol. The implication is that we can irrevocably bind speculative loads. This allows us to commit reordered loads out-of-order without having to wait (for the loads to become non-speculative) or without having to checkpoint committed state (and rollback if needed), just to ensure correctness in the rare case of some core seeing the reordering. We show that by exposing a reordering to the coherence layer and by appropriately modifying a typical directory protocol we can successfully hide load-load reordering without perceptible performance cost and without deadlock. Our solution is cost-effective and increases the performance of out-of-order commit by a sizable margin, compared to the base case where memory operations are not allowed to commit if the consistency model could be violated.

---
### MTraceCheck: Validating Non-Deterministic Behavior of Memory Consistency Models in Post-Silicon Validation.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080235
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080235?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Multicore architectures, Hardware, Hardware validation, Post-manufacture validation and debug, Bug detection, localization and diagnosis, 
* **Abstract**: This work presents a minimally-intrusive, high-performance, post-silicon validation framework for validating memory consistency in multi-core systems. Our framework generates constrained-random tests that are instrumented with observability-enhancing code for memory consistency verification. For each test, we generate a set of compact signatures reflecting the memory-ordering patterns observed over many executions of the test, with each of the signatures corresponding to a unique memory-ordering pattern. We then leverage an efficient and novel analysis to quickly determine if the observed execution patterns represented by each unique signature abide by the memory consistency model. Our analysis derives its efficiency by exploiting the structural similarities among the patterns observed.

---
### Redundant Memory Array Architecture for Efficient Selective Protection.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080213
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080213?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Multicore architectures, Dependable and fault-tolerant systems and networks, Processors and memory architectures, Redundancy, Reliability, 
* **Abstract**: Memory hardware errors may result from transient particle-induced faults as well as device defects due to aging. These errors are an important threat to computer system reliability as VLSI technologies continue to scale. Managing memory hardware errors is a critical component in developing an overall system dependability strategy. Memory error detection and correction are supported in a range of available hardware mechanisms. However, memory protections (particularly the more advanced ones) come at substantial costs in performance and energy usage. Moreover, the protection mechanisms are often a fixed, system-wide choice and can not easily adapt to different protection demand of different applications or memory regions.

---
### Clank: Architectural Support for Intermittent Computation.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080238
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080238?download=true
* **Key Words**: Computer systems organization, Dependable and fault-tolerant systems and networks, Processors and memory architectures, Reliability, Embedded and cyber-physical systems, Embedded systems, Embedded hardware, Hardware, Emerging technologies, Memory and dense storage, 
* **Abstract**: The processors that drive embedded systems are getting smaller; meanwhile, the batteries used to provide power to those systems have stagnated. If we are to realize the dream of ubiquitous computing promised by the Internet of Things, processors must shed large, heavy, expensive, and high maintenance batteries and, instead, harvest energy from their environment. One challenge with this transition is that harvested energy is insufficient for continuous operation. Unfortunately, existing programs fail miserably when executed intermittently.

---
### MeRLiN: Exploiting Dynamic Instruction Behavior for Fast and Accurate Microarchitecture Level Reliability Assessment.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080225
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080225?download=true
* **Key Words**: Computer systems organization, Dependable and fault-tolerant systems and networks, Reliability, 
* **Abstract**: Early reliability assessment of hardware structures using microarchitecture level simulators can effectively guide major error protection decisions in microprocessor design. Statistical fault injection on microarchitectural structures modeled in performance simulators is an accurate method to measure their Architectural Vulnerability Factor (AVF) but requires excessively long campaigns to obtain high statistical significance.

---
### The Reach Profiler (REAPER): Enabling the Mitigation of DRAM Retention Failures via Profiling at Aggressive Conditions.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080242
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080242?download=true
* **Key Words**: 
* **Abstract**: Modern DRAM-based systems suffer from significant energy and latency penalties due to conservative DRAM refresh standards. Volatile DRAM cells can retain information across a wide distribution of times ranging from milliseconds to many minutes, but each cell is currently refreshed every 64ms to account for the extreme tail end of the retention time distribution, leading to a high refresh overhead. Due to poor DRAM technology scaling, this problem is expected to get worse in future device generations. Hence, the current approach of refreshing all cells with the worst-case refresh rate must be replaced with a more intelligent design.

---
### Quality of Service Support for Fine-Grained Sharing on GPUs.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080203
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080203?download=true
* **Key Words**: Applied computing, Enterprise computing, Enterprise information systems, Data centers, Computer systems organization, Architectures, Parallel architectures, Multiple instruction, multiple data, 
* **Abstract**: GPUs have been widely adopted in data centers to provide acceleration services to many applications. Sharing a GPU is increasingly important for better processing throughput and energy efficiency. However, quality of service (QoS) among concurrent applications is minimally supported. Previous efforts are too coarse-grained and not scalable with increasing QoS requirements. We propose QoS mechanisms for a fine-grained form of GPU sharing. Our QoS support can provide control over the progress of kernels on a per cycle basis and the amount of thread-level parallelism of each kernel. Due to accurate resource management, our QoS support has significantly better scalability compared with previous best efforts. Evaluations show that, when the GPU is shared by three kernels, two of which have QoS goals, the proposed techniques achieve QoS goals 43.8% more often than previous techniques and have 20.5% higher throughput.

---
### Accelerating GPU Hardware Transactional Memory with Snapshot Isolation.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080204
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080204?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Single instruction, multiple data, Computing methodologies, Concurrent computing methodologies, 
* **Abstract**: Snapshot Isolation (SI) is an established model in the database community, which permits write-read conflicts to pass and aborts transactions only on write-write conflicts. With the Write Skew anomaly correctly eliminated, SI can reduce the occurrence of aborts, save the work done by transactions, and greatly benefit long transactions involving complex data structures.

---
### Decoupled Affine Computation for SIMT GPUs.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080205
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080205?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Single instruction, multiple data, 
* **Abstract**: This paper introduces a method of decoupling affine computations---a class of expressions that produces extremely regular values across SIMT threads---from the main execution stream, so that the affine computations can be performed with greater efficiency and with greater independence from the main execution stream. This decoupling has two benefits: (1) For compute-bound programs, it significantly reduces the dynamic warp instruction count; (2) for memory-bound workloads, it significantly reduces memory latency, since it acts as a non-speculative prefetcher for the data specified by the many memory address calculations that are affine computations.

---
### Access Pattern-Aware Cache Management for Improving Data Utilization in GPU.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080239
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080239?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Single instruction, multiple data, 
* **Abstract**: Long latency of memory operation is a prominent performance bottleneck in graphics processing units (GPUs). The small data cache that must be shared across dozens of warps (a collection of threads) creates significant cache contention and premature data eviction. Prior works have recognized this problem and proposed warp throttling which reduces the number of active warps contending for cache space. In this paper we discover that individual load instructions in a warp exhibit four different types of data locality behavior: (1) data brought by a warp load instruction is used only once, which is classified as streaming data (2) data brought by a warp load is reused multiple times within the same warp, called intra-warp locality (3) data brought by a warp is reused multiple times but across different warps, called inter-warp locality (4) and some data exhibit both a mix of intra- and inter-warp locality. Furthermore, each load instruction exhibits consistently the same locality type across all warps within a GPU kernel. Based on this discovery we argue that cache management must be done using per-load locality type information, rather than applying warp-wide cache management policies. We propose Access Pattern-aware Cache Management (APCM), which dynamically detects the locality type of each load instruction by monitoring the accesses from one exemplary warp. APCM then uses the detected locality type to selectively apply cache bypassing and cache pinning of data based on load locality characterization. Using an extensive set of simulations we show that APCM improves performance of GPUs by 34% for cache sensitive applications while saving 27% of energy consumption over baseline GPU.

---
### MCM-GPU: Multi-Chip-Module GPUs for Continued Performance Scalability.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080231
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080231?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Single instruction, multiple data, Computing methodologies, Computer graphics, Graphics systems and interfaces, Graphics processors, 
* **Abstract**: Historically, improvements in GPU-based high performance computing have been tightly coupled to transistor scaling. As Moore's law slows down, and the number of transistors per die no longer grows at historical rates, the performance curve of single monolithic GPUs will ultimately plateau. However, the need for higher performing GPUs continues to exist in many domains. To address this need, in this paper we demonstrate that package-level integration of multiple GPU modules to build larger logical GPUs can enable continuous performance scaling beyond Moore's law. Specifically, we propose partitioning GPUs into easily manufacturable basic GPU Modules (GPMs), and integrating them on package using high bandwidth and power efficient signaling technologies. We lay out the details and evaluate the feasibility of a basic Multi-Chip-Module GPU (MCM-GPU) design. We then propose three architectural optimizations that significantly improve GPM data locality and minimize the sensitivity on inter-GPM bandwidth. Our evaluation shows that the optimized MCM-GPU achieves 22.8% speedup and 5x inter-GPM bandwidth reduction when compared to the basic MCM-GPU architecture. Most importantly, the optimized MCM-GPU design is 45.5% faster than the largest implementable monolithic GPU, and performs within 10% of a hypothetical (and unbuildable) monolithic GPU. Lastly we show that our optimized MCM-GPU is 26.8% faster than an equally equipped Multi-GPU system with the same total number of SMs and DRAM bandwidth.

---
### EDDIE: EM-Based Detection of Deviations in Program Execution.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080223
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080223?download=true
* **Key Words**: Security and privacy, Intrusion/anomaly detection and malware mitigation, 
* **Abstract**: This paper describes EM-Based Detection of Deviations in Program Execution (EDDIE), a new method for detecting anomalies in program execution, such as malware and other code injections, without introducing any overheads, adding any hardware support, changing any software, or using any resources on the monitored system itself. Monitoring with EDDIE involves receiving electromagnetic (EM) emanations that are emitted as a side effect of execution on the monitored system, and it relies on spikes in the EM spectrum that are produced as a result of periodic (e.g. loop) activity in the monitored execution. During training, EDDIE characterizes normal execution behavior in terms of peaks in the EM spectrum that are observed at various points in the program execution, but it does not need any characterization of the malware or other code that might later be injected. During monitoring, EDDIE identifies peaks in the observed EM spectrum, and compares these peaks to those learned during training. Since EDDIE requires no resources on the monitored machine and no changes to the monitored software, it is especially well suited for security monitoring of embedded and IoT devices. We evaluate EDDIE on a real IoT system and in a cycle-accurate simulator, and find that even relatively brief injected bursts of activity (a few milliseconds) are detected by EDDIE with high accuracy, and that it also accurately detects when even a few instructions are injected into an existing loop within the application.

---
### Secure Hierarchy-Aware Cache Replacement Policy (SHARP): Defending Against Cache-Based Side Channel Atacks.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080222
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080222?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Multicore architectures, Security and privacy, Security in hardware, Hardware attacks and countermeasures, Side-channel analysis and countermeasures, 
* **Abstract**: In cache-based side channel attacks, a spy that shares a cache with a victim probes cache locations to extract information on the victim's access patterns. For example, in evict+reload, the spy repeatedly evicts and then reloads a probe address, checking if the victim has accessed the address in between the two operations. While there are many proposals to combat these cache attacks, they all have limitations: they either hurt performance, require programmer intervention, or can only defend against some types of attacks.

---
### Lemonade from Lemons: Harnessing Device Wearout to Create Limited-Use Security Architectures.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080226
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080226?download=true
* **Key Words**: Security and privacy, Security in hardware, Hardware security implementation, 
* **Abstract**: Most architectures are designed to mitigate the usually undesirable phenomenon of device wearout. We take a contrarian view and harness this phenomenon to create hardware security mechanisms that resist attacks by statistically enforcing an upper bound on hardware uses, and consequently attacks. For example, let us assume that a user may log into a smartphone a maximum of 50 times a day for 5 years, resulting in approximately 91,250 legitimate uses. If we assume at least 8-character passwords and we require login (and retrieval of the storage decryption key) to traverse hardware that wears out in 91,250 uses, then an adversary has a negligible chance of successful brute-force attack before the hardware wears out, even assuming real-world password cracking by professionals. M-way replication of our hardware and periodic re-encryption of storage can increase the daily usage bound by a factor of M.

---
### LogCA: A High-Level Performance Model for Hardware Accelerators.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080216
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080216?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Heterogeneous (hybrid) systems, Computing methodologies, Modeling and simulation, Model development and analysis, Modeling methodologies, Hardware, Integrated circuits, Reconfigurable logic and FPGAs, Hardware accelerators, 
* **Abstract**: With the end of Dennard scaling, architects have increasingly turned to special-purpose hardware accelerators to improve the performance and energy efficiency for some applications. Unfortunately, accelerators don't always live up to their expectations and may under-perform in some situations. Understanding the factors which effect the performance of an accelerator is crucial for both architects and programmers early in the design stage. Detailed models can be highly accurate, but often require low-level details which are not available until late in the design cycle. In contrast, simple analytical models can provide useful insights by abstracting away low-level system details.

---
### Plasticine: A Reconfigurable Architecture For Parallel Paterns.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080256
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080256?download=true
* **Key Words**: Hardware, Integrated circuits, Reconfigurable logic and FPGAs, Hardware accelerators, Software and its engineering, Software notations and tools, Compilers, Retargetable compilers, 
* **Abstract**: Reconfigurable architectures have gained popularity in recent years as they allow the design of energy-efficient accelerators. Fine-grain fabrics (e.g. FPGAs) have traditionally suffered from performance and power inefficiencies due to bit-level reconfigurable abstractions. Both fine-grain and coarse-grain architectures (e.g. CGRAs) traditionally require low level programming and suffer from long compilation times. We address both challenges with Plasticine, a new spatially reconfigurable architecture designed to efficiently execute applications composed of parallel patterns. Parallel patterns have emerged from recent research on parallel programming as powerful, high-level abstractions that can elegantly capture data locality, memory access patterns, and parallelism across a wide range of dense and sparse applications.

---
### A Programmable Hardware Accelerator for Simulating Dynamical Systems.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080252
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080252?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Cellular architectures, Hardware, Emerging technologies, Analysis and design of emerging devices and systems, Emerging architectures, 
* **Abstract**: The fast and energy-efficient simulation of dynamical systems defined by coupled ordinary/partial differential equations has emerged as an important problem. The accelerated simulation of coupled ODE/PDE is critical for analysis of physical systems as well as computing with dynamical systems. This paper presents a fast and programmable accelerator for simulating dynamical systems. The computing model of the proposed platform is based on multilayer cellular nonlinear network (CeNN) augmented with nonlinear function evaluation engines. The platform can be programmed to accelerate wide classes of ODEs/PDEs by modulating the connectivity within the multilayer CeNN engine. An innovative hardware architecture including data reuse, memory hierarchy, and near-memory processing is designed to accelerate the augmented multilayer CeNN. A dataflow model is presented which is supported by optimized memory hierarchy for efficient function evaluation. The proposed solver is designed and synthesized in 15nm technology for the hardware analysis. The performance is evaluated and compared to GPU nodes when solving wide classes of differential equations and the power consumption is analyzed to show orders of magnitude improvement in energy efficiency.

---
### Stream-Dataflow Acceleration.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080255
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080255?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Data flow architectures, Heterogeneous (hybrid) systems, Reconfigurable computing, Special purpose systems, Parallel architectures, Single instruction, multiple data, 
* **Abstract**: Demand for low-power data processing hardware continues to rise inexorably. Existing programmable and "general purpose" solutions (eg. SIMD, GPGPUs) are insufficient, as evidenced by the order-of-magnitude improvements and industry adoption of application and domain-specific accelerators in important areas like machine learning, computer vision and big data. The stark tradeoffs between efficiency and generality at these two extremes poses a difficult question: how could domain-specific hardware efficiency be achieved without domain-specific hardware solutions?

---
### Hardware Translation Coherence for Virtualized Systems.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080211
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080211?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Heterogeneous (hybrid) systems, Software and its engineering, Software organization and properties, Contextual software domains, Operating systems, Memory management, Virtual memory, Software infrastructure, Virtual machines, 
* **Abstract**: To improve system performance, operating systems (OSes) often undertake activities that require modification of virtual-to-physical address translations. For example, the OS may migrate data between physical pages to manage heterogeneous memory devices. We refer to such activities as page remappings. Unfortunately, page remappings are expensive. We show that a big part of this cost arises from address translation coherence, particularly on systems employing virtualization. In response, we propose hardware translation invalidation and coherence or HATRIC, a readily implementable hardware mechanism to piggyback translation coherence atop existing cache coherence protocols. We perform detailed studies using KVM-based virtualization, showing that HATRIC achieves up to 30% performance and 10% energy benefits, for per-CPU area overheads of 0.2%. We also quantify HATRIC's benefits on systems running Xen and find up to 33% performance improvements.

---
### Hybrid TLB Coalescing: Improving TLB Translation Coverage under Diverse Fragmented Memory Allocations.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080217
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080217?download=true
* **Key Words**: Computer systems organization, Architectures, Software and its engineering, Software organization and properties, Contextual software domains, Operating systems, Memory management, Allocation / deallocation strategies, Virtual memory, 
* **Abstract**: To mitigate excessive TLB misses in large memory applications, techniques such as large pages, variable length segments, and HW coalescing, increase the coverage of limited hardware translation entries by exploiting the contiguous memory allocation. However, recent studies show that in non-uniform memory systems, using large pages often leads to performance degradation, or allocating large chunks of memory becomes more difficult due to memory fragmentation. Although each of the prior techniques favors its own best chunk size, diverse contiguity of memory allocation in real systems cannot always provide the optimal chunk of each technique. Under such fragmented and diverse memory allocations, this paper proposes a novel HW-SW hybrid translation architecture, which can adapt to different memory mappings efficiently. In the proposed hybrid coalescing technique, the operating system encodes memory contiguity information in a subset of page table entries, called anchor entries. During address translation through TLBs, an anchor entry provides translation for contiguous pages following the anchor entry. As a smaller number of anchor entries can cover a large portion of virtual address space, the efficiency of TLB can be significantly improved. The most important benefit of hybrid coalescing is its ability to change the coverage of the anchor entry dynamically, reflecting the current allocation contiguity status. By using the contiguity information directly set by the operating system, the technique can provide scalable translation coverage improvements with minor hardware changes, while allowing the flexibility of memory allocation. Our experimental results show that across diverse allocation scenarios with different distributions of contiguous memory chunks, the proposed scheme can effectively reap the potential translation coverage improvement from the existing contiguity.

---
### Do-It-Yourself Virtual Memory Translation.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080209
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080209?download=true
* **Key Words**: Software and its engineering, Software organization and properties, Contextual software domains, Operating systems, Memory management, Virtual memory, Software infrastructure, Virtual machines, 
* **Abstract**: In this paper, we introduce the Do-It-Yourself virtual memory translation (DVMT) architecture as a flexible complement for current hardware-fixed translation flows. DVMT decouples the virtual-to-physical mapping process from the access permissions, giving applications freedom in choosing mapping schemes, while maintaining security within the operating system. Furthermore, DVMT is designed to support virtualized environments, as a means to collapse the costly, hardware-assisted two-dimensional translations. We describe the architecture in detail and demonstrate its effectiveness by evaluating several different DVMT schemes on a range of virtualized applications with a model based on measurements from a commercial system. We show that different DVMT configurations preserve the native performance, while achieving speedups of 1.2x to 2.0x in virtualized environments.

---
### Rethinking TLB Designs in Virtualized Environments: A Very Large Part-of-Memory TLB.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080210
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080210?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Heterogeneous (hybrid) systems, 
* **Abstract**: With increasing deployment of virtual machines for cloud services and server applications, memory address translation overheads in virtualized environments have received great attention. In the radix-4 type of page tables used in x86 architectures, a TLB-miss necessitates up to 24 memory references for one guest to host translation. While dedicated page walk caches and such recent enhancements eliminate many of these memory references, our measurements on the Intel Skylake processors indicate that many programs in virtualized mode of execution still spend hundreds of cycles for translations that do not hit in the TLBs.

---
### Language-level persistency.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080229
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080229?download=true
* **Key Words**: Computer systems organization, Architectures, Software and its engineering, Software notations and tools, 
* **Abstract**: The commercial release of byte-addressable persistent memories, such as Intel/Micron 3D XPoint memory, is imminent. Ongoing research has sought mechanisms to allow programmers to implement recoverable data structures in these new main memories. Ensuring recoverability requires programmer control of the order of persistent stores; recent work proposes persistency models as an extension to memory consistency to specify such ordering. Prior work has considered persistency models at the abstraction of the instruction set architecture. Instead, we argue for extending the language-level memory model to provide guarantees on the order of persistent writes.

---
### ShortCut: Architectural Support for Fast Object Access in Scripting Languages.
* **Publisher**: ACM
* **DOI**: https://doi.org/10.1145/3079856.3080237
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080237?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, High-level language architectures, Software and its engineering, Software notations and tools, Compilers, Just-in-time compilers, Context specific languages, Scripting languages, 
* **Abstract**: The same flexibility that makes dynamic scripting languages appealing to programmers is also the primary cause of their low performance. To access objects of potentially different types, the compiler creates a dispatcher with a series of if statements, each performing a comparison to a type and a jump to a handler. This induces major overhead in instructions executed and branches mispredicted.

---
### Architectural Support for Server-Side PHP Processing.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080234
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080234?download=true
* **Key Words**: Computer systems organization, Architectures, Distributed architectures, Cloud computing, Other architectures, Special purpose systems, 
* **Abstract**: PHP is the dominant server-side scripting language used to implement dynamic web content. Just-in-time compilation, as implemented in Facebook's state-of-the-art HipHopVM, helps mitigate the poor performance of PHP, but substantial overheads remain, especially for realistic, large-scale PHP applications. This paper analyzes such applications and shows that there is little opportunity for conventional microarchitectural enhancements. Furthermore, prior approaches for function-level hardware acceleration present many challenges due to the extremely flat distribution of execution time across a large number of functions in these complex applications. In-depth analysis reveals a more promising alternative: targeted acceleration of four fine-grained PHP activities: hash table accesses, heap management, string manipulation, and regular expression handling. We highlight a set of guiding principles and then propose and evaluate inexpensive hardware accelerators for these activities that accrue substantial performance and energy gains across dozens of functions. Our results reflect an average 17.93% improvement in performance and 21.01% reduction in energy while executing these complex PHP workloads on a state-of-the-art software and hardware platform.

---
### HeteroOS: OS Design for Heterogeneous Memory Management in Datacenter.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080245
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080245?download=true
* **Key Words**: Computer systems organization, Dependable and fault-tolerant systems and networks, Processors and memory architectures, Hardware, Integrated circuits, Semiconductor memory, Non-volatile memory, Software and its engineering, Software organization and properties, Contextual software domains, Operating systems, Memory management, Main memory, Virtual memory, Software infrastructure, Virtual machines, 
* **Abstract**: Heterogeneous memory management combined with server virtualization in datacenters is expected to increase the software and OS management complexity. State-of-the-art solutions rely exclusively on the hypervisor (VMM) for expensive page hotness tracking and migrations, limiting the benefits from heterogeneity. To address this, we design HeteroOS, a novel application-transparent OS-level solution for managing memory heterogeneity in virtualized system. The HeteroOS design first makes the guest-OSes heterogeneity-aware and then extracts rich OS-level information about applications' memory usage to place data in the 'right' memory avoiding page migrations. When such pro-active placements are not possible, HeteroOS combines the power of the guest-OSes' information about applications with the VMM's hardware control to track for hotness and migrate only performance-critical pages. Finally, HeteroOS also designs an efficient heterogeneous memory sharing across multiple guest-VMs. Evaluation of HeteroOS with memory, storage, and network-intensive datacenter applications shows up to 2x performance improvement compared to the state-of-the-art VMM-exclusive approach.

---
### Maximizing CNN Accelerator Efficiency Through Resource Partitioning.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080221
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080221?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Neural networks, Reconfigurable computing, 
* **Abstract**: Convolutional neural networks (CNNs) are revolutionizing machine learning, but they present significant computational challenges. Recently, many FPGA-based accelerators have been proposed to improve the performance and efficiency of CNNs. Current approaches construct a single processor that computes the CNN layers one at a time; the processor is optimized to maximize the throughput at which the collection of layers is computed. However, this approach leads to inefficient designs because the same processor structure is used to compute CNN layers of radically varying dimensions.

---
### Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080215
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080215?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Computing methodologies, Machine learning, Parallel computing methodologies, 
* **Abstract**: As the size of Deep Neural Networks (DNNs) continues to grow to increase accuracy and solve more complex problems, their energy footprint also scales. Weight pruning reduces DNN model size and the computation by removing redundant weights. However, we implemented weight pruning for several popular networks on a variety of hardware platforms and observed surprising results. For many networks, the network sparsity caused by weight pruning will actually hurt the overall performance despite large reductions in the model size and required multiply-accumulate operations. Also, encoding the sparse format of pruned networks incurs additional storage space overhead. To overcome these challenges, we propose Scalpel that customizes DNN pruning to the underlying hardware by matching the pruned network structure to the data-parallel hardware organization. Scalpel consists of two techniques: SIMD-aware weight pruning and node pruning. For low-parallelism hardware (e.g., microcontroller), SIMD-aware weight pruning maintains weights in aligned fixed-size groups to fully utilize the SIMD units. For high-parallelism hardware (e.g., GPU), node pruning removes redundant nodes, not redundant weights, thereby reducing computation without sacrificing the dense matrix format. For hardware with moderate parallelism (e.g., desktop CPU), SIMD-aware weight pruning and node pruning are synergistically applied together. Across the microcontroller, CPU and GPU, Scalpel achieves mean speedups of 3.54x, 2.61x, and 1.25x while reducing the model sizes by 88%, 82%, and 53%. In comparison, traditional weight pruning achieves mean speedups of 1.90x, 1.06x, 0.41x across the three platforms.

---
### Understanding and Optimizing Asynchronous Low-Precision Stochastic Gradient Descent.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080248
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080248?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Reconfigurable computing, Parallel architectures, Multicore architectures, Computing methodologies, Machine learning, Machine learning algorithms, Mathematics of computing, Mathematical analysis, Mathematical optimization, Continuous optimization, 
* **Abstract**: Stochastic gradient descent (SGD) is one of the most popular numerical algorithms used in machine learning and other domains. Since this is likely to continue for the foreseeable future, it is important to study techniques that can make it run fast on parallel hardware. In this paper, we provide the first analysis of a technique called Buck-wild! that uses both asynchronous execution and low-precision computation. We introduce the DMGC model, the first conceptualization of the parameter space that exists when implementing low-precision SGD, and show that it provides a way to both classify these algorithms and model their performance. We leverage this insight to propose and analyze techniques to improve the speed of low-precision SGD. First, we propose software optimizations that can increase throughput on existing CPUs by up to 11X. Second, we propose architectural changes, including a new cache technique we call an obstinate cache, that increase throughput beyond the limits of current-generation hardware. We also implement and analyze low-precision SGD on the FPGA, which is a promising alternative to the CPU for future SGD systems.

---
### Aggressive Pipelining of Irregular Applications on Reconfigurable Hardware.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080228
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080228?download=true
* **Key Words**: Computer systems organization, Architectures, Other architectures, Reconfigurable computing, Computing methodologies, Parallel computing methodologies, Parallel programming languages, Hardware, Integrated circuits, Reconfigurable logic and FPGAs, Hardware accelerators, 
* **Abstract**: CPU-FPGA heterogeneous platforms offer a promising solution for high-performance and energy-efficient computing systems by providing specialized accelerators with post-silicon reconfigurability. To unleash the power of FPGA, however, the programmability gap has to be filled so that applications specified in high-level programming languages can be efficiently mapped and scheduled on FPGA. The above problem is even more challenging for irregular applications, in which the execution dependency can only be determined at run time. Thus over-serialized accelerators are generated from existing works that rely on compile time analysis to schedule the computation.

---
### Fractal: An Execution Model for Fine-Grain Nested Speculative Parallelism.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080218
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080218?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Multicore architectures, 
* **Abstract**: Most systems that support speculative parallelization, like hardware transactional memory (HTM), do not support nested parallelism. This sacrifices substantial parallelism and precludes composing parallel algorithms. And the few HTMs that do support nested parallelism focus on parallelizing at the coarsest (shallowest) levels, incurring large overheads that squander most of their potential.

---
### Parallel Automata Processor.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080207
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080207?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Hardware, Emerging technologies, Analysis and design of emerging devices and systems, Emerging architectures, Theory of computation, Formal languages and automata theory, 
* **Abstract**: Finite State Machines (FSM) are widely used computation models for many application domains. These embarrassingly sequential applications with irregular memory access patterns perform poorly on conventional von-Neumann architectures. The Micron Automata Processor (AP) is an in-situ memory-based computational architecture that accelerates non-deterministic finite automata (NFA) processing in hardware. However, each FSM on the AP is processed sequentially, limiting potential speedups.

---
### Viyojit: Decoupling Battery and DRAM Capacities for Battery-Backed DRAM.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080236
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080236?download=true
* **Key Words**: Hardware, Communication hardware, interfaces and storage, External storage, Power and energy, Energy generation and storage, Batteries, Power estimation and optimization, Enterprise level and data centers power issues, 
* **Abstract**: Non-Volatile Memories (NVMs) can significantly improve the performance of data-intensive applications. A popular form of NVM is Battery-backed DRAM, which is available and in use today with DRAMs latency and without the endurance problems of emerging NVM technologies. Modern servers can be provisioned with up-to 4 TB of DRAM, and provisioning battery backup to write out such large memories is hard because of the large battery sizes and the added hardware and cooling costs. We present Viyojit, a system that exploits the skew in write working sets of applications to provision substantially smaller batteries while still ensuring durability for the entire DRAM capacity. Viyojit achieves this by bounding the number of dirty pages in DRAM based on the provisioned battery capacity and proactively writing out infrequently written pages to an SSD. Even for write-heavy workloads with less skew than we observe in analysis of real data center traces, Viyojit reduces the required battery capacity to 11% of the original size, with a performance overhead of 7-25%. Thus, Viyojit frees battery-backed DRAM from stunted growth of battery capacities and enables servers with terabytes of battery-backed DRAM.

---
### DICE: Compressing DRAM Caches for Bandwidth and Capacity.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080243
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080243?download=true
* **Key Words**: Hardware, Emerging technologies, Memory and dense storage, 
* **Abstract**: This paper investigates compression for DRAM caches. As the capacity of DRAM cache is typically large, prior techniques on cache compression, which solely focus on improving cache capacity, provide only a marginal benefit. We show that more performance benefit can be obtained if the compression of the DRAM cache is tailored to provide higher bandwidth. If a DRAM cache can provide two compressed lines in a single access, and both lines are useful, the effective bandwidth of the DRAM cache would double. Unfortunately, it is not straight-forward to compress DRAM caches for bandwidth. The typically used Traditional Set Indexing (TSI) maps consecutive lines to consecutive sets, so the multiple compressed lines obtained from the set are from spatially distant locations and unlikely to be used within a short period of each other. We can change the indexing of the cache to place consecutive lines in the same set to improve bandwidth; however, when the data is incompressible, such spatial indexing reduces effective capacity and causes significant slowdown.

---
### The Mondrian Data Engine.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080233
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080233?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Single instruction, multiple data, Dependable and fault-tolerant systems and networks, Processors and memory architectures, Hardware, Very large scale integration design, VLSI packaging, Die and wafer stacking, Information systems, Data management systems, Database management system engines, Main memory engines, 
* **Abstract**: The increasing demand for extracting value out of ever-growing data poses an ongoing challenge to system designers, a task only made trickier by the end of Dennard scaling. As the performance density of traditional CPU-centric architectures stagnates, advancing compute capabilities necessitates novel architectural approaches. Near-memory processing (NMP) architectures are reemerging as promising candidates to improve computing efficiency through tight coupling of logic and memory. NMP architectures are especially fitting for data analytics, as they provide immense bandwidth to memory-resident data and dramatically reduce data movement, the main source of energy consumption.

---
### Jenga: Software-Defined Cache Hierarchies.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080214
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080214?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Multicore architectures, 
* **Abstract**: Caches are traditionally organized as a rigid hierarchy, with multiple levels of progressively larger and slower memories. Hierarchy allows a simple, fixed design to benefit a wide range of applications, since working sets settle at the smallest (i.e., fastest and most energy-efficient) level they fit in. However, rigid hierarchies also add overheads, because each level adds latency and energy even when it does not fit the working set. These overheads are expensive on emerging systems with heterogeneous memories, where the differences in latency and energy across levels are small. Significant gains are possible by specializing the hierarchy to applications.

---
### APPROX-NoC: A Data Approximation Framework for Network-On-Chip Architectures.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080241
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080241?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Interconnection architectures, Multicore architectures, Networks, Network performance evaluation, Network performance analysis, 
* **Abstract**: The trend of unsustainable power consumption and large memory bandwidth demands in massively parallel multicore systems, with the advent of the big data era, has brought upon the onset of alternate computation paradigms utilizing heterogeneity, specialization, processor-in-memory and approximation. Approximate Computing is being touted as a viable solution for high performance computation by relaxing the accuracy constraints of applications. This trend has been accentuated by emerging data intensive applications in domains like image/video processing, machine learning and big data analytics that allow inaccurate outputs within an acceptable variance. Leveraging relaxed accuracy for high throughput in Networks-on-Chip (NoCs), which have rapidly become the accepted method for connecting a large number of on-chip components, has not yet been explored. We propose APPROX-NoC, a hardware data approximation framework with an online data error control mechanism for high performance NoCs. APPROX-NoC facilitates approximate matching of data patterns, within a controllable value range, to compress them thereby reducing the volume of data movement across the chip.

---
### There and Back Again: Optimizing the Interconnect in Networks of Memory Cubes.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080251
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080251?download=true
* **Key Words**: Hardware, Emerging technologies, Analysis and design of emerging devices and systems, Emerging architectures, Emerging interfaces, Memory and dense storage, 
* **Abstract**: High-performance computing, enterprise, and datacenter servers are driving demands for higher total memory capacity as well as memory performance. Memory "cubes" with high per-package capacity (from 3D integration) along with high-speed point-to-point interconnects provide a scalable memory system architecture with the potential to deliver both capacity and performance. Multiple such cubes connected together can form a "Memory Network" (MN), but the design space for such MNs is quite vast, including multiple topology types and multiple memory technologies per memory cube.

---
### Footprint: Regulating Routing Adaptiveness in Networks-on-Chip.
* **Publisher**: ACM
* **DOI**: https://dl.acm.org/citation.cfm?id=3080249
* **PDF**: https://dl.acm.org/doi/pdf/10.1145/3079856.3080249?download=true
* **Key Words**: Computer systems organization, Architectures, Parallel architectures, Interconnection architectures, 
* **Abstract**: Routing algorithms can improve network performance by maximizing routing adaptiveness but can be problematic in the presence of endpoint congestion. Tree-saturation is a well-known behavior caused by endpoint congestion. Adaptive routing can, however, spread the congestion and result in thick branches of the congestion tree -- creating Head-of-Line (HoL) blocking and degrading performance. In this work, we identify how ignoring virtual channels (VCs) and their occupancy during adaptive routing results in congestion trees with thick branches as congestion is spread to all VCs. To address this limitation, we propose Footprint routing algorithm -- a new adaptive routing algorithm that minimizes the size of the congestion tree, both in terms of the number of nodes in the congestion tree as well as branch thickness. Footprint achieves this by regulating adaptiveness by requiring packets to follow the path of prior packets to the same destination if the network is congested instead of forking a new path or VC. Thus, the congestion tree is dynamically kept as slim as possible and reduces HoL blocking or congestion spreading while maintaining high adaptivity and maximizing VC buffer utilization. We evaluate the proposed Footprint routing algorithm against other adaptive routing algorithms and our simulation results show that the network saturation throughput can be improved by up to 43% (58%) compared with the fully adaptive routing (partially adaptive routing) algorithms.
