### hpca 2016 | 57 papers.
---
### Memristive Boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446049
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446049
* **Key Words**: Hardware, Machine learning, Computational modeling, Arrays, Simulated annealing, Training, application program interfaces, Boltzmann machines, Boolean algebra, computability, field programmable gate arrays, graph theory, graphics processing units, learning (artificial intelligence), mathematics computing, message passing, multiprocessing systems, optimisation, parallel processing, resistive RAM, parallel computational model, combinatorial optimization problems, deep machine learning models, high performance implementations, GPU, MPI-based HPC clusters, FPGAs, all-to-all communication, hardware accelerators, large-scale combinatorial optimization, memristive Boltzmann machines, massively parallel memory-centric hardware accelerator, resistive RAM technology, RRAM technology, electrical properties, fine-grained parallel computation, memory arrays, graph partitioning, boolean satisfiability, deep belief network application, multicore system, memristive accelerator, RRAM based processing-in-memory system, RRAM PIM system, 
* **Abstract**: The Boltzmann machine is a massively parallel computational model capable of solving a broad class of combinatorial optimization problems. In recent years, it has been successfully applied to training deep machine learning models on massive datasets. High performance implementations of the Boltzmann machine using GPUs, MPI-based HPC clusters, and FPGAs have been proposed in the literature. Regrettably, the required all-to-all communication among the processing units limits the performance of these efforts. This paper examines a new class of hardware accelerators for large-scale combinatorial optimization and deep learning based on memristive Boltzmann machines. A massively parallel, memory-centric hardware accelerator is proposed based on recently developed resistive RAM (RRAM) technology. The proposed accelerator exploits the electrical properties of RRAm to realize in situ, fine-grained parallel computation within memory arrays, thereby eliminating the need for exchanging data between the memory cells and the computational units. Two classical optimization problems, graph partitioning and boolean satisfiability, and a deep belief network application are mapped onto the proposed hardware. As compared to a multicore system, the proposed accelerator achieves 57x higher performance and 25x lower energy with virtually no loss in the quality of the solution to the optimization problems. The memristive accelerator is also compared against an RRAM based processing-in-memory (PIM) system, with respective performance and energy improvements of 6.89x and 5.2x.

---
### TABLA: A unified template-based framework for accelerating statistical machine learning.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446050
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446050
* **Key Words**: Machine learning algorithms, Field programmable gate arrays, Linear programming, Stochastic processes, Algorithm design and analysis, Hardware, Data models, field programmable gate arrays, graphics processing units, learning (artificial intelligence), multiprocessing systems, stochastic programming, TABLA framework, unified template-based framework, statistical machine learning, ML algorithm, compute-intensive applications, field programmable gate arrays, FPGA, machine learning algorithms, ASIC, application specific integrated circuits, general-purpose processors, accelerators generation, stochastic optimization problem, gradient descent solver, objective function, multicore CPU, many-core GPU, TABLA-generated accelerators, 
* **Abstract**: A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. While the demand for these compute-intensive applications is growing, the performance benefits from general-purpose platforms are diminishing. Field Programmable Gate Arrays (FPGAs) provide a promising path forward to accommodate the needs of machine learning algorithms and represent an intermediate point between the efficiency of ASICs and the programmability of general-purpose processors. However, acceleration with FPGAs still requires long development cycles and extensive expertise in hardware design. To tackle this challenge, instead of designing an accelerator for a machine learning algorithm, we present TABLA, a framework that generates accelerators for a class of machine learning algorithms. The key is to identify the commonalities across a wide range of machine learning algorithms and utilize this commonality to provide a high-level abstraction for programmers. TABLA leverages the insight that many learning algorithms can be expressed as a stochastic optimization problem. Therefore, learning becomes solving an optimization problem using stochastic gradient descent that minimizes an objective function over the training data. The gradient descent solver is fixed while the objective function changes for different learning algorithms. TABLA provides a template-based framework to accelerate this class of learning algorithms. Therefore, a developer can specify the learning task by only expressing the gradient of the objective function using our high-level language. Tabla then automatically generates the synthesizable implementation of the accelerator for FPGA realization using a set of hand-optimized templates. We use Tabla to generate accelerators for ten different learning tasks targeted at a Xilinx Zynq FPGA platform. We rigorously compare the benefits of FPGA acceleration to multi-core CPUs (ARM Cortex A15 and Xeon E3) and many-core GP...

---
### Pushing the limits of accelerator efficiency while retaining programmability.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446051
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446051
* **Key Words**: Hardware, Concurrent computing, Engines, Algorithm design and analysis, Multicore processing, Out of order, concurrency control, multiprocessing systems, parallel architectures, accelerator efficiency analysis, programmability, domain specific accelerators, domain volatility, design cost, verification cost, programmable architecture, performance analysis, energy analysis, area efficiency analysis, DSA-based approach, concurrency principle, computation principle, communication principle, data-reuse principle, coordination principle, microarchitectural mechanism, LSSD architecture, configurable spatial architecture, scratchpads, DMA, programmable-specialized architecture, domain-specific approach, 
* **Abstract**: The waning benefits of device scaling have caused a push towards domain specific accelerators (DSAs), which sacrifice programmability for efficiency. While providing huge benefits, DSAs are prone to obsoletion due to domain volatility, have recurring design and verification costs, and have large area footprints when multiple DSAs are required in a single device. Because of the benefits of generality, this work explores how far a programmable architecture can be pushed, and whether it can come close to the performance, energy, and area efficiency of a DSA-based approach. Our insight is that DSAs employ common specialization principles for concurrency, computation, communication, data-reuse and coordination, and that these same principles can be exploited in a programmable architecture using a composition of known microarchitectural mechanisms. Specifically, we propose and study an architecture called LSSD, which is composed of many low-power and tiny cores, each having a configurable spatial architecture, scratchpads, and DMA. Our results show that a programmable, specialized architecture can indeed be competitive with a domain-specific approach. Compared to four prominent and diverse DSAs, LSSD can match the DSAs' 10× to 150× speedup over an OOO core, with only up to 4× more area and power than a single DSA, while retaining programmability.

---
### A low power software-defined-radio baseband processor for the Internet of Things.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446052
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446052
* **Key Words**: Standards, Kernel, Finite impulse response filters, Baseband, Wireless communication, Wireless sensor networks, Synchronization, Bluetooth, Internet of Things, parallel processing, power consumption, quadrature phase shift keying, radio transceivers, signal processing, software radio, telecommunication power management, Zigbee, low power software defined radio baseband processor, Internet of Things, SDR baseband processor design, IoT, microarchitecture design, custom SIMD execution model, architectural optimization, flexible reduction network, voltage scaling, general purpose SDR SIMD architecture, power consumption, SPICE simulation, elementary signal processing kernels, end-to-end upper bound system, IEEE 802.15.4-OQPSK, Bluetooth low energy, control plane MCU, wireless transceiver, wireless system, ALU, 
* **Abstract**: In this paper, we define a configurable Software Defined Radio (SDR) baseband processor design for the Internet of Things (IoT). We analyzed the fundamental algorithms in communications systems on IoT devices to enable a microarchitecture design that supports many IoT standards and custom nonstandard communications. Based on this analysis, we propose a custom SIMD execution model coupled with a scalar unit. We introduce several architectural optimizations to this design: streaming registers, variable bit width datapath, dedicated ALUs for critical kernels, and an optimized flexible reduction network. We employ voltage scaling and clock gating to further reduce the power, while more than a 100% time margin has been reserved for reliable operation in the near-threshold region. Together our architectural enhancements lead to a 71× power reduction compared to a classic general purpose SDR SIMD architecture. Our IoT SDR datapath has sub-mW power consumption based on SPICE simulation, and is placed and routed to fit within an area of 0.074mm 2 in a 28nm process. We implemented several essential elementary signal processing kernels and combined them to demonstrate two end-to-end upper bound systems, 802.15.4-OQPSK and Bluetooth Low Energy. Our full SDR baseband system consists of a configurable SIMD with a control plane MCU and memory. For comparison, the best commercial wireless transceiver consumes 23.8mW for the entire wireless system (digital/RF/ analog). We show that our digital system power is below 2mW, in other words only 8% of the total system power. The wireless system is dominated by RF/analog power comsumption, thus the price of flexibility that SDR affords is small. We believe this work is unique in demonstrating the value of baseband SDR in the low power IoT domain.

---
### Improving smartphone user experience by balancing performance and energy with probabilistic QoS guarantee.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446053
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446053
* **Key Words**: Quality of service, Frequency control, Time-frequency analysis, Mobile communication, Optimization, Complexity theory, Mobile applications, Internet, Linux, mobile computing, optimisation, power aware computing, probability, quality of service, smart phones, smartphone user experience, probabilistic QoS guarantee, performance-energy balance, Web-based application, energy efficiency, user satisfaction, stochastic quantities, optimization problem, optimal energy control, mobile applications, modern smartphone devices, statistical models, lightweight controller, Qualcomm Snapdragon 8074 mobile chipset, Linux governors, Web page load time, 
* **Abstract**: User satisfaction is pivotal to the success of a mobile application. A recent study has shown that 49% of users would abandon a web-based application if it failed to load within 10 seconds. At the same time, it is imperative to maximize energy efficiency to ensure maximum usage of the limited energy source available to smartphones while maintaining the necessary levels of user satisfaction. An important factor to consider, that has been previously neglected, is variability of execution times of an application, requiring them to be modeled as stochastic quantities. This changes the nature of the objective function and the constraints of the underlying optimization problem. In this paper, we present a new approach to optimal energy control of mobile applications running on modern smartphone devices, focusing on the need to ensure a specified level of user satisfaction. The proposed statistical models address both single and multi-stage applications and are used in the formulation of an optimization problem, the solution to which is a static, lightweight controller that optimizes energy efficiency of mobile applications, subject to constraints on the likelihood that the application execution time meets a given deadline. We demonstrate the proposed models and the corresponding optimization method on three common mobile applications running on a real Qualcomm Snapdragon 8074 mobile chipset. The results show that the proposed statistical estimates of application execution times are within 99.34% of the measured values. Additionally, on the actual Qualcomm Snapdragon 8074 mobile chipset, the proposed control scheme achieves a 29% power savings over commonly-used Linux governors while maintaining an average web page load time of 2 seconds with a likelihood of 90%.

---
### Mobile CPU's rise to power: Quantifying the impact of generational mobile CPU design trends on performance, energy, and user satisfaction.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446054
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446054
* **Key Words**: Mobile communication, Market research, Multicore processing, Hardware, Benchmark testing, Power demand, Technological innovation, microprocessor chips, mobile computing, performance evaluation, power aware computing, quality of experience, mobile CPU design trends, performance, energy, hardware design, user satisfaction trends, Amazon Mechanical Turk service, user experience quality, technology scaling, desktop-like scaling, mobile computing, 
* **Abstract**: In this paper, we assess the past, present, and future of mobile CPU design. We study how mobile CPU designs trends have impacted the end-user, hardware design, and the holistic mobile device. We analyze the evolution often cutting-edge mobile CPU designs released over the past seven years. Specifically, we report measured performance, power, energy and user satisfaction trends across mobile CPU generations. A key contribution of our work is that we contextualize the mobile CPU's evolution in terms of user satisfaction, which has largely been absent from prior mobile hardware studies. To bridge the gap between mobile CPU design and user satisfaction, we construct and conduct a novel crowdsourcing study that spans over 25,000 survey participants using the Amazon Mechanical Turk service. Our methodology allows us to identify what mobile CPU design techniques provide the most benefit to the end-user's quality of user experience. Our results quantitatively demonstrate that CPUs play a crucial role in modern mobile system-on-chips (SoCs). Over the last seven years, both single-and multicore performance improvements have contributed to end-user satisfaction by reducing user-critical application response latencies. Mobile CPUs aggressively adopted many power-hungry desktop-oriented design techniques to reach these performance levels. Unlike other smartphone components (e.g. display and radio) whose peak power consumption has decreased over time, the mobile CPU's peak power consumption has steadily increased. As the limits of technology scaling restrict the ability of desktop-like scaling to continue for mobile CPUs, specialized accelerators appear to be a promising alternative that can help sustain the power, performance, and energy improvements that mobile computing necessitates. Such a paradigm shift will redefine the role of the CPU within future SoCs, which merit several design considerations based on our findings.

---
### Atomic persistence for SCM with a non-intrusive backend controller.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446055
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446055
* **Key Words**: Random access memory, Nonvolatile memory, Software, Hardware, Computer crashes, Retirement, Libraries, firmware, storage management, atomic persistence, SCM, storage class memory, nonintrusive backend memory controller, nonvolatile byte-addressable memory, instruction-grained direct access, persistent data, lightweight failure atomicity, synchronous persistent memory operations, concurrency-driven atomicity, front-end cache hierarchy, firmware managed volatile space, 
* **Abstract**: Non-volatile byte-addressable memory has the potential to revolutionize system architecture by providing instruction-grained direct access to vast amounts of persistent data. We describe a non-intrusive memory controller that uses backend operations for achieving lightweight failure atomicity. By moving synchronous persistent memory operations to the background, the performance overheads are minimized. Our solution avoids costly software intervention by decoupling isolation and concurrency-driven atomicity from failure atomicity and durability, and does not require changes to the front-end cache hierarchy. Two implementation alternatives - one using a hardware structure, and the other extending the memory controller with a firmware managed volatile space - are described. Our results show the performance is significantly better than traditional approaches.

---
### CompEx: Compression-expansion coding for energy, latency, and lifetime improvements in MLC/TLC NVM.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446056
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446056
* **Key Words**: Nonvolatile memory, Encoding, Resistance, Programming, Codecs, Random access memory, Benchmark testing, block codes, data compression, iterative methods, linear codes, random-access storage, compression-expansion coding, energy improvement, latency improvement, lifetime improvement, MLC/TLC NVM, multilevel/triple-level cell nonvolatile memories, PCM, active research and development, DRAM, memory capacity, iterative program-and-verify procedure, NVM cell programming, CompEx coding, statistical compression, frequent pattern compression, FPC, base-delta-immediate compression, BΔI compression, expansion codes, q-ary linear block codes, data encoding, q-ary NVM cell, TLC RRAM, high performance computing systems, 
* **Abstract**: Multi-level/triple-level cell non-volatile memories (MLC/TLC NVMs) such as PCM and RRAM are the subject of active research and development as replacement candidates for DRAM, which is limited by its high refresh power and poor scaling potential. Besides the benefits of non-volatility (low refresh power) and improved scalability, MLC/TLC NVMs offer high data density and memory capacity over DRAM. However, the viability of MLC/TLC NVMs is limited primarily due to the high programming energy and latency as well as the low endurance of NVM cells; these are primarily attributable to the iterative program-and-verify procedure necessary for programming the NVM cells. In this paper, we propose compression-expansion (CompEx) coding, a low overhead scheme that synergistically integrates statistical compression with expansion coding to realize simultaneous energy, latency, and lifetime improvements in MLC/TLC NVMs. CompEx coding is agnostic to the choice of compression technique; in this paper, we evaluate CompEx coding using both frequent pattern compression (FPC) and base-delta-immediate (BΔI) compression. CompEx coding integrates FPC/BΔI with (k, m)q `expansion' coding; expansion codes are a class of q-ary linear block codes that encode data using only the low energy states of a q-ary NVM cell. CompEx coding simultaneously reduces energy and latency and improves lifetime for no memory overhead and negligible logic overhead (≈ 10k gates, which is <; 0.1% per NVM module). Our full-system simulations of a system that integrates TLC RRAM show that CompEx coding reduces total memory energy by 53% and write latency by 24%; these improvements translate to a 5.7% improvement in IPC, a 11.8% improvement in main memory bandwidth, and 1.8× improvement in lifetime over classical binary coding using data-comparison write. CompEx coding thus addresses the programming energy/latency and lifetime challenges of MLC/-TLC NVMs that pose a serious technological roadblock to their adoption in h...

---
### A low-power hybrid reconfigurable architecture for resistive random-access memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446057
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446057
* **Key Words**: Memristors, Resistance, Memory management, Transistors, Microprocessors, Energy consumption, power aware computing, random-access storage, reconfigurable architectures, transistors, low-power hybrid reconfigurable architecture, resistive random-access memories, access transistor free memristive crossbars, next generation non-volatile memories, transistor per memory element, parasitic currents, line drivers, complementary resistive switch, CRS, 
* **Abstract**: Access-transistor-free memristive crossbars have shown to be excellent candidates for next generation non-volatile memories. While the elimination of the transistor per memory element enables higher memory densities, it also introduces parasitic currents during the normal operation of the memory that increases both the overall power consumption of the crossbar, and the current requirements of the line drivers. In this work we present a hybrid reconfigurable memory architecture that takes advantage of the fact that a complementary resistive switch (CRS) can behave both as a memristor and as a CRS. By dynamically keeping frequently accessed regions of the memory in the memristive mode and others in the CRS mode, our hybrid memory offer all the benefits that a memristor and a CRS offer individually, without any of their drawbacks. We validate our architecture using the SPEC CPU2006 benchmark and found that our hybrid memory offers average energy savings of 3.6x with respect to a memristive-only memory. In addition, we can offer a memory lifetime that is, on average, 6.4x longer than that of a CRS-only memory.

---
### A performance analysis framework for optimizing OpenCL applications on FPGAs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446058
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446058
* **Key Words**: Field programmable gate arrays, Optimization, Kernel, Pipelines, Analytical models, Performance analysis, Measurement, field programmable gate arrays, graphics processing units, optimisation, program diagnostics, software metrics, Altera, Xilinx, OpenCL SDK program, CPU-GPUs, FPGA-based performance analysis framework, static analysis, dynamic analysis, programmer-interpretable metrics, input optimization combination, 
* **Abstract**: Recently, FPGA vendors such as Altera and Xilinx have released OpenCL SDK for programming FPGAs. However, the architecture of FPGA is significantly different from that of CPU/GPU, for which OpenCL is originally designed. Tuning the OpenCL code for good performance on FPGAs is still an open problem, since the existing OpenCL tools and models designed for CPUs/GPUs are not directly applicable to FPGAs. In the paper, we present an FPGA-based performance analysis framework that can shed light on the performance bottleneck and thus guide the code tuning for OpenCL applications on FPGAs. Particularly, we leverage static and dynamic analysis to develop an analytical performance model, which has captured the key architectural features of FPGA abstractions under OpenCL. Then, we provide four programmer-interpretable metrics to quantify the performance potentials of the OpenCL program with input optimization combination for the next optimization step. We evaluate our framework with a number of user cases, and demonstrate that 1) our analytical performance model can accurately predict the performance of OpenCL programs with different optimization combinations on FPGAs, and 2) our tool can be used to effectively guide the code tuning on alleviating the performance bottleneck.

---
### HRL: Efficient and flexible reconfigurable logic for near-data processing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446059
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446059
* **Key Words**: Field programmable gate arrays, Arrays, Three-dimensional displays, Bandwidth, Random access memory, Layout, field programmable gate arrays, power aware computing, reconfigurable architectures, flexible reconfigurable logic, near-data processing, energy constraints, Dennard scaling, in-memory analytics, 3D integration technology, near-data processing architectures, NDP architectures, logic layer, NDP compute units, vertical memory channel bandwidth, fine-grained reconfigurable logic, coarse-grained reconfigurable logic, programmable cores, bit-level reconfiguration, data layouts, heterogeneous reconfigurable logic, HRL, NDP systems, CGRA arrays, FPGA arrays, fine-grained logic blocks, coarse-grained logic blocks, power efficiency, MapReduce, routing networks, control signals, data signals, graph processing, deep neural networks, 
* **Abstract**: The energy constraints due to the end of Dennard scaling, the popularity of in-memory analytics, and the advances in 3D integration technology have led to renewed interest in near-data processing (NDP) architectures that move processing closer to main memory. Due to the limited power and area budgets of the logic layer, the NDP compute units should be area and energy efficient while providing sufficient compute capability to match the high bandwidth of vertical memory channels. They should also be flexible to accommodate a wide range of applications. Towards this goal, NDP units based on fine-grained (FPGA) and coarse-grained (CGRA) reconfigurable logic have been proposed as a compromise between the efficiency of custom engines and the flexibility of programmable cores. Unfortunately, FPGAs incur significant area overheads for bit-level reconfiguration, while CGRAs consume significant power in the interconnect and are inefficient for irregular data layouts and control flows. This paper presents Heterogeneous Reconfigurable Logic (HRL), a reconfigurable array for NDP systems that improves on both FPGA and CGRA arrays. HRL combines both coarse-grained and fine-grained logic blocks, separates routing networks for data and control signals, and uses specialized units to effectively support branch operations and irregular data layouts in analytics workloads. HRL has the power efficiency of FPGA and the area efficiency of CGRA. It improves performance per Watt by 2.2x over FPGA and 1.7x over CGRA. For NDP systems running MapReduce, graph processing, and deep neural networks, HRL achieves 92% of the peak performance of an NDP system based on custom accelerators for each application.

---
### Software transparent dynamic binary translation for coarse-grain reconfigurable architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446060
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446060
* **Key Words**: Optimization, Computer architecture, Hardware, Software, Coprocessors, Radiation detectors, Monitoring, optimising compilers, performance evaluation, power aware computing, reconfigurable architectures, software transparent dynamic binary translation, coarse-grain reconfigurable architectures, Dennard scaling, software flexibility, translation time, dynamic optimizer for reconfigurable architectures, performance improvements, CGRA-targeted compiled code, DORA, dynamic register information, 
* **Abstract**: The end of Dennard Scaling has forced architects to focus on designing for execution efficiency. Course-grained reconfigurable architectures (CGRAs) are a class of architectures that provide a configurable grouping of functional units that aim to bridge the gap between the power and performance of custom hardware and the flexibility of software. Despite their potential benefit, CGRAs face a major adoption challenge as they do not execute a standard instruction stream. Dynamic translation for CGRAs has the potential to solve this problem, but faces non-trivial challenges. Existing attempts either do not achieve the full power and performance potential CGRAs offer or suffer from excessive translation time. In this work we propose DORA, a Dynamic Optimizer for Reconfigurable Architectures, which achieves substantial (2X) power and performance improvements while having low hardware and insertion overhead and benefiting the current execution. In addition to traditional optimizations, DORA leverages dynamic register information to perform optimizations not available to compilers and achieves performance similar to or better than CGRA-targeted compiled code.

---
### Core tunneling: Variation-aware voltage noise mitigation in GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446061
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446061
* **Key Words**: Graphics processing units, Tunneling, Histograms, Power demand, Monitoring, Delays, Kernel, energy conservation, graphics processing units, power aware computing, core tunneling, variation-aware voltage noise mitigation, GPU, graphics processing unit, microprocessors, low-nanometer technologies, process variation model, distributed power delivery, voltage droops, voltage guardbands, voltage margin reduction, clock gating, energy consumption reduction, 
* **Abstract**: Voltage noise and manufacturing process variation represent significant reliability challenges for modern microprocessors. Voltage noise is caused by rapid changes in processor activity that can lead to timing violations and errors. Process variation is caused by manufacturing challenges in low-nanometer technologies and can lead to significant heterogeneity in performance and reliability across the chip. To ensure correct execution under worst-case conditions, chip designers generally add operating margins that are often unnecessarily conservative for most use cases, which results in wasted energy. This paper investigates the combined effects of process variation and voltage noise on modern GPU architectures. A distributed power delivery and process variation model at functional unit granularity was developed and used to simulate supply voltage behavior in a multicore GPU system. We observed that, just like in CPUs, large changes in power demand can lead to significant voltage droops. We also note that process variation makes some cores much more vulnerable to noise than others in the same GPU. Therefore, protecting the chip against large voltage droops by using fixed and uniform voltage guardbands is costly and inefficient. This paper presents core tunneling, a variation-aware solution for dynamically reducing voltage margins. The system relies on hardware critical path monitors to detect voltage noise conditions and quickly reacts by clock-gating vulnerable cores to prevent timing violations. This allows a substantial reduction in voltage margins. Since clock gating is enabled infrequently and only on the most vulnerable cores, the performance impact of core tunneling is very low. On average, core tunneling reduces energy consumption by 15%.

---
### Warped-preexecution: A GPU pre-execution approach for improving latency hiding.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446062
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446062
* **Key Words**: Registers, Graphics processing units, Instruction sets, Hazards, Pipelines, Memory management, Switches, cache storage, concurrency (computers), graphics processing units, multi-threading, GPU performance, pre-execution mode, processing delay, long-latency operations, thread concurrency, fast thread switching capability, memory contention, on-chip storage demands, write-after-write hazards, write-after-read hazards, P-mode output values, physical registers, register file underutilization, L1 cache, memory access penalties, memory intensive applications, 
* **Abstract**: This paper presents a pre-execution approach for improving GPU performance, called P-mode (pre-execution mode). GPUs utilize a number of concurrent threads for hiding processing delay of operations. However, certain long-latency operations such as off-chip memory accesses often take hundreds of cycles and hence leads to stalls even in the presence of thread concurrency and fast thread switching capability. It is unclear if adding more threads can improve latency tolerance due to increased memory contention. Further, adding more threads increases on-chip storage demands. Instead we propose that when a warp is stalled on a long-latency operation it enters P-mode. In P-mode, a warp continues to fetch and decode successive instructions to identify any independent instruction that is not on the long latency dependence chain. These independent instructions are then pre-executed. To tackle write-after-write and write-after-read hazards, during P-mode output values are written to renamed physical registers. We exploit the register file underutilization to re-purpose a few unused registers to store the P-mode results. When a warp is switched from P-mode to normal execution mode it reuses pre-executed results by reading the renamed registers. Any global load operation in P-mode is transformed into a pre-load which fetches data into the L1 cache to reduce future memory access penalties. Our evaluation results show 23% performance improvement for memory intensive applications, without negatively impacting other application categories.

---
### Approximating warps with intra-warp operand value similarity.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446063
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446063
* **Key Words**: Registers, Graphics processing units, Hardware, Writing, Computers, Microarchitecture, approximation theory, energy conservation, graphics processing units, optimisation, intra-warp operand value similarity, value locality, myriad optimization techniques, micro-architecture, Dennard scaling, parallel accelerators, GPU, warp approximation, energy efficiency, 
* **Abstract**: Value locality, the recurrence of a previously-seen value, has been the enabler of myriad optimization techniques in traditional processors. Value similarity relaxes the constraint of value locality by allowing values to differ in the lowest significant bits where values are micro-architecturally near. With the end of Dennard Scaling and the turn towards massively parallel accelerators, we revisit value similarity in the context of GPUs. We identify a form of value similarity called intra-warp operand value similarity, which is abundant in GPUs. We present Warp Approximation, which leverages intra-warp operand value similarity to trade off accuracy for energy. Warp Approximation dynamically identifies intra-warp operand value similarity in hardware, and executes a single representative thread on behalf of all the active threads in a warp, thereby producing a representative value with approximate value locality. This representative value can then be stored compactly in the register file as a value similar scalar, reducing the read and write energy when dealing with approximate data. With Warp Approximation, we can reduce execution unit energy by 37%, register file energy by 28%, and improve overall GPGPU energy efficiency by 26% with minimal quality degradation.

---
### A case for toggle-aware compression for GPU systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446064
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446064
* **Key Words**: Bandwidth, Data compression, System-on-chip, Graphics processing units, Compression algorithms, Data transfer, Random access memory, bandwidth allocation, data compression, graphics processing units, meta data, power aware computing, redundancy, system buses, telecommunication channels, toggle-aware compression algorithm, GPU systems, system performance, data compression techniques, bandwidth utilization, off-chip interconnects, on-chip interconnects, communication energy-efficiency, communication channel, off-chip buses, on-chip buses, wire discharging, toggle-aware compression techniques, energy control, metadata consolidation, bandwidth reduction, 
* **Abstract**: Data compression can be an effective method to achieve higher system performance and energy efficiency in modern data-intensive applications by exploiting redundancy and data similarity. Prior works have studied a variety of data compression techniques to improve both capacity (e.g., of caches and main memory) and bandwidth utilization (e.g., of the on-chip and off-chip interconnects). In this paper, we make a new observation about the energy-efficiency of communication when compression is applied. While compression reduces the amount of transferred data, it leads to a substantial increase in the number of bit toggles (i.e., communication channel switchings from 0 to 1 or from 1 to 0). The increased toggle count increases the dynamic energy consumed by on-chip and off-chip buses due to more frequent charging and discharging of the wires. Our results show that the total bit toggle count can increase from 20% to 2.2x when compression is applied for some compression algorithms, averaged across different application suites. We characterize and demonstrate this new problem across 242 GPU applications and six different compression algorithms. To mitigate the problem, we propose two new toggle-aware compression techniques: Energy Control and Metadata Consolidation. These techniques greatly reduce the bit toggle count impact of the data compression algorithms we examine, while keeping most of their bandwidth reduction benefits.

---
### Minimal disturbance placement and promotion.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446065
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446065
* **Key Words**: Radiation detectors, Binary trees, Marine vehicles, Electronics packaging, Neural networks, Genetic algorithms, Random access memory, cache storage, DRAM chips, multiprocessing systems, performance evaluation, trees (mathematics), cache replacement policies, distinct positions, tree-based PseudoLRU, space-efficient replacement policy, binary tree, nonpromoted blocks, minimal disturbance placement and promotion policy, MDPP policy, SRRIP, single-threaded workloads, multicore workloads, dead block prediction, dynamic MDPP, read-modify-write cycle, replacement state, DRAM caches, 
* **Abstract**: Cache replacement policies often order blocks into distinct positions. A block is placed into a set in some initial position. A re-referenced block is promoted into a higher position while other blocks may move into lower positions. A block in the lowest position is a candidate for replacement. Tree-based PseudoLRU is a well-known space-efficient replacement policy based on representing block positions as distinct paths in a binary tree. We find that a placement or promotion for one block often needlessly disturbs the non-promoted blocks. Guided by the principle of minimal disturbance, i.e. that a policy should seek to disturb the order of non-promoted blocks to the smallest extent possible, we develop a simple modification to PseudoLRU resulting in a policy that improves performance over previous techniques while retaining the low cost of PseudoLRU. The result is a minimal disturbance placement and promotion (MDPP) policy. We first give a static formulation of MDPP and show that it provides superior performance to LRU, PseudoLRU and matches performance for SRRIP for both single-threaded and multi-core workloads. We then give a dynamic formulation that uses dead block prediction for placement and bypass and show that it meets or exceeds state-of-the-art performance with lower overhead. For single-threaded workloads, dynamic MDPP matches the 5.9% speedup over LRU of the state-of-the-art policy SHiP. For multi-core workloads, dynamic MDPP gives a normalized weighted speedup of 14.3% over LRU, compared with SHiP that yields a speedup of 12.3% over LRU and requires double the storage overhead per set. We show that minimal disturbance policies can reduce the frequency of a costly read-modify-write cycle for replacement state, making them potentially suitable for future work in DRAM caches.

---
### Revisiting virtual L1 caches: A practical design using dynamic synonym remapping.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446066
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446066
* **Key Words**: Computers, Software, Organizations, Energy consumption, Microarchitecture, Hardware, Coherence, cache storage, energy conservation, energy consumption, paged storage, power aware computing, table lookup, virtual L1 caches, practical design, energy consumption, temporal characteristics, unique virtual page number, synonymous virtual pages, physical page, unique page number, look up data, virtual cache with dynamic synonym remapping, VC-DSR, dynamic energy savings, TLB lookups, latency benefits, 
* **Abstract**: Virtual caches have potentially lower access latency and energy consumption than physical caches because they do not consult the TLB prior to cache access. However, they have not been popular in commercial designs. The crux of the problem is the possibility of synonyms. This paper makes several empirical observations about the temporal characteristics of synonyms, especially in caches of sizes that are typical of L1 caches. By leveraging these observations, the paper proposes a practical design of an L1 virtual cache that (1) dynamically decides a unique virtual page number for all the synonymous virtual pages that map to the same physical page and (2) uses this unique page number to place and look up data in the virtual caches. Accesses to this unique page number proceed without any intervention. Accesses to other synonymous pages are dynamically detected, and remapped to the corresponding unique virtual page number to correctly access data in the cache. Such remapping operations are rare, due to the temporal properties of synonyms, allowing a Virtual Cache with Dynamic Synonym Remapping (VC-DSR) to achieve most of the benefits of virtual caches but without software involvement. Experimental results based on real world applications show that VC-DSR can achieve about 92% of the dynamic energy savings for TLB lookups, and 99.4% of the latency benefits of ideal (but impractical) virtual caches for the configurations considered.

---
### Modeling cache performance beyond LRU.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446067
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446067
* **Key Words**: Analytical models, Program processors, Optimization, Electronics packaging, Organizations, Computational modeling, cache storage, multiprocessing systems, probability, quality of service, cache performance modelling, LRU, high-performance cache replacement policies, least-recently used, stack distances, multicore systems, cache partitioning, quality of service, probabilistic cache model, absolute reuse distance, abstract ranking function, arbitrary age-based replacement policies, synthetic benchmark, SPEC CPU2006 benchmarks, shared cache performance, 
* **Abstract**: Modern processors use high-performance cache replacement policies that outperform traditional alternatives like least-recently used (LRU). Unfortunately, current cache models do not capture these high-performance policies as most use stack distances, which are inherently tied to LRU or its variants. Accurate predictions of cache performance enable many optimizations in multicore systems. For example, cache partitioning uses these predictions to divide capacity among applications in order to maximize performance, guarantee quality of service, or achieve other system objectives. Without an accurate model for high-performance replacement policies, these optimizations are unavailable to modern processors. We present a new probabilistic cache model designed for high-performance replacement policies. It uses absolute reuse distances instead of stack distances, and models replacement policies as abstract ranking functions. These innovations let us model arbitrary age-based replacement policies. Our model achieves median error of less than 1% across several high-performance policies on both synthetic and SPEC CPU2006 benchmarks. Finally, we present a case study showing how to use the model to improve shared cache performance.

---
### Efficient footprint caching for Tagless DRAM Caches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446068
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446068
* **Key Words**: Random access memory, Bandwidth, Resource management, Tagging, History, Scalability, Three-dimensional displays, cache storage, DRAM chips, power aware computing, SRAM chips, footprint caching, tagless DRAM caches, cache tag management, in-package DRAM caches, TDCs, tagging structures, on-die SRAM, multigigabyte DRAM caches, translation management, page granularity, page-based caching, off-package DRAM bandwidth waste, auxiliary on-die structure, footprint thrashing, footprint prediction, footprint-augmented tagless DRAM cache, F-TDC, bandwidth efficiency, energy efficiency, 3D through-silicon-via-based in-package DRAM, 
* **Abstract**: Efficient cache tag management is a primary design objective for large, in-package DRAM caches. Recently, Tagless DRAM Caches (TDCs) have been proposed to completely eliminate tagging structures from both on-die SRAM and in-package DRAM, which are a major scalability bottleneck for future multi-gigabyte DRAM caches. However, TDC imposes a constraint on DRAM cache block size to be the same as OS page size (e.g., 4KB) as it takes a unified approach to address translation and cache tag management. Caching at a page granularity, or page-based caching, incurs significant off-package DRAM bandwidth waste by over-fetching blocks within a page that are not actually used. Footprint caching is an effective solution to this problem, which fetches only those blocks that will likely be touched during the page's lifetime in the DRAM cache, referred to as the page's footprint. In this paper we demonstrate TDC opens up unique opportunities to realize efficient footprint caching with higher prediction accuracy and a lower hardware cost than the original footprint caching scheme. Since there are no cache tags in TDC, the footprints of cached pages are tracked at TLB, instead of cache tag array, to incur much lower on-die storage overhead than the original design. Besides, when a cached page is evicted, its footprint will be stored in the corresponding page table entry, instead of an auxiliary on-die structure (i.e., Footprint History Table), to prevent footprint thrashing among different pages, thus yielding higher accuracy in footprint prediction. The resulting design, called Footprint-augmented Tagless DRAM Cache (F-TDC), significantly improves the bandwidth efficiency of TDC, and hence its performance and energy efficiency. Our evaluation with 3D Through-Silicon-Via-based in-package DRAM demonstrates an average reduction of off-package bandwidth by 32.0%, which, in turn, improves IPC and EDP by 17.7% and 25.4%, respectively, over the state-of-the-art TDC with no footprint caching.

---
### SCsafe: Logging sequential consistency violations continuously and precisely.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446069
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446069
* **Key Words**: Hardware, Production, Load modeling, Coherence, Proposals, Debugging, program debugging, system recovery, SCsafe, sequential consistency violation logging, SCV detection, SCV logging, recovery techniques, total store order, release consistency, program debugging, 
* **Abstract**: Sequential Consistency Violations (SCV) in relaxed consistency machines cause programs to malfunction and are hard to debug. While there are proposals for detecting and recording SCVs, they are limited in that they end program execution after detecting the first SCV because the program is now non-SC. Therefore, they cannot be used in production runs. In addition, such proposals rely on complicated hardware. To address these problems, this paper proposes the first architecture that detects and logs SCVs in a continuous manner, while retaining SC. In addition, the scheme is precise and uses substantially simpler hardware. The scheme, called SCsafe, operates continously because, after SCV detection and logging, it recovers and resumes execution while retaining SC. As a result, it can be used in production runs. In addition, SCsafe is precise in that it identifies only true SCVs - rather than dependence cycles due to false sharing. Finally, SCsafe's hardware is mostly local to each processor, and uses known recovery techniques. We evaluate SCsafe using simulations of 16-processor multicores with Total Store Order or Release Consistency. In codes with SCVs, SCsafe detects and reports SCVs while enforcing SC during the execution. In codes with few SCVs, it adds a negligible performance overhead. Finally, SCsafe is scalable with the processor count.

---
### LASER: Light, Accurate Sharing dEtection and Repair.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446070
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446070
* **Key Words**: Maintenance engineering, Coherence, Hardware, Benchmark testing, Radiation detectors, Instruction sets, cache storage, performance evaluation, shared memory systems, LASER, light-accurate-sharing-detection-and repair, shared memory, performance bug, cache contention, program sharing behavior, memory allocator, false sharing, operating system, runtime system environment, performance counter capabilities, Intel Haswell architecture, cache coherence events, online contention detection, performance overhead, average runtime overhead, Phoenix benchmarks, Parsec benchmarks, Splash2x benchmarks, commodity hardware, 
* **Abstract**: Contention for shared memory, in the forms of true sharing and false sharing, is a challenging performance bug to discover and to repair. Understanding cache contention requires global knowledge of the program's actual sharing behavior, and can even arise invisibly in the program due to the opaque decisions of the memory allocator. Previous schemes have focused only on false sharing, and impose significant performance penalties or require non-trivial alterations to the operating system or runtime system environment. This paper presents the Light, Accurate Sharing dEtection and Repair (LASER) system, which leverages new performance counter capabilities available on Intel's Haswell architecture that identify the source of expensive cache coherence events. Using records of these events generated by the hardware, we build a system for online contention detection and repair that operates with low performance overhead and does not require any invasive program, compiler or operating system changes. Our experiments show that LASER imposes just 2% average runtime overhead on the Phoenix, Parsec and Splash2x benchmarks. LASER can automatically improve the performance of programs by up to 19% on commodity hardware.

---
### Efficient GPU hardware transactional memory through early conflict resolution.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446071
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446071
* **Key Words**: Graphics processing units, Hardware, Spatial resolution, Instruction sets, Programming, Hazards, Partitioning algorithms, graphics processing units, storage management, synchronisation, transaction processing, GPU hardware transactional memory, graphics processing unit, Warp TM, memory access location, parallel application, inter-thread synchronization, early-abort global conflict resolution, pause-and-go execution scheme, commit units, energy consumption reduction, 
* **Abstract**: It has been proposed that Transactional Memory be added to Graphics Processing Units (GPUs) in recent years. One proposed hardware design, Warp TM, can scale to 1000s of concurrent transactions. As a programming method that can atomicize an arbitrary number of memory access locations and greatly reduce the efforts to program parallel applications, transactional memory handles the complexity of inter-thread synchronization. However, when thousands of transactions run concurrently on a GPU, conflicts and resource contentions arise, causing performance loss. In this paper, we identify and analyze the cause of conflicts and contentions and propose two enhancements that try to resolve conflicts early: (1) Early-Abort global conflict resolution that allows conflicts to be detected before they reach the Commit Units so that contention in the Commit Units is reduced and (2) Pause-and-Go execution scheme that reduces the chance of conflict and the performance penalty of re-executing long transactions. These two enhancements are enabled by a single hardware modification. Our evaluation shows the combination of the two enhancements greatly improves overall execution speed while reducing energy consumption.

---
### PleaseTM: Enabling transaction conflict management in requester-wins hardware transactional memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446072
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446072
* **Key Words**: Coherence, Protocols, Message systems, Hardware, Proposals, Memory management, Benchmark testing, cache storage, multi-threading, hardware transactional memory, transaction conflict management, HTM, multithreaded applications, cache block, coherence messages, PleaseTM, coherence protocol design, STAMP benchmarks, 
* **Abstract**: With recent commercial offerings, hardware transactional memory (HTM) has finally become an important tool in writing multithreaded applications. However, current offerings are commonly implemented in a way that keep the coherence protocol unmodified. Data conflicts are recognized by coherence messages sent by the requester to sharers of the cache block (e.g., a write to a speculatively read line), who are then aborted. This tends to abort transactions that have done more work, leading to suboptimal performance. Even worse, this can lead to live-lock situations where transactions repeatedly abort each other. In this paper, we present PleaseTM, a mechanism that allows more freedom in deciding which transaction to abort, while leaving the coherence protocol design unchanged. In PleaseTM, transactions insert plea bits into their responses to coherence requests as a simple payload, and use these bits to inform conflict management decisions. Coherence permission changes are then achieved with normal coherence requests. Our experiments show that this additional freedom can provide on average 43% speedup, with a maximum of 7-fold speedup, on STAMP benchmarks running at 32 threads compared to requester-wins HTM.

---
### Efficient synthetic traffic models for large, complex SoCs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446073
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446073
* **Key Words**: Graphics processing units, Protocols, Solid modeling, Central Processing Unit, Coherence, Analytical models, Computational modeling, cache storage, circuit simulation, extrapolation, graphics processing units, microprocessor chips, network-on-chip, synthetic traffic models, large-complex SoC, network on chip, interconnects, SynFull methodology, statistical analysis, NoC traffic workload, traffic generator, Markov models, multicore CPU, cache coherence protocol, MESI, system on a chip, heterogeneous architecture, GPU, complex cache hierarchy, shared caches, heterogeneous workloads, extrapolation, synthetic memory reference model, fixed latency model, memory subsystem, NoC simulation methodology, heterogeneous SoC architectures, 
* **Abstract**: The interconnect or network on chip (NoC) is an increasingly important component in processors. As systems scale up in size and functionality, the ability to efficiently model larger and more complex NoCs becomes increasingly important to the design and evaluation of such systems. Recent work proposed the "SynFull" methodology that performs statistical analysis of a workload's NoC traffic to create compact traffic generators based on Markov models. While the models generate synthetic traffic, the traffic is statistically similar to the original trace and can be used for fast NoC simulation. However, the original SynFull work only evaluated multi-core CPU scenarios with a very simple cache coherence protocol (MESI). We find the original SynFull methodology to be insufficient when modeling the NoC of a more complex system on a chip (SoC). We identify and analyze the shortcomings of SynFull in the context of a SoC consisting of a heterogeneous architecture (CPU and GPU), a more complex cache hierarchy including support for full coherence between CPU, GPU, and shared caches, and heterogeneous workloads. We introduce new techniques to address these shortcomings. Furthermore, the original SynFull methodology can only model a NoC with N nodes when the original application analysis is performed on an identically-sized N-node system, but one typically wants to model larger future systems. Therefore, we introduce new techniques to enable SynFull-like analysis to be extrapolated to model such larger systems. Finally, we present a novel synthetic memory reference model to replace SynFull's fixed latency model; this allows more realistic evaluation of the memory subsystem and its interaction with the NoC. The result is a robust NoC simulation methodology that works for large, heterogeneous SoC architectures.

---
### DVFS for NoCs in CMPs: A thread voting approach.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446074
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446074
* **Key Words**: Message systems, Nickel, System-on-chip, Tuning, Delays, Benchmark testing, energy conservation, multiprocessing systems, network-on-chip, power aware computing, NoC, CMP, thread voting approach, dynamic voltage frequency scaling, network-on-chip, chip multiprocessors, energy efficacy, network-level metrics, communicating threads, runtime profiled message generation rate, data sharing characteristics, packet header, distributed decision making process, multithreaded PARSEC benchmark, voting based DVFS mechanism, 
* **Abstract**: As the core count grows rapidly, dynamic voltage/frequency scaling (DVFS) in networks-on-chip (NoCs) becomes critical in optimizing energy efficacy in chip multiprocessors (CMPs). Previously proposed techniques often exploit inherent network-level metrics to do so. However, such network metrics may contradictorily reflect application's performance need, leading to power over/under provisioning. We propose a novel on-chip DVFS technique for NoCs that is able to adjust per-region V/F level according to voted V/F levels of communicating threads. Each region is composed of a few adjacent routers sharing the same V/F level. With a voting-based approach, threads seek to influence the DVFS decisions independently by voting for a preferred V/F level that best suits their own performance interest according to their runtime profiled message generation rate and data sharing characteristics. The vote expressed in a few bits is then carried in the packet header and spread to the routers on the packet route. The final DVFS decision is made democratically by a region DVFS controller based on the majority election result of collected votes from all active threads. To achieve scalable V/F adjustment, each region works independently, and the voting-based V/F tuning forms a distributed decision making process. We evaluate our technique with detailed simulations of a 64-core CMP running a variety of multi-threaded PARSEC benchmarks. Compared with a network without DVFS and a network metric (router buffer occupancy) based approach, experimental results show that our voting based DVFS mechanism improves the network energy efficacy measured in MPPJ (million packets per joule) by about 17.9% and 9.7% on average, respectively, and the system energy efficacy measured in MIPJ (million instructions per joule) by about 26.3% and 17.1% on average, respectively.

---
### SLaC: Stage laser control for a flattened butterfly network.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446075
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446075
* **Key Words**: Photonics, Optical waveguides, System-on-chip, Layout, Waveguide lasers, Energy efficiency, Delays, computer centres, energy conservation, hypercube networks, integrated optics, optical control, optical interconnections, optical losses, power aware computing, SLaC, stage laser control, high-performance energy-efficient, on-chip interconnects, on-board interconnects, datacenter-scale interconnects, optical loss, nanophotonic components, laser sources, total power requirements, optical interconnects, system inactivity, laser gating, low-scalability on-chip photonic interconnects, photonic crossbars, network connectivity, photonic flattened-butterfly network, laser energy saving, datacenter network, 
* **Abstract**: Photonic interconnects have emerged as a promising candidate technology for high-performance energy-efficient on-chip, on-board, and datacenter-scale interconnects. However, the high optical loss of many nanophotonic components coupled with the low efficiency of current laser sources result in exceedingly high total power requirements for the laser. As optical interconnects stay on even during periods of system inactivity, most of this power is wasted, which has prompted research on laser gating. Unfortunately, prior work on laser gating has only focused on low-scalability on-chip photonic interconnects (photonic crossbars), and disrupts the connectivity of the network which renders a high-performance implementation challenging. In this paper we propose SLaC, a laser gating technique that turns on and off redundant paths in a photonic flattened-butterfly network to save laser energy while maintaining high performance and full connectivity. Maintaining full connectivity removes the laser turn-on latency from the critical path and results in minimal performance degradation. SLaC is equally applicable to on-chip, on-board, and datacenter level interconnects. For on-chip and multi-chip applications, SLaC saves up to 67% of the laser energy (43-57% on average) when running real-world workloads. On a datacenter network, SLaC saves 79% of the laser energy on average when running traffic traces collected from university datacenter servers.

---
### The runahead network-on-chip.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446076
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446076
* **Key Words**: Ports (Computers), Routing, Complexity theory, Power demand, Turning, System-on-chip, Bandwidth, network-on-chip, power aware computing, storage management, power-emcient communication, average channel utilization, router microarchitecture, internetworking, runahead NoC, runahead network-on-chip, single-cycle hops, bufferless router microarchitecture, ultra-low latency communication, PAR-SEC workloads, SPLASH-2 workloads, power consumption, 
* **Abstract**: With increasing core counts and higher memory demands from applications, it is imperative that networks-on-chip (NoCs) provide low-latency, power-efficient communication. Conventional NoCs tend to be over-provisioned for worst-case bandwidth demands leading to ineffective use of network resources and significant power inefficiency; average channel utilization is typically less than 5% in real-world applications. In terms of performance, low-latency techniques often introduce power and area overheads and incur significant complexity in the router microarchitecture. We find that both low latency and power efficiency are possible by relaxing the constraint of lossless communication. This is inspired from internetworking where best effort delivery is commonplace. We propose the Runahead NoC, a lightweight, lossy network that provides single-cycle hops. Allowing for lossy delivery enables an extremely simple bufferless router microarchitecture that performs routing and arbitration within the same cycle as link traversal. The Runahead NoC operates either as a power-saver that is integrated into an existing conventional NoC to improve power efficiency, or as an accelerator that is added on top to provide ultra-low latency communication for select packets. On a range of PAR-SEC and SPLASH-2 workloads, we find that the Runahead NoC reduces power consumption by 1.81 as a power-saver and improves runtime and packet latency by 1.08× and 1.66× as an accelerator.

---
### Towards high performance paged memory for GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446077
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446077
* **Key Words**: Graphics processing units, Kernel, Bandwidth, Runtime, Memory management, Programming, graphics processing units, paged storage, parallel processing, peripheral interfaces, pipeline processing, software maintenance, high performance paged memory, parallel accelerators, discrete GPU, on-die GPU, PCIe, privately managed physical memory, data movement management, CPU, on-chip GPU memory hierarchy, GPU vendors, GPU memory capacity, memory migration, high latency low bandwidth link, software runtimes, GPU hardware, GPU software, GPU paged memory, legacy programmer directed memory management, GPU execution pipeline, GPU memory oversubscription, GPU instances, 
* **Abstract**: Despite industrial investment in both on-die GPUs and next generation interconnects, the highest performing parallel accelerators shipping today continue to be discrete GPUs. Connected via PCIe, these GPUs utilize their own privately managed physical memory that is optimized for high bandwidth. These separate memories force GPU programmers to manage the movement of data between the CPU and GPU, in addition to the on-chip GPU memory hierarchy. To simplify this process, GPU vendors are developing software runtimes that automatically page memory in and out of the GPU on-demand, reducing programmer effort and enabling computation across datasets that exceed the GPU memory capacity. Because this memory migration occurs over a high latency and low bandwidth link (compared to GPU memory), these software runtimes may result in significant performance penalties. In this work, we explore the features needed in GPU hardware and software to close the performance gap of GPU paged memory versus legacy programmer directed memory management. Without modifying the GPU execution pipeline, we show it is possible to largely hide the performance overheads of GPU paged memory, converting an average 2× slowdown into a 12% speedup when compared to programmer directed transfers. Additionally, we examine the performance impact that GPU memory oversubscription has on application run times, enabling application designers to make informed decisions on how to shard their datasets across hosts and GPU instances.

---
### Simultaneous Multikernel GPU: Multi-tasking throughput processors via fine-grained sharing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446078
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446078
* **Key Words**: Kernel, Graphics processing units, Context, Instruction sets, Switches, Throughput, Hardware, graphics processing units, resource allocation, simultaneous multikernel GPU, multitasking throughput processors, complementary resource occupancy, streaming multiprocessor, fine-grain dynamic sharing mechanism, SMK, GPU resource utilization, code transformation, recompilation, static sharing, resource under-utilization, nongraphics programs, fine-grained sharing, 
* **Abstract**: Studies show that non-graphics programs can be less optimized for the GPU hardware, leading to significant resource under-utilization. Sharing the GPU among multiple programs can effectively improve utilization, which is particularly attractive to systems where many applications require access to the GPU (e.g., cloud computing). However, current GPUs lack proper architecture features to support sharing. Initial attempts are preliminary: They either provide only static sharing, which requires recompilation or code transformation, or they do not effectively improve GPU resource utilization. We propose Simultaneous Multikernel (SMK), a fine-grain dynamic sharing mechanism, that fully utilizes resources within a streaming multiprocessor by exploiting heterogeneity of different kernels. We propose several resource allocation strategies to improve system throughput while maintaining fairness. Our evaluation shows that for shared workloads with complementary resource occupancy, SMK improves GPU throughput by 52% over non-shared execution and 17% over a state-of-the-art design.

---
### iPAWS: Instruction-issue pattern-based adaptive warp scheduling for GPGPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446079
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446079
* **Key Words**: Radio frequency, Computational fluid dynamics, Adaptive scheduling, Instruction sets, Computer architecture, Random access memory, Processor scheduling, adaptive scheduling, cache storage, graphics processing units, greedy algorithms, multiprocessing systems, performance evaluation, processor scheduling, iPAWS, instruction-issue pattern-based adaptive warp scheduling, GPGPU, thread scheduling, greedy warp scheduler, warp scheduling policy, interwarp locality, round-robin warp scheduler, static warp scheduling, workload characteristics, instruction-issue pattern, cache-conscious wavefront scheduling, CCWS, memory aware scheduling and cache access reexecution, MASCAR, 
* **Abstract**: Thread or warp scheduling in GPGPUs has been shown to have a significant impact on overall performance. Recently proposed warp schedulers have been based on a greedy warp scheduler where some warps are prioritized over other warps. However, a single warp scheduling policy does not necessarily provide good performance across all types of workloads; in particular, we show that greedy warp schedulers are not necessarily optimal for workloads with inter-warp locality while a simple round-robin warp scheduler provides better performance. Thus, we argue that instead of single, static warp scheduling, an adaptive warp scheduler that dynamically changes the warp scheduler based on the workload characteristics should be leveraged. In this work, we propose an instruction-issue pattern-based adaptive warp scheduler (iPAWS) that dynamically adapts between a greedy warp scheduler and a fair, round-robin scheduler. We exploit the observation that workloads that favor a greedy warp scheduler will have an instruction-issue pattern that is biased towards some warps while workloads that favor a fair, round-robin warp scheduler will tend to issue instructions across all of the warps. Our evaluations show that iPAWS is able to adapt to the more optimal warp scheduler dynamically and achieve performance that is within a few percent of the statically determined, more optimal warp scheduler. We also show that iPAWS can be extended to other warp schedulers, including the cache-conscious wavefront scheduling (CCWS) and Memory Aware Scheduling and Cache Access Re-execution (MASCAR) to exploit the benefits of other warp schedulers while still providing adaptivity in warp scheduling.

---
### Lattice priority scheduling: Low-overhead timing-channel protection for a shared memory controller.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446080
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446080
* **Key Words**: Timing, Security, Lattices, Throughput, Processor scheduling, Hardware, Schedules, cryptography, microprocessor chips, performance evaluation, processor scheduling, shared memory systems, storage management, lattice priority scheduling, low-overhead timing-channel protection, shared memory controller, computer hardware component, hardware sharing, timing-channel protection, memory scheduling algorithm, target system security requirement, lattice policy, LPS evaluation, 8-core microprocessor, resource allocation, memory controller capability, 
* **Abstract**: Computer hardware is increasingly shared by distrusting parties in platforms such as commercial clouds and web servers. Though hardware sharing is critical for performance and efficiency, this sharing creates timing-channel vulnerabilities in hardware components such as memory controllers and shared memory. Past work on timing-channel protection for memory controllers assumes all parties are mutually distrusting and require timing-channel protection. This assumption limits the capability of the memory controller to allocate resources effectively, and causes severe performance penalties. Further, the assumption that all entities are mutually distrusting is often a poor fit for the security needs of real systems. Often, some entities do not require timing-channel protection or trust others with information. We propose lattice priority scheduling (LPS), a secure memory scheduling algorithm that improves performance by more precisely meeting the target system's security requirements, expressed as a lattice policy. We evaluate LPS in a simulated 8-core microprocessor. Compared to prior solutions [34], lattice priority scheduling improves system throughput by over 30% on average and by up to 84% for some workloads.

---
### A complete key recovery timing attack on a GPU.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446081
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446081
* **Key Words**: Graphics processing units, Timing, Encryption, Kernel, Message systems, data integrity, graphics processing units, parallel architectures, private key cryptography, key-recovery timing channel attack, graphics processing units, parallel computing devices, massive-parallel computational resources, high-volume data encryption, data integrity, data confidentiality, GPU security, AES key recovery, Advanced Encryption Standard, ciphertext, GPU architecture vulnerability, side-channel timing attack vulnerability, AES-128, 
* **Abstract**: Graphics Processing Units (GPUs) have become mainstream parallel computing devices. They are deployed on diverse platforms, and an increasing number of applications have been moved to GPUs to exploit their massive parallel computational resources. GPUs are starting to be used for security services, where high-volume data is encrypted to ensure integrity and confidentiality. However, the security of GPUs has only begun to receive attention. Issues such as side-channel vulnerability have not been addressed. The goal of this paper is to evaluate the side-channel security of GPUs and demonstrate a complete AES (Advanced Encryption Standard) key recovery using known ciphertext through a timing channel. To the best of our knowledge, this is the first work that clearly demonstrates the vulnerability of a commercial GPU architecture to side-channel timing attacks. Specifically, for AES-128, we have been able to recover all key bytes utilizing a timing side channel in under 30 minutes.

---
### CATalyst: Defeating last-level cache side channel attacks in cloud computing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446082
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446082
* **Key Words**: Cloud computing, Hardware, Servers, Resource management, Electronic mail, Security, cache storage, cloud computing, microprocessor chips, security of data, virtual machines, CATalyst, last-level cache side channel attacks, cloud computing, security threat, multitenant public cloud platform, secret information extraction, virtual machine, high-bandwidth low-noise side channel attack, LLC side channel attacks, performance optimization, commodity processor, cloud server, Intel processor, Intel Cache Allocation Technology, Intel CAT, system-level protection mechanism, way-based hardware cache-partitioning mechanism, quality of service, LLC occupancy, pseudolocking mechanism, LLC partitioning, hybrid hardware-software managed cache, Xen, Linux, performance overhead, 
* **Abstract**: Cache side channel attacks are serious threats to multi-tenant public cloud platforms. Past work showed how secret information in one virtual machine (VM) can be extracted by another co-resident VM using such attacks. Recent research demonstrated the feasibility of high-bandwidth, low-noise side channel attacks on the last-level cache (LLC), which is shared by all the cores in the processor package, enabling attacks even when VMs are scheduled on different cores. This paper shows how such LLC side channel attacks can be defeated using a performance optimization feature recently introduced in commodity processors. Since most cloud servers use Intel processors, we show how the Intel Cache Allocation Technology (CAT) can be used to provide a system-level protection mechanism to defend from side channel attacks on the shared LLC. CAT is a way-based hardware cache-partitioning mechanism for enforcing quality-of-service with respect to LLC occupancy. However, it cannot be directly used to defeat cache side channel attacks due to the very limited number of partitions it provides. We present CATalyst, a pseudo-locking mechanism which uses CAT to partition the LLC into a hybrid hardware-software managed cache. We implement a proof-of-concept system using Xen and Linux running on a server with Intel processors, and show that LLC side channel attacks can be defeated. Furthermore, CATalyst only causes very small performance overhead when used for security, and has negligible impact on legacy applications.

---
### Predicting the memory bandwidth and optimal core allocations for multi-threaded applications on large-scale NUMA machines.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446083
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446083
* **Key Words**: Resource management, Bandwidth, Instruction sets, Benchmark testing, Scalability, Random access memory, Linear programming, integer programming, multi-threading, optimal core allocations, multithreaded applications, large-scale NUMA machines, NUMA platforms, parallelism, multithreading, performance scalability, asymmetric NUMA memory configuration, asymmetric application memory behavior, NUMA memory resources, NuCore, memory bandwidth usage, NUMA asymmetry, integer programming, NUMA memory systems, 
* **Abstract**: Modern NUMA platforms offer large numbers of cores to boost performance through parallelism and multi-threading. However, because performance scalability is limited by available memory bandwidth, the strategy of allocating all cores can result in degraded performance. Consequently, accurately predicting optimal (best performing) core allocations, and executing applications with these allocations are crucial for achieving the best performance. Previous research focused on the prediction of optimal numbers of cores. However, in this paper, we show that, because of the asymmetric NUMA memory configuration and the asymmetric application memory behavior, optimal core allocations are not merely optimal numbers of cores. Additionally, previous studies do not adequately consider NUMA memory resources, which further limits their ability to accurately predict optimal core allocations. In this paper, we present a model, NuCore, which predicts both memory bandwidth usage and optimal core allocations. NuCore considers various memory resources and NUMA asymmetry, and employs Integer Programming to achieve high accuracy and low overhead. Experimental results from real NUMA machines show that the core allocations predicted by NuCore provide 1.27x average speedup over using all cores with only 75.6% cores allocated. NuCore also provides 1.18x and 1.21x average speedups over two state-of-the-art techniques. Our results also show that NuCore faithfully models NUMA memory systems and predicts memory bandwidth usages with only 10% average error.

---
### A market approach for handling power emergencies in multi-tenant data center.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446084
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446084
* **Key Words**: Servers, Power demand, Uninterruptible power systems, Aggregates, Cooling, Partial discharges, Power measurement, computer centres, power aware computing, profitability, power emergencies, multitenant data center, market approach, power oversubscription, aggregate power demand, power capping solution, owner-operated data centers, COOP, total performance loss, power capping constraints, operator profit, 
* **Abstract**: Power oversubscription in data centers may occasionally trigger an emergency when the aggregate power demand exceeds the capacity. Handling such an emergency requires a graceful power capping solution that minimizes the performance loss. In this paper, we study power capping in a multi-tenant data center where the operator supplies power to multiple tenants that manage their own servers. Unlike owner-operated data centers, the operator lacks control over tenants' servers. To address this challenge, we propose a novel market mechanism based on supply function bidding, called COOP, to financially incentivize and coordinate tenants' power reduction for minimizing total performance loss (quantified in performance cost) while satisfying multiple power capping constraints. We build a prototype to show that COOP is efficient in terms of minimizing the total performance cost, even compared to the ideal but infeasible case that assumes the operator has full control over tenants' servers. We also demonstrate that COOP is "win-win", increasing the operator's profit (through oversubscription) and reducing tenants' cost (through financial compensation for their power reduction during emergencies).

---
### SizeCap: Efficiently handling power surges in fuel cell powered data centers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446085
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446085
* **Key Words**: Fuel cells, Electrostatic discharges, Surges, Servers, Fuels, Power generation, Power demand, computer centres, energy conservation, fuel cells, power system reliability, surge protection, SizeCap, power surges, fuel cell powered data center, energy efficiency, greenhouse gas emission, reliability, data center power demand, power shortfall, energy storage device, service level agreement, total cost of ownership, power capping policy, ESD size, SLA, 
* **Abstract**: Fuel cells are a promising power source for future data centers, offering high energy efficiency, low greenhouse gas emissions, and high reliability. However, due to mechanical limitations related to fuel delivery, fuel cells are slow to adjust to sudden increases in data center power demands, which can result in temporary power shortfalls. To mitigate the impact of power shortfalls, prior work has proposed to either perform power capping by throttling the servers, or to leverage energy storage devices (ESDs) that can temporarily provide enough power to make up for the shortfall while the fuel cells ramp up power generation. Both approaches have disadvantages: power capping conservatively limits server performance and can lead to service level agreement (SLA) violations, while ESD-only solutions must significantly overprovision the energy storage device capacity to tolerate the shortfalls caused by the worst-case (i.e., largest) power surges, which greatly increases the total cost of ownership (TCO). We propose SizeCap, the first ESD sizing framework for fuel cell powered data centers, which coordinates ESD sizing with power capping to enable a cost-effective solution to power shortfalls in data centers. SizeCap sizes the ESD just large enough to cover the majority of power surges, but not the worst-case surges that occur infrequently, to greatly reduce TCO. It then uses the smaller capacity ESD in conjunction with power capping to cover the power shortfalls caused by the worst-case power surges. As part of our new flexible framework, we propose multiple power capping policies with different degrees of awareness of fuel cell and workload behavior, and evaluate their impact on workload performance and ESD size. Using traces from Microsoft's production data center systems, we demonstrate that SizeCap significantly reduces the ESD size (by 85%ofor a workload with infrequent yet large power surges, and by 50% for a workload with frequent power surges) without violating ...

---
### MaPU: A novel mathematical computing architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446086
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446086
* **Key Words**: Computer architecture, Pipelines, Program processors, Kernel, Routing, Data models, Supercomputers, graphics processing units, mathematics computing, parallel architectures, parallel memories, performance evaluation, pipeline processing, power aware computing, mathematical computing architecture, semiconductor process, high performance processors, PLOPS, power consumption, power efficiency, sustained performance, MaPU architecture, data-intensive computing, innovative multigranularity parallel memory system, intrinsic shuffle ability, cascading pipelines, wide SIMD data path, state-machine-based program model, signal processing algorithm, single MaPU core, GLOPS, CPU, GPGPU, 
* **Abstract**: As the feature size of the semiconductor process is scaling down to 10nm and below, it is possible to assemble systems with high performance processors that can theoretically provide computational power of up to tens of PLOPS. However, the power consumption of these systems is also rocketing up to tens of millions watts, and the actual performance is only around 60% of the theoretical performance. Today, power efficiency and sustained performance have become the main foci of processor designers. Traditional computing architecture such as superscalar and GPGPU are proven to be power inefficient, and there is a big gap between the actual and peak performance. In this paper, we present the MaPU architecture, a novel architecture which is suitable for data-intensive computing with great power efficiency and sustained computation throughput. To achieve this goal, MaPU attempts to optimize the application from a system perspective, including the hardware, algorithm and corresponding program model. It uses an innovative multi-granularity parallel memory system with intrinsic shuffle ability, cascading pipelines with wide SIMD data paths and a state-machine-based program model. When executing typical signal processing algorithms, a single MaPU core implemented with a 40nm process exhibits a sustained performance of 134 GLOPS while consuming only 2.8 W in power, which increases the actual power efficiency by an order of magnitude comparable with the traditional CPU and GPGPU.

---
### Best-offset hardware prefetching.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446087
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446087
* **Key Words**: Prefetching, Hardware, System-on-chip, Pollution, History, Benchmark testing, cache storage, parallel processing, storage management, hardware prefetching, high-performance processors, on-chip caches, performance reduction, sandbox method, SPEC CPU2006 benchmarks, 
* **Abstract**: Hardware prefetching is an important feature of modern high-performance processors. When the application working set is too large to fit in on-chip caches, disabling hardware pre-fetchers may result in severe performance reduction. A new prefetcher was recently introduced, the Sandbox prefetcher, that tries to find dynamically the best prefetch offset using the sandbox method. The Sandbox prefetcher uses simple hardware and was shown to be quite effective. However, the sandbox method does not take into account prefetch timeliness. We propose an offset prefetcher with a new method for selecting the prefetch offset that takes into account prefetch timeliness. We show that our Best-Offset prefetcher outperforms the Sandbox prefetcher on the SPEC CPU2006 benchmarks, with equally simple hardware.

---
### DUANG: Fast and lightweight page migration in asymmetric memory systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446088
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446088
* **Key Words**: Phase change materials, Memory architecture, Random access memory, Memory management, Runtime, Resistance, buffer storage, memory architecture, DUANG, page migration, asymmetric memory systems, resistive memory architecture, migrating memory pages, memory access patterns, asymmetric memory architecture, shared row-buffer architecture, 
* **Abstract**: Main memory systems have gone through dramatic increases in bandwidth and capacity. At the same time, their random access latency has remained relatively constant. For given memory technology, optimizing the latency typically requires sacrificing the density (i.e., cost per bit), which is one of the most critical concerns for memory industry. Recent studies have proposed memory architectures comprised of asymmetric (fast/low-density and slow/high-density) regions to optimize between overall latency and negative impact on density. Such memory architectures attempt to cost-effectively offer both high capacity and high performance. Yet they present a unique challenge, requiring direct placements of hot memory pages 1 in the fast region and/or expensive runtime page migrations. In this paper, we propose a novel resistive memory architecture sharing a set of row buffers between a pair of neighboring banks. It enables two attractive techniques: (1) migrating memory pages between slow and fast banks with little performance overhead and (2) adaptively allocating more row buffers to busier banks based on memory access patterns. For an asymmetric memory architecture with both slow/high-density and fast/low-density banks, our shared row-buffer architecture can capture 87-93% of the performance of a memory architecture with only fast banks.

---
### Selective GPU caches to eliminate CPU-GPU HW cache coherence.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446089
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446089
* **Key Words**: Graphics processing units, Coherence, Hardware, Memory management, Bandwidth, Protocols, System-on-chip, cache storage, graphics processing units, microprocessor chips, protocols, shared memory systems, HW cache coherence, hardware cache coherence, shared memory multiprocessor, programming model, aggressive request coalescing, CPU-side coherent caching, CPU-GPU interconnect optimization, hardware coherence protocol, 
* **Abstract**: Cache coherence is ubiquitous in shared memory multiprocessors because it provides a simple, high performance memory abstraction to programmers. Recent work suggests extending hardware cache coherence between CPUs and GPUs to help support programming models with tightly coordinated sharing between CPU and GPU threads. However, implementing hardware cache coherence is particularly challenging in systems with discrete CPUs and GPUs that may not be produced by a single vendor. Instead, we propose, selective caching, wherein we disallow GPU caching of any memory that would require coherence updates to propagate between the CPU and GPU, thereby decoupling the GPU from vendor-specific CPU coherence protocols. We propose several architectural improvements to offset the performance penalty of selective caching: aggressive request coalescing, CPU-side coherent caching for GPU-uncacheable requests, and a CPU-GPU interconnect optimization to support variable-size transfers. Moreover, current GPU workloads access many read-only memory pages; we exploit this property to allow promiscuous GPU caching of these pages, relying on page-level protection, rather than hardware cache coherence, to ensure correctness. These optimizations bring a selective caching GPU implementation to within 93% of a hardware cache-coherent implementation without the need to integrate CPUs and GPUs under a single hardware coherence protocol.

---
### Venice: Exploring server architectures for effective resource sharing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446090
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446090
* **Key Words**: Servers, Resource management, Fabrics, Memory management, Software, Prototypes, computer centres, memory architecture, resource allocation, Venice framework, server architectures, IT infrastructure, data-center server architectures, communication substrate, resource-joining mechanisms, user programs, resource sharing, performance analysis, power analysis, memory sharing, accelerator sharing, NIC sharing, latency reduction, data-sharing access patterns, communication channels, interchannel collaboration, consolidated server racks, nonlocal resource leveraging, 
* **Abstract**: Consolidated server racks are quickly becoming the backbone of IT infrastructure for science, engineering, and business, alike. These servers are still largely built and organized as when they were distributed, individual entities. Given that many fields increasingly rely on analytics of huge datasets, it makes sense to support flexible resource utilization across servers to improve cost-effectiveness and performance. We introduce Venice, a family of data-center server architectures that builds a strong communication substrate as a first-class resource for server chips. Venice provides a diverse set of resource-joining mechanisms that enables user programs to efficiently leverage non-local resources. To better understand the implications of design decisions about system support for resource sharing we have constructed a hardware prototype that allows us to more accurately measure end-to-end performance of at-scale applications and to explore tradeoffs among performance, power, and resource-sharing transparency. We present results from our initial studies analyzing these tradeoffs when sharing memory, accelerators, or NICs. We find that it is particularly important to reduce or hide latency, that data-sharing access patterns should match the features of the communication channels employed, and that inter-channel collaboration can be exploited for better performance.

---
### A large-scale study of soft-errors on GPUs in the field.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446091
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446091
* **Key Words**: Graphics processing units, Supercomputers, Reliability, Computer architecture, Correlation, Error correction codes, Resource management, graphics processing units, parallel architectures, reliability, soft-errors, GPU architecture, large-scale cluster, GPU reliability, Titan supercomputer, 
* **Abstract**: Parallelism provided by the GPU architecture has enabled domain scientists to simulate physical phenomena at a much faster rate and finer granularity than what was previously possible by CPU-based large-scale clusters. Architecture researchers have been investigating reliability characteristics of GPUs and innovating techniques to increase the reliability of these emerging computing devices. Such efforts are often guided by technology projections and simplistic scientific kernels, and performed using architectural simulators and modeling tools. Lack of large-scale field data impedes the effectiveness of such efforts. This study attempts to bridge this gap by presenting a large-scale field data analysis of GPU reliability. We characterize and quantify different kinds of soft-errors on the Titan supercomputer's GPU nodes. Our study uncovers several interesting and previously unknown insights about the characteristics and impact of soft-errors.

---
### Design and implementation of a mobile storage leveraging the DRAM interface.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446092
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446092
* **Key Words**: Random access memory, Mobile communication, Protocols, Performance evaluation, Bandwidth, Computer architecture, DRAM chips, Linux, memory architecture, microprocessor chips, mobile computing, smart phones, storage management, user interfaces, mobile storage architecture, DRAM interface, user interface, host software stack, Linux device driver, boot loader, Android system, mobile CPU chip, 
* **Abstract**: Storage I/O performance remains a key factor that determines the overall user experience of a computer system. This is especially true for mobile systems as users commonly browse and navigate through many high-quality pictures and video clips stored in their device. The appetite for more appealing user interface has continuously pushed the mobile storage interface speed up; emerging UFS 2.0 standard provisions a maximum bandwidth of as high as 1,200 MB/s. In this work, we propose, design, and implement a mobile storage architecture that leverages the high-speed DRAM interface for communication, thus substantially expanding the storage performance headroom. In order to effectively turn the existing DRAM interface into a storage interface, we design a new storage protocol that runs on top of the DRAM interface. Our protocol builds on a small host interface buffer structure mapped to the system's memory space. Based on this protocol, we develop and fabricate a storage controller chip that natively supports the LPDDR3 interface. We also develop a host software stack (Linux device driver and boot loader) and a host platform board. Finally we show the feasibility of our proposal by constructing a full Android system running on the developed storage device and platform. Our detailed evaluation shows that the proposed storage architecture has very low protocol handling overheads and compares favorably to a UFS 2.0 device. The proposed architecture obviates the need for implementing a separate host-side storage controller on a mobile CPU chip.

---
### Restore truncation for performance improvement in future DRAM systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446093
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446093
* **Key Words**: Timing, Transistors, Capacitors, Sensors, DRAM chips, Hardware, DRAM chips, performance evaluation, power aware computing, performance improvement, DRAM systems, DRAM scaling, bit cell structure, DRAM chips, cell data restoration, read write access, restore truncation, restore strategy, DRAM module performance, restore timing, process variations, energy consumption, RT schemes, 
* **Abstract**: Scaling DRAM below 20nm has become a major challenge due to intrinsic limitations in the structure of a bit cell. Future DRAM chips are likely to suffer from significant variations and degraded timings, such as taking much more time to restore cell data after read and write access. In this paper, we propose restore truncation (RT), a low-cost restore strategy to improve performance of DRAM modules that adopt relaxed restore timing. After an access, RT restores a bit cell's voltage only to the level required to persist data to the next scheduled refresh rather than to the default full voltage. Because restore time is shortened, the performance of the cell is improved under process variations. We devise two schemes to balance performance, energy consumption, and hardware overhead. We simulate our proposed RT schemes and compare them with the state of the art. Experimental results show that, on average, RT improves performance by 19.5% and reduces energy consumption by 17%.

---
### Parity Helix: Efficient protection for single-dimensional faults in multi-dimensional memory systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446094
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446094
* **Key Words**: Resilience, DRAM chips, Reliability, Error correction, Through-silicon vias, Supercomputers, DRAM chips, Parity Helix, single-dimensional fault protection, multidimensional memory systems, die-stacked DRAM, bandwidth factor, energy efficiency factor, channel faults, die faults, memory subsystems, memory banks, DRAM banks, horizontal plane, vertical banks, error correction resources, error correction overhead minimization, memory capacity, memory energy reduction, 
* **Abstract**: Emerging die-stacked DRAMs provide several factors higher bandwidth and energy efficiency than 2D DRAMs, making them excellent candidates for future memory systems. To be deployed in server and high-performance computing systems, however, die-stacked DRAMs need to provide equivalent or better reliability than existing 2D DRAMs. This includes protecting against channel and die faults, which have been observed in existing 2D DRAM production systems. In this paper, we observe that memory subsystems can be viewed as a multi-dimensional collection of memory banks in which faults generally affect memory banks that lie along a single dimension. For instance, in die-stacked DRAMs, a die consists of a group of DRAM banks that lie in a horizontal plane while a channel consists of a vertical group of banks spanning across multiple dies. We exploit this fault behavior to propose Parity Helix to efficiently protect against single-dimensional faults in multi-dimensional memory systems. Parity Helix shares the same error correction resources across all dimensions to minimize error correction overheads. For die-stacked DRAMs, our evaluation shows that compared to a straightforward extension of previous schemes, Parity Helix increases memory capacity by 16.7%, reduces memory energy per program access by 21%, on average, and by up to 45%.

---
### Low-Cost Inter-Linked Subarrays (LISA): Enabling fast inter-subarray data movement in DRAM.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446095
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446095
* **Key Words**: DRAM chips, Substrates, Energy efficiency, System performance, Arrays, Buffer storage, cache storage, DRAM chips, logic design, low-cost interlinked subarrays, LISA, intersubarray data movement, bulk data movement, DRAM chip design, data transfer, off-chip channel, narrow channel, latency, energy consumption, DRAM bandwidth, wire connectivity, DRAM subarray, low-cost connections, internal wire interconnection, wide-bandwidth data transfer, DRAM area overhead, intersubarray bulk data copy, in-DRAM caching, DRAM architecture, heterogeneous access latencies, accelerated bitline precharging, performance improvement, memory energy efficiency improvement, 
* **Abstract**: This paper introduces a new DRAM design that enables fast and energy-efficient bulk data movement across subarrays in a DRAM chip. While bulk data movement is a key operation in many applications and operating systems, contemporary systems perform this movement inefficiently, by transferring data from DRAM to the processor, and then back to DRAM, across a narrow off-chip channel. The use of this narrow channel for bulk data movement results in high latency and energy consumption. Prior work proposed to avoid these high costs by exploiting the existing wide internal DRAM bandwidth for bulk data movement, but the limited connectivity of wires within DRAM allows fast data movement within only a single DRAM subarray. Each subarray is only a few megabytes in size, greatly restricting the range over which fast bulk data movement can happen within DRAM. We propose a new DRAM substrate, Low-Cost Inter-Linked Subarrays (LISA), whose goal is to enable fast and efficient data movement across a large range of memory at low cost. LISA adds low-cost connections between adjacent subarrays. By using these connections to interconnect the existing internal wires (bitlines) of adjacent subarrays, LISA enables wide-bandwidth data transfer across multiple subarrays with little (only 0.8%) DRAM area overhead. As a DRAM substrate, LISA is versatile, enabling an array of new applications. We describe and evaluate three such applications in detail: (1) fast inter-subarray bulk data copy, (2) in-DRAM caching using a DRAM architecture whose rows have heterogeneous access latencies, and (3) accelerated bitline precharging by linking multiple precharge units together. Our extensive evaluations show that each of LISA's three applications significantly improves performance and memory energy efficiency, and their combined benefit is higher than the benefit of each alone, on a variety of workloads and system configurations.

---
### ChargeCache: Reducing DRAM latency by exploiting row access locality.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446096
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446096
* **Key Words**: DRAM chips, Capacitors, Timing, Hardware, Parallel processing, Computer architecture, cache storage, DRAM chips, storage allocation, ChargeCache, DRAM latency reduction, row access locality, system performance bottleneck, DRAM chips, memory controller, DRAM request, timing parameters, row address, single-core system, multicore system, 
* **Abstract**: DRAM latency continues to be a critical bottleneck for system performance. In this work, we develop a low-cost mechanism, called ChargeCache, that enables faster access to recently-accessed rows in DRAM, with no modifications to DRAM chips. Our mechanism is based on the key observation that a recently-accessed row has more charge and thus the following access to the same row can be performed faster. To exploit this observation, we propose to track the addresses of recently-accessed rows in a table in the memory controller. If a later DRAM request hits in that table, the memory controller uses lower timing parameters, leading to reduced DRAM latency. Row addresses are removed from the table after a specified duration to ensure rows that have leaked too much charge are not accessed with lower latency. We evaluate ChargeCache on a wide variety of workloads and show that it provides significant performance and energy benefits for both single-core and multi-core systems.

---
### Amdahl's law for lifetime reliability scaling in heterogeneous multicore processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446097
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446097
* **Key Words**: Program processors, Multicore processing, Reliability, Processor scheduling, Computational modeling, Stress, Microarchitecture, energy conservation, multiprocessing systems, multi-threading, performance evaluation, processor scheduling, reliability, Amdahl law, lifetime reliability scaling, heterogeneous multicore processors, microarchitectural designs, energy efficiency, heterogeneous models, homogeneous systems, compact thermal estimation, device aging, core utilization, Amdahl scaling factor, processor composition, thread scheduling, processor performance, 
* **Abstract**: Heterogeneous multicore processors have been suggested as alternative microarchitectural designs to enhance performance and energy efficiency. Using Amdahl's Law, heterogeneous models were primarily analyzed in performance and energy efficiency aspects to demonstrate its advantage over conventional homogeneous systems. In this paper, we further extend the study to understand the lifetime reliability consequences of heterogeneous multicore processors, as reliability becomes an increasingly important constraint. We present the lifetime reliability models of multicore processors based on Amdahl's Law, including compact thermal estimation that has strong correlation with device aging. Lifetime reliability is analyzed by varying i) core utilization (Amdahl's scaling factor), ii) processor composition (number of big and small cores), and iii) thread scheduling method. The study shows that the heterogeneous processor may have a serious reliability challenge. If the processor is comprised of only one big core and many small cores, stresses can be biased to the big core especially when workloads spend more time on sequential operations. Our study reveals that incorporating multiple big cores can mitigate reliability bottleneck in big cores and enhance processor lifetime, but adding too many big cores will have an adverse impact on lifetime reliability as well as performance.

---
### LiveSim: Going live with microarchitecture simulation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446098
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446098
* **Key Words**: Microarchitecture, Benchmark testing, Computational modeling, Simulation, Load modeling, Real-time systems, Computers, checkpointing, digital simulation, storage management, LiveSim, microarchitectural simulation methodology, in-memory checkpoints, random execution, LiveSample, confidence intervals, target confidence level, LiveCI, SPEC CPU 2006 benchmarks, computer architects, 
* **Abstract**: Computer architects rely heavily on software-based microarchitecture simulators, which typically take hours or days to produce results. We have developed LiveSim, a novel microarchitectural simulation methodology that provides simulation results within seconds, making it suitable for interactive use. LiveSim works by creating in-memory checkpoints of application state, and then executing randomly selected samples from these checkpoints in parallel to produce simulation results. The initial results, which we call LiveSample, are reported less than one second after starting the simulation. As more samples are simulated the results become more accurate and are updated in real-time. Once enough samples are gathered, LiveSim provides confidence intervals for the reported values and continues simulation until it reaches the target confidence level, which we call LiveCI. We evaluated LiveSim using SPEC CPU 2006 benchmarks and found that within 5 seconds after starting simulation, LiveSample results reached an average error of 3.51% compared to full simulation, and the LiveCI results were available within 41 seconds on average.

---
### McVerSi: A test generation framework for fast memory consistency verification in simulation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446099
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446099
* **Key Words**: Computer bugs, Hardware, Coherence, Protocols, Throughput, Pipelines, Measurement, digital simulation, formal specification, formal verification, genetic algorithms, parallel programming, program debugging, program testing, shared memory systems, McVerSi, test generation framework, memory consistency model, MCM, formal verification, formal specification, full-system simulation, genetic programming, GP, program bug, parallel program, multiprocessor system, 
* **Abstract**: The memory consistency model (MCM), which formally specifies the behaviour of the memory system, is used by programmers to reason about parallel programs. It is imperative that hardware adheres to the promised MCM. For this reason, hardware designs must be verified against the specified MCM. One common way to do this is via executing tests, where specific threads of instruction sequences are generated and their executions are checked for adherence to the MCM. It would be extremely beneficial to execute such tests under simulation, i.e. when the functional design implementation of the hardware is being prototyped. Most prior verification methodologies, however, target post-silicon environments, which when applied under simulation would be too slow. We propose McVerSi, a test generation framework for fast MCM verification of a full-system design implementation under simulation. Our primary contribution is a Genetic Programming (GP) based approach to MCM test generation, which relies on a novel crossover function that prioritizes memory operations contributing to non-determinism, thereby increasing the probability of uncovering MCM bugs. To guide tests towards exercising as much logic as possible, the simulator's reported coverage is used as the fitness function. Furthermore, we increase test throughput by making the test workload simulation-aware. We evaluate our proposed framework using the Gem5 cycle accurate simulator in full-system mode with Ruby. We discover 2 new bugs due to the faulty interaction of the pipeline and the cache coherence protocol. Crucially, these bugs would not have been discovered through individual verification of the pipeline or the coherence protocol. We study 11 bugs in total. Our GP-based test generation approach finds all bugs consistently, therefore providing much higher guarantees compared to alternative approaches (pseudo-random test generation and litmus tests).

---
### Energy-efficient address translation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446100
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446100
* **Key Words**: Organizations, Hardware, Memory management, Bridges, Monitoring, Complexity theory, Proposals, buffer storage, energy conservation, microprocessor chips, storage allocation, energy-efficient address translation, processor performance, translation lookaside buffer miss, energy cost reduction, Lite mechanism, performance monitoring, L1 TLB utility monitoring, TLB miss cycle, performance overhead, redundant memory mapping, RMM address translation mechanism, process address space, virtual address space, physical address space, dynamic energy, energy efficient memory system design, memory demands, 
* **Abstract**: Address translation is fundamental to processor performance. Prior work focused on reducing Translation Lookaside Buffer (TLB) misses to improve performance and energy, whereas we show that even TLB hits consume a significant amount of dynamic energy. To reduce the energy cost of address translation, we first propose Lite, a mechanism that monitors the performance and utility of L1 TLBs, and adaptively changes their sizes with way-disabling. The resulting TLBLite organization opportunistically reduces the dynamic energy spent in address translation by 23% on average with minimal impact on TLB miss cycles. To further reduce the energy and performance overheads of L1 TLBs, we also propose RMMLite that targets the recently proposed Redundant Memory Mappings (RMM) address-translation mechanism. RMM maps most of a process's address space with arbitrarily large ranges of contiguous pages in both virtual and physical address space using a modest number of entries in a range TLB. RMMLite adds to RMM an L1-range TLB and the Lite mechanism. The high hit ratio of the L1-range TLB allows Lite to downsize the L1-page TLBs more aggressively. RMMLite reduces the dynamic energy spent in address translation by 71% on average. Above the near-zero L2 TLB misses from RMM, RMMLite further reduces the overhead from L1 TLB misses by 99%. These proposed designs target current and future energy-efficient memory system design to meet the ever increasing memory demands of applications.

---
### RADAR: Runtime-assisted dead region management for last-level caches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446101
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446101
* **Key Words**: Runtime, Programming, Radar tracking, Niobium, Synchronization, Radar detection, cache storage, parallel programming, RADAR, runtime-assisted dead region management, last-level cache, LLC, processor/memory speed gap, hybrid static/dynamic dead-block management technique, address region granularity, task-parallel programming models, static control-flow information, region access patterns, runtime system, history-based scheme, look-back, look-ahead, hardware dead-block prediction technique, 
* **Abstract**: Last-level caches (LLCs) bridge the processor/memory speed gap and reduce energy consumed per access. Unfortunately, LLCs are poorly utilized because of the relatively large occurrence of dead blocks. We propose RADAR, a hybrid static/dynamic dead-block management technique that can accurately predict and evict dead blocks in LLCs. RADAR does dead-block prediction and eviction at the granularity of address regions supported in many of today's task-parallel programming models. The runtime system utilizes static control-flow information about future region accesses in conjunction with past region access patterns to make accurate predictions about dead regions. The runtime system instructs the cache to demote and eventually evict blocks belonging to such dead regions. This paper considers three RADAR schemes to predict dead regions: a scheme that uses control-flow information provided by the programming model (Look-ahead), a history-based scheme (Look-back) and a combined scheme (Look-ahead and Look-back). Our evaluation shows that, on average, all RADAR schemes outperform state-of-the-art hardware dead-block prediction techniques, whereas the combined scheme always performs best.

---
### Cache QoS: From concept to reality in the Intel® Xeon® processor E5-2600 v3 product family.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446102
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446102
* **Key Words**: Monitoring, Quality of service, Resource management, Registers, Servers, System software, cache storage, computer centres, quality of service, shared memory systems, virtual machines, Intel Xeon processor E5-2600 v3 product family, quality of service, multicore server platforms, shared resource contention, virtual machines, shared cache contention, shared cache QoS techniques, cache monitoring technology, cache allocation technology, multiprogrammed workloads, virtualized platforms, data centers, 
* **Abstract**: Over the last decade, addressing quality of service (QoS) in multi-core server platforms has been growing research topic. QoS techniques have been proposed to address the shared resource contention between co-running applications or virtual machines in servers and thereby provide better isolation, performance determinism and potentially improve overall throughput. One of the most important shared resources is cache space. Most proposals for addressing shared cache contention are based on simulations and analysis and no commercial platforms were available that integrated such techniques and provided a practical solution. In this paper, we will present the first set of shared cache QoS techniques designed and implemented in state-of-the-art commercial servers (the Intel® Xeon® processor E5-2600 v3 product family). We will describe two key technologies: (i) Cache Monitoring Technology (CMT) to enable monitoring of shared cache usage by different applications and (ii) Cache Allocation Technology (CAT) which enables redistribution of shared cache space between applications to address contention. This is the first paper to describing these techniques as they moved from concept to reality, starting from early research to product implementation. We will also present case studies highlighting the value of these techniques using example scenarios of multi-programmed workloads, virtualized platforms in datacenters and communications platforms. Finally, we will describe initial software infrastructure and enabling for industry practitioners and researchers to take advantage of these technologies for their QoS needs.

---
### Symbiotic job scheduling on the IBM POWER8.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446103
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446103
* **Key Words**: Multicore processing, Symbiosis, Program processors, Interference, Predictive models, Optimal scheduling, Linux, Linux, multiprocessing systems, multiprogramming, multi-threading, scheduling, symbiotic job scheduling, simultaneous multithreading processor, SMT processor, microarchitectural core components, shared resources, symbiotic job scheduler, IBM POWER8 processor, cycle accounting mechanism, symbiosis, Linux operating system, IBM POWER8 server, multiprogrammed workloads, agnostic random scheduler, default Linux scheduler, 
* **Abstract**: Simultaneous multithreading (SMT) processors share most of the microarchitectural core components among the co-running applications. The competition for shared resources causes performance interference between applications. Therefore, the performance benefits of SMT processors heavily depend on the complementarity of the co-running applications. Symbiotic job scheduling, i.e., scheduling applications that co-run well together on a core, can have a considerable impact on the performance of a processor with SMT cores. Prior work uses sampling or novel hardware support to perform symbiotic job scheduling, which has either a non-negligible overhead or is impossible to use on existing hardware. This paper proposes a symbiotic job scheduler for the IBM POWER8 processor. We leverage the existing cycle accounting mechanism to predict symbiosis between applications, and use that information at run-time to decide which applications should run on the same core or on separate cores. We implement the scheduler in the Linux operating system and evaluate it on an IBM POWER8 server running multiprogrammed workloads. The symbiotic job scheduler significantly improves performance compared to both an agnostic random scheduler and the default Linux scheduler. With respect to Linux, it achieves an average speedup by 8.8% for workloads comprising 12 applications, and by 4.7% on average across all evaluated workloads.

---
### ScalCore: Designing a core for voltage scalability.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446104
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446104
* **Key Words**: Pipelines, Multicore processing, Scalability, Transistors, Delays, Program processors, Registers, multiprocessing systems, pipeline processing, power aware computing, voltage scalability, multicore system, energy-efficient execution modes, DVFS, low-voltage core design, high-performance mode, energy-efficient mode, ScalCore. core, EEMode, logic structures, storage structures, storage-intensive pipeline stage, average program execution time reduction, HPMode, 
* **Abstract**: Upcoming multicores need to provide increasingly stringent energy-efficient execution modes. Currently, energy efficiency is attained by lowering the voltage (V dd ) through DVFS. However, the effectiveness of DVFS is limited: designing cores for low V dd results in energy inefficiency at nominal V dd . Our goal is to design a core for Voltage Scalability, i.e., one that can work in high-performance mode (HPMode) at nominal V dd , and in a very energy-efficient mode (EEMode) at low V dd . We call this core ScalCore. To operate energy-efficiently in EEMode, ScalCore introduces two ideas. First, since logic and storage structures scale differently with V dd , ScalCore applies two low V dd s to the pipeline: one to the logic stages (V logic ) and a higher one to storage-intensive stages. Secondly, ScalCore further increases the low V dd of the storage-intensive stages (V op ), so that they are substantially faster than the logic ones. Then, it exploits the speed differential by either fusing storage-intensive pipeline stages or increasing the size of storage structures in the pipeline. Our simulations of 16 cores show that a design with ScalCores in EEMode is much more energy-efficient than one with conventional cores and aggressive DVFS: for approximately the same power, ScalCores reduce the average execution time of programs by 31%, the energy (E) consumed by 48%, and the ED product by 60%. In addition, dynamically switching between EEMode and HPMode based on program phases is very effective: it reduces the average execution time and ED product by a further 28% and 15%, respectively.

---
### Cost effective physical register sharing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2016.7446105
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7446105
* **Key Words**: Registers, Radiation detectors, Optimization, History, Pipelines, Tracking, Patents, buffer storage, checkpointing, optimising compilers, cost effective physical register sharing, microarchitectural optimization, register reclaiming process, physical register allocation, architectural destination register, register reference counting, dependency matrix, per register counter, branch misprediction recovery, sequential rollback, inflight shared register buffer, ISRB, storage overhead, checkpoint-based recovery, pipeline flush, move elimination, short-circuiting moves, speculative memory bypassing, short-circuiting store-load pair, TAGE-like predictor, memory dependenc identification, small register tracking structure, 
* **Abstract**: Sharing a physical register between several instructions is needed to implement several microarchitectural optimizations. However, register sharing requires modifications to the register reclaiming process: Committing a single instruction does not guarantee that the physical register allocated to the previous mapping of its architectural destination register is free-able anymore. Consequently, a form of register reference counting must be implemented. While such mechanisms (e.g., dependency matrix, per register counters) have been described in the literature, we argue that they either require too much storage, or that they lengthen branch misprediction recovery by requiring sequential rollback. As an alternative, we present the Inflight Shared Register Buffer (ISRB), a new structure for register reference counting. The ISRB has low storage overhead and lends itself to checkpoint-based recovery schemes, therefore allowing fast recovery on pipeline flushes. We illustrate our scheme with Move Elimination (short-circuiting moves) and an implementation of Speculative Memory Bypassing (short-circuiting store-load pairs) that makes use of a TAGE-like predictor to identify memory dependencies. We show that the whole potential of these two mechanisms can be achieved with a small register tracking structure.
