### hpca 2017 | 55 papers.
---
### Towards Pervasive and User Satisfactory CNN across GPU Microarchitectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.52
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920809
* **Key Words**: Feature extraction, Frequency modulation, Real-time systems, Computer architecture, Convolution, Support vector machines, Entropy, feedforward neural nets, graphics processing units, inference mechanisms, operating system kernels, user satisfactory CNN, pervasive CNN, GPU microarchitectures, convolutional neural networks, two-stage process, high-end GPU-equipped servers, mobile GPU, P-CNN, user satisfaction-aware CNN inference framework, cross-platform offline compilation, run-time management, user requirements, offline compilation, optimal kernel, architecture-independent techniques, adaptive batch size selection, coordinated fine-tuning, run-time management, coordinated fine-tuning, runtime management phase, accuracy tuning, run-time kernel scheduler partitions, optimal computing resource, GPU thread blocks, user satisfaction metric, pervasive deign, 
* **Abstract**: Accelerating Convolutional Neural Networks (CNNs) on GPUs usually involves two stages: training and inference. Traditionally, this two-stage process is deployed on high-end GPU-equipped servers. Driven by the increase in compute power of desktop and mobile GPUs, there is growing interest in performing inference on various kinds of platforms. In contrast to the requirements of high throughput and accuracy during the training stage, end-users will face diverse requirements related to inference tasks. To address this emerging trend and new requirements, we propose Pervasive CNN (P-CNN), a user satisfaction-aware CNN inference framework. P-CNN is composed of two phases: cross-platform offline compilation and run-time management. Based on users' requirements, offline compilation generates the optimal kernel using architecture-independent techniques, such as adaptive batch size selection and coordinated fine-tuning. The runtime management phase consists of accuracy tuning, execution, and calibration. First, accuracy tuning dynamically identifies the fastest kernels with acceptable accuracy. Next, the run-time kernel scheduler partitions the optimal computing resource for each layer and schedules the GPU thread blocks. If its accuracy is not acceptable to the end-user, the calibration stage selects a slower but more precise kernel to improve the accuracy. Finally, we design a user satisfaction metric for CNNs to evaluate our Pervasive deign. Our evaluation results show P-CNN can provide the best user satisfaction for different inference tasks.

---
### Near-Optimal Access Partitioning for Memory Hierarchies with Multiple Heterogeneous Bandwidth Sources.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.46
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920810
* **Key Words**: Bandwidth, Random access memory, Metals, Memory architecture, Optimization, Metadata, Benchmark testing, cache storage, DRAM chips, learning (artificial intelligence), near-optimal access partitioning, memory hierarchy, multiple heterogeneous bandwidth sources, memory wall, performance bottleneck, on-die cache, memory technologies, embedded DRAM, eDRAM, high bandwidth memory, HBM, CPU package, DDR main memory, memory-side cache, system performance, cache utilization, dynamic access partitioning algorithm, cache hit rate, near-optimal bandwidth partitioning, light-weight learning mechanism, die-stacked memory-side DRAM cache, bandwidth points, capacity points, on-chip SRAM cache hierarchy, DRAM cache, memory system bandwidth, access partitioning, 
* **Abstract**: The memory wall continues to be a major performance bottleneck. While small on-die caches have been effective so far in hiding this bottleneck, the ever-increasing footprint of modern applications renders such caches ineffective. Recent advances in memory technologies like embedded DRAM (eDRAM) and High Bandwidth Memory (HBM) have enabled the integration of large memories on the CPU package as an additional source of bandwidth other than the DDR main memory. Because of limited capacity, these memories are typically implemented as a memory-side cache. Driven by traditional wisdom, many of the optimizations that target improving system performance have been tried to maximize the hit rate of the memory-side cache. A higher hit rate enables better utilization of the cache, and is therefore believed to result in higher performance. In this paper, we challenge this traditional wisdom and present DAP, a Dynamic Access Partitioning algorithm that sacrifices cache hit rates to exploit under-utilized bandwidth available at main memory. DAP achieves a near-optimal bandwidth partitioning between the memory-side cache and main memory by using a light-weight learning mechanism that needs just sixteen bytes of additional hardware. Simulation results show a 13% average performance gain when DAP is implemented on top of a die-stacked memory-side DRAM cache. We also show that DAP delivers large performance benefits across different implementations, bandwidth points, and capacity points of the memory-side cache, making it a valuable addition to any current or future systems based on multiple heterogeneous bandwidth sources beyond the on-chip SRAM cache hierarchy.

---
### NCAP: Network-Driven, Packet Context-Aware Power Management for Client-Server Architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.57
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920811
* **Key Words**: Program processors, Servers, Time factors, Kernel, Computer architecture, Linux, Energy efficiency, client-server systems, contracts, energy conservation, network interfaces, telecommunication power management, ubiquitous computing, NCAP, network-driven packet context-aware power management, client-server architecture, network packets, energy efficiency, low-performance state, sleep state, performance penalty, high-performance state, service level agreement, server operators, network interface card, received network packets, transmitted network packets, latency-critical requests, online data-intensive applications, OLDI, SLA, load levels, server, on-line data-intensive applications, power management, 
* **Abstract**: The rate of network packets encapsulating requests from clients can significantly affect the utilization, and thus performance and sleep states of processors in servers deploying a power management policy. To improve energy efficiency, servers may adopt an aggressive power management policy that frequently transitions a processor to a low-performance or sleep state at a low utilization. However, such servers may not respond to a sudden increase in the rate of requests from clients early enough due to a considerable performance penalty of transitioning a processor from a sleep or low-performance state to a high-performance state. This in turn entails violations of a service level agreement (SLA), discourages server operators from deploying an aggressive power management policy, and thus wastes energy during low-utilization periods. For both fast response time and high energy-efficiency, we propose NCAP, Network-driven, packet Context-Aware Power management for client-server architecture. NCAP enhances a network interface card (NIC) and its driver such that it can examine received and transmitted network packets, determine the rate of network packets containing latency-critical requests, and proactively transition a processor to an appropriate performance or sleep state. To demonstrate the efficacy, we evaluate on-line data-intensive (OLDI) applications and show that a server deploying NCAP consumes 37~61% lower processor energy than a baseline server while satisfying a given SLA at various load levels.

---
### Supporting Address Translation for Accelerator-Centric Architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.19
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920812
* **Key Words**: Acceleration, Arrays, Performance evaluation, Hardware, Pipeline processing, cache storage, computer architecture, computer networks, data communication, IP networks, microprocessor chips, virtual machines, address translation support, accelerator-centric architectures, orders-of-magnitude performance, energy improvements, rigid programming model, unified virtual address space, host CPU cores, customized accelerators, hardware support, infinite-sized TLB, zero page walk latency, IOMMU, memory access behavior, TLB augmentation, MMU designs, consecutive data bulk transfers, customized accelerator scratchpad memory, memory system, private TLB design, low-latency translations caching, data tiling techniques, shared level-two TLB, private TLB misses, virtual pages, duplicate page walk elimination, dedicated MMU, hardware complexity, host per-core MMU, page walk handling, CPU MMU, 
* **Abstract**: While emerging accelerator-centric architectures offer orders-of-magnitude performance and energy improvements, use cases and adoption can be limited by their rigid programming model. A unified virtual address space between the host CPU cores and customized accelerators can largely improve the programmability, which necessitates hardware support for address translation. However, supporting address translation for customized accelerators with low overhead is nontrivial. Prior studies either assume an infinite-sized TLB and zero page walk latency, or rely on a slow IOMMU for correctness and safety- which penalizes the overall system performance. To provide efficient address translation support for accelerator-centric architectures, we examine the memory access behavior of customized accelerators to drive the TLB augmentation and MMU designs. First, to support bulk transfers of consecutive data between the scratchpad memory of customized accelerators and the memory system, we present a relatively small private TLB design to provide low-latency caching of translations to each accelerator. Second, to compensate for the effects of the widely used data tiling techniques, we design a shared level-two TLB to serve private TLB misses on common virtual pages, eliminating duplicate page walks from accelerators working on neighboring data tiles that are mapped to the same physical page. This two-level TLB design effectively reduces page walks by 75.8% on average. Finally, instead of implementing a dedicated MMU which introduces additional hardware complexity, we propose simply leveraging the host per-core MMU for efficient page walk handling. This mechanism is based on our insight that the existing MMU cache in the CPU MMU satisfies the demand of customized accelerators with minimal overhead. Our evaluation demonstrates that the combined approach incurs only a 6.4% performance overhead compared to the ideal address translation.

---
### Vulnerabilities in MLC NAND Flash Memory Programming: Experimental Analysis, Exploits, and Mitigation Techniques.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.61
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920813
* **Key Words**: Programming, Threshold voltage, Flash memories, Computer architecture, Microprocessors, Interference, Transistors, circuit reliability, driver circuits, flash memories, NAND circuits, security, MLC NAND flash memory programming, multilevel cell, flash cell programming, cell-to-cell program interference, reliability, security, solid-state drives, SSD, 
* **Abstract**: Modern NAND flash memory chips provide high density by storing two bits ofdata in each flash cell, called a multi-level cell (MLC). An MLC partitions the threshold voltage range of a flash cell into four voltage states. When aflash cell is programmed, a high voltage is applied to the cell. Due to parasitic capacitance coupling between flash cells that are physically close to each other, flash cell programming can lead to cell-to-cell program interference, which introduces errors into neighboring flash cells. In order to reduce the impact of cell-to-cell interference on the reliability ofMLC NAND flash memory, flash manufacturers adopt a two-step programming method, which programs the MLC in two separate steps. First, the flash memory partially programs the least significant bit of the MLC to some intermediate threshold voltage. Second, it programs the most significant bit to bring the MLC up to its full voltage state. In this paper, we demonstrate that two-step programming exposes new reliability and security vulnerabilities. We experimentally characterize the effects of two-step programming using contemporary 1X-nm (i.e., 15-19nm)flash memory chips. Wefind that a partially-programmed flash cell (i.e., a cell where the second programming step has not yet been performed) is much more vulnerable to cell-to-cell interference and read disturb than a fully-programmed cell. We show that it is possible to exploit these vulnerabilities on solid-state drives (SSDs) to alter the partially-programmed data, causing (potentially malicious) data corruption. Building on our experimental observations, we propose several new mechanisms for MLC NAND flash memory that eliminate or mitigate data corruption in partially-programmed cells, thereby removing or reducing the extent of the vulnerabilities, and at the same time increasing flash memory lifetime by 16%.

---
### Defect Analysis and Cost-Effective Resilience Architecture for Future DRAM Devices.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.30
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920814
* **Key Words**: Random access memory, Circuit faults, Error correction codes, Capacitors, Capacitance, Performance evaluation, Transistors, DRAM chips, error correction codes, fault diagnosis, performance evaluation, cost-effective resilience architecture, DRAM devices, energy efficiency, DRAM-based main memory systems, defective cells, DRAM cells, random telegraph noise, variable retention time, randomly scattered single-cell faults, high area efficiency, performance degradation, DRAM access time, In-DRAM ECC codeword, rank-level ECC, In-DRAM ECC architecture, error correction code, DRAM, resilience, In-DRAM ECC, single-cell faults, long-burst writes, 
* **Abstract**: Technology scaling has continuously improved the density, performance, energy efficiency, and cost of DRAM-based main memory systems. Starting from sub-20nm processes, however, the industry began to pay considerably higher costs to screen and manage notably increasing defective cells. The traditional technique, which replaces the rows/columns containing faulty cells with spare rows/columns, has been able to cost-effectively repair the defective cells so far, but it will become unaffordable soon because an excessive number of spare rows/columns are required to manage the increasing number of defective cells. This necessitates a synergistic application of an alternative resilience technique such as In-DRAM ECC with the traditional one. Through extensive measurement and simulation, we first identify that aggressive miniaturization makes DRAM cells more sensitive to random telegraph noise or variable retention time, which is dominantly manifested as a surge in randomly scattered single-cell faults. Second, we advocate using InDRAM ECC to overcome the DRAM scaling challenges and architect In-DRAM ECC to accomplish high area efficiency and minimal performance degradation. Moreover, we show that advancement in process technology reduces decoding/correction time to a small fraction of DRAM access time, and that the throughput penalty of a write operation due to an additional read for a parity update is mostly overcome by the multi-bank structure and long burst writes that span an entire In-DRAM ECC codeword. Lastly, we demonstrate that system reliability with modern rank-level ECC schemes such as single device data correction is further improved by hundred million times with the proposed In-DRAM ECC architecture.

---
### Architecting an Energy-Efficient DRAM System for GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.58
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920815
* **Key Words**: Random access memory, Bandwidth, Computer architecture, Graphics processing units, Graphics, Throughput, buffer circuits, buffer storage, DRAM chips, graphics processing units, energy-efficient DRAM system, GPU, DRAM architecture, throughput processors, DRAM row buffers, DRAM bank, DRAM datapath, semi-independent subchannels, static data reordering scheme, toggle rate, energy consumption, die-stacked DRAM, memory access protocol, 
* **Abstract**: This paper proposes an energy-efficient, high-throughput DRAM architecture for GPUs and throughput processors. In these systems, requests from thousands of concurrent threads compete for a limited number of DRAM row buffers. As a result, only a fraction of the data fetched into a row buffer is used, leading to significant energy overheads. Our proposed DRAM architecture exploits the hierarchical organization of a DRAM bank to reduce the minimum row activation granularity. To avoid significant incremental area with this approach, we must partition the DRAM datapath into a number of semi-independent subchannels. These narrow subchannels increase data toggling energy which we mitigate using a static data reordering scheme designed to lower the toggle rate. This design has 35% lower energy consumption than a die-stacked DRAM with 2.6% area overhead. The resulting architecture, when augmented with an improved memory access protocol, can support parallel operations across the semi-independent subchannels, thereby improving system performance by 13% on average for a range of workloads.

---
### Design and Analysis of an APU for Exascale Computing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.42
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920816
* **Key Words**: Graphics processing units, Computer architecture, Three-dimensional displays, Central Processing Unit, Bandwidth, Architecture, Supercomputers, parallel architectures, parallel machines, APU, exascale computing, exaflop levels, memory capacity, memory bandwidth, power efficiency, exascale systems, exascale node architecture, ENA, exascale supercomputer, exascale heterogeneous processor, EHP, memory system, high-performance accelerated processing unit, CPU, GPU, high-bandwidth 3D memory, die-stacking, chiplet technologies, 
* **Abstract**: The challenges to push computing to exaflop levels are difficult given desired targets for memory capacity, memory bandwidth, power efficiency, reliability, and cost. This paper presents a vision for an architecture that can be used to construct exascale systems. We describe a conceptual Exascale Node Architecture (ENA), which is the computational building block for an exascale supercomputer. The ENA consists of an Exascale Heterogeneous Processor (EHP) coupled with an advanced memory system. The EHP provides a high-performance accelerated processing unit (CPU+GPU), in-package high-bandwidth 3D memory, and aggressive use of die-stacking and chiplet technologies to meet the requirements for exascale computing in a balanced manner. We present initial experimental analysis to demonstrate the promise of our approach, and we discuss remaining open research challenges for the community.

---
### BRAVO: Balanced Reliability-Aware Voltage Optimization.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.56
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920817
* **Key Words**: Measurement, Reliability engineering, Power system reliability, Integrated circuit reliability, Industries, Threshold voltage, circuit optimisation, circuit reliability, microprocessor chips, multiprocessing systems, BRAVO, balanced reliability-aware voltage optimization, processor micro-architecture, multidimensional optimization, reliability-aware design space exploration, multicore simulation framework, balanced reliability metric, BRM, Processor reliability, soft errors, hard errors, optimal voltage, 
* **Abstract**: Defining a processor micro-architecture for a targeted product space involves multi-dimensional optimization across performance, power and reliability axes. A key decision in such a definition process is the circuitand technology-driven parameter of the nominal (voltage, frequency) operating point. This is a challenging task, since optimizing individually or pair-wise amongst these metrics usually results in a design that falls short of the specification in at least one of the three dimensions. Aided by academic research, industry has now adopted early-stage definition methodologies that consider both energyand performance-related metrics. Reliabilityrelated enhancements, on the other hand, tend to get factored in via a separate thread of activity. This task is typically pursued without thorough pre-silicon quantifications of the energy or even the performance cost. In the late-CMOS design era, reliability needs to move from a post-silicon afterthought or validation-only effort to a pre-silicon definition process. In this paper, we present BRAVO, a methodology for such reliability-aware design space exploration. BRAVO is supported by a multi-core simulation framework that integrates performance, power and reliability modeling capability. Errors induced by both soft and hard fault incidence are captured within the reliability models. We introduce the notion of the Balanced Reliability Metric (BRM), that we use to evaluate overall reliability of the processor across soft and hard error incidences. We demonstrate up to 79% improvement in reliability in terms of this metric, for only a 6% drop in overall energy efficiency over design points that maximize energy efficiency. We also demonstrate several real-life usecase applications of BRAVO in an industrial setting.

---
### Maximizing Cache Performance Under Uncertainty.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.43
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920818
* **Key Words**: Uncertainty, Analytical models, Marine vehicles, Measurement, Markov processes, Electronics packaging, Economics, cache storage, decision making, Markov processes, cache performance, cache replacement, MIN, economic value added, EVA, Markov decision processes, cache replacement, cache modeling, Markov decision processes, 
* **Abstract**: Much prior work has studied cache replacement, but a large gap remains between theory and practice. The design of many practical policies is guided by the optimal policy, Belady's MIN. However, MIN assumes perfect knowledge of the future that is unavailable in practice, and the obvious generalizations of MIN are suboptimal with imperfect information. What, then, is the right metric for practical cache replacement? We propose that practical policies should replace lines based on their economic value added (EVA), the difference of their expected hits from the average. Drawing on the theory of Markov decision processes, we discuss why this metric maximizes the cache's hit rate. We present an inexpensive implementation of EVA and evaluate it exhaustively. EVA outperforms several prior policies and saves area at iso-performance. These results show that formalizing cache replacement yields practical benefits.

---
### SWAP: Effective Fine-Grain Management of Shared Last-Level Caches with Minimum Hardware Support.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.65
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920819
* **Key Words**: Color, Hardware, Program processors, Probabilistic logic, Servers, Throughput, Interference, cache storage, microprocessor chips, multiprocessing systems, SWAP, shared last-level caches, hardware support, server-class environments, chip multiprocessor, CMP, scalable cache management, fine-grained cache management, set and way partitioning, fine-grained cache partitions, manycore, cache partitioning, resource management, QoS, throughput, multicore, 
* **Abstract**: Performance isolation is an important goal in server-class environments. Partitioning the last-level cache of a chip multiprocessor (CMP) across co-running applications has proven useful in this regard. Two popular approaches are (a) hardware support for way partitioning, or (b) operating system support for set partitioning through page coloring. Unfortunately, neither approach by itself is scalable beyond a handful of cores without incurring in significant performance overheads. We propose SWAP, a scalable and fine-grained cache management technique that seamlessly combines set and way partitioning. By cooperatively managing cache ways and sets, SWAP (“Set and WAy Partitioning”) can successfully provide hundreds of fine-grained cache partitions for the manycore era. SWAP requires no additional hardware beyond way partitioning. In fact, SWAP can be readily implemented in existing commercial servers whose processors do support hardware way partitioning. In this paper, we prototype SWAP on a 48-core Cavium ThunderX platform running Linux, and we show average speedups over no cache partitioning that are twice as large as those attained with ThunderX's hardware way partitioning alone.

---
### A Split Cache Hierarchy for Enabling Data-Oriented Optimizations.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.25
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920820
* **Key Words**: Metadata, Coherence, Optimization, Encoding, Arrays, Multicore processing, Couplings, cache storage, meta data, split cache hierarchy, data-oriented optimizations, cache line granularity, data coherent, data optimizations, splitting, metadata hierarchy, smart data placement, dynamic coherence, direct-to-master, D2M, cache searching, level-by-level data movement, associative cache address tags, LLC slices, cache hierarchy EDP, mobile processor, server processor, network traffic, L1 miss latency, cache pressure, 
* **Abstract**: Today's caches tightly couple data with metadata (Address Tags) at the cache line granularity. The co-location of data and its identifying metadata means that they require multiple approaches to locate data (associative way searches and level-by-level searches), evict data (coherent writebacks buffers and associative level-by-level searches) and keep data coherent (directory indirections and associative level-by-level searches). This results in complex implementations with many corner cases, increased latency and energy, and limited flexibility for data optimizations. We propose splitting the metadata and data into two separate structures: a metadata hierarchy and a data hierarchy. The metadata hierarchy tracks the location of the data in the data hierarchy. This allows us to easily apply many different optimizations to the data hierarchy, including smart data placement, dynamic coherence, and direct accesses. The new split cache hierarchy, Direct-to-Master (D2M), provides a unified mechanism for cache searching, eviction, and coherence, that eliminates level-by-level data movement and searches, associative cache address tags comparisons and about 90% of the indirections through a central directory. Optimizations such as moving LLC slices to the near-side of the network and private/shared data classification can easily be built on top off D2M to further improve its efficiency. This approach delivers a 54% improvement in cache hierarchy EDP vs. a mobile processor and 40% vs. a server processor, reduces network traffic by an average of 70%, reduces the L1 miss latency by 30% and is especially effective for workloads with high cache pressure.

---
### Fast and Accurate Exploration of Multi-level Caches Using Hierarchical Reuse Distance.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.11
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920821
* **Key Words**: Computational modeling, Benchmark testing, Estimation, Analytical models, Measurement, Organizations, Computers, cache storage, estimation theory, optimisation, multilevel cache exploration, hierarchical reuse distance, HRD, memory hierarchy design space exploration, locality models, analytical cache miss estimation, program characterization, synthetic trace generation, single locality granularity, reuse distance generalization, profiling, synthesis complexity, miss rate error reduction, reuse distance, cache, simulation, statistical, 
* **Abstract**: Exploring the design space of the memory hierarchy requires the use of effective methodologies, tools, and models to evaluate different parameter values. Reuse distance is of one of the locality models used in the design exploration and permits analytical cache miss estimation, program characterization, and synthetic trace generation. Unfortunately, the reuse distance is limited to a single locality granularity. Hence, it is not a suitable model for caches with hybrid line sizes, such as sectored caches, an increasingly popular choice for large caches. In this work, we introduce a generalization to the reuse distance, which is able to capture locality seen at multiple granularities. We refer to it as Hierarchical Reuse Distance (HRD). The proposed model has same profiling and synthesis complexity as the traditional reuse distance, and our results show that HRD reduces the average miss rate error on sectored caches by more than three times. In addition, it has superior characteristics in exploring multi-level caches with conventional single line size. For instance, our method increases the accuracy on L2 and L3 by a factor of 4 and converges three orders of magnitude faster.

---
### Enabling Effective Module-Oblivious Power Gating for Embedded Processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.48
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920822
* **Key Words**: Logic gates, Program processors, Hardware, Monitoring, Correlation, Microprocessors, embedded systems, power aware computing, module-oblivious power gating, embedded processors, power requirements, energy requirements, multiple RTL modules, software-based power management, Power-gating, Embedded, Low-power, Leakage, 
* **Abstract**: The increasingly-stringent power and energy requirements of emerging embedded applications have led to a strong recent interest in aggressive power gating techniques. Conventional techniques for aggressive power gating perform module-based power gating in processors, where power domains correspond to RTL modules. We observe that there can be significant power benefits from module-oblivious power gating, where power domains can include an arbitrary set of gates, possibly from multiple RTL modules. However, since it is not possible to infer the activity of module-oblivious power domains from software alone, conventional software-based power management techniques cannot be applied for module-oblivious power gating in processors. Also, since module-oblivious domains are not encapsulated with a well-defined port list and functionality like RTL modules, hardware-based management of module-oblivious domains is prohibitively expensive. In this paper, we present a technique for low-cost management of moduleoblivious power domains in embedded processors. The technique involves symbolic simulation-based co-analysis of a processor's hardware design and a software binary to derive profitable and safe power gating decisions for a given set of module-oblivious domains when the software binary is run on the processor. Our technique is automated, does not require programmer intervention, and incurs low management overhead. We demonstrate that module-oblivious power gating based on our technique reduces leakage energy by 2× with respect to state-of-the-art aggressive module-based power gating for a common embedded processor.

---
### Application-Specific Performance-Aware Energy Optimization on Android Mobile Devices.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.32
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920823
* **Key Words**: Performance evaluation, Frequency control, Bandwidth, Mobile handsets, Androids, Humanoid robots, Hardware, Android (operating system), microprocessor chips, minimisation, performance evaluation, power aware computing, smart phones, application-specific performance-aware energy optimization, Android mobile devices, power management, OS modules, DVFS, energy consumption minimization, user-specified performance target, online controlling, offline profiling data, CPU frequency, memory bandwidth, Nexus 6 smartphone, Android, energy optimization, control theory, DVFS, SoC, 
* **Abstract**: Energy management is a key issue for mobile devices. On current Android devices, power management relies heavily on OS modules known as governors. These modules are created for various hardware components, including the CPU, to support DVFS. They implement algorithms that attempt to balance performance and power consumption. In this paper we make the observation that the existing governors are (1) general-purpose by nature (2) focused on power reduction and (3) are not energy-optimal for many applications. We thus establish the need for an application-specific approach that could overcome these drawbacks and provide higher energy efficiency for suitable applications. We also show that existing methods manage power and performance in an independent and isolated fashion and that co-ordinated control of multiple components can save more energy. In addition, we note that on mobile devices, energy savings cannot be achieved at the expense of performance. Consequently, we propose a solution that minimizes energy consumption of specific applications while maintaining a user-specified performance target. Our solution consists of two stages: (1) offline profiling and (2) online controlling. Utilizing the offline profiling data of the target application, our control theory based online controller dynamically selects the optimal system configuration (in this paper, combination of CPU frequency and memory bandwidth) for the application, while it is running. Our energy management solution is tested on a Nexus 6 smartphone with 6 real-world applications. We achieve 4 - 31% better energy than default governors with a worst case performance loss of <; 1%.

---
### Fast Decentralized Power Capping for Server Clusters.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.49
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920824
* **Key Words**: Computer architecture, circuit breakers, optimisation, power aware computing, power consumption, workstation clusters, decentralized power capping, server clusters, power consumption, power distribution devices, equipment protection, demand-response programs, hierarchical management systems, DPC, maximum throughput optimization formulation, workloads priorities, circuit breakers, cluster performance, decentralized power management, real computing cluster, network bandwidth, 
* **Abstract**: Power capping is a mechanism to ensure that the power consumption of clusters does not exceed the provisioned resources. A fast power capping method allows for a safe over-subscription of the rated power distribution devices, provides equipment protection, and enables large clusters to participate in demand-response programs. However, current methods have a slow response time with a large actuation latency when applied across a large number of servers as they rely on hierarchical management systems. We propose a fast decentralized power capping (DPC) technique that reduces the actuation latency by localizing power management at each server. The DPC method is based on a maximum throughput optimization formulation that takes into account the workloads priorities as well as the capacity of circuit breakers. Therefore, DPC significantly improves the cluster performance compared to alternative heuristics. We implement the proposed decentralized power management scheme on a real computing cluster. Compared to state-of-the-art hierarchical methods, DPC reduces the actuation latency by 72% up to 86% depending on the cluster size. In addition, DPC improves the system throughput performance by 16%, while using only 0.02% of the available network bandwidth. We describe how to minimize the overhead of each local DPC agent to a negligible amount. We also quantify the traffic and fault resilience of our decentralized power capping approach.

---
### Random Folded Clos Topologies for Datacenter Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.26
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920825
* **Key Words**: Topology, Network topology, Routing, Bandwidth, System recovery, Graph theory, Servers, computer centres, computer networks, fault tolerant computing, telecommunication network routing, telecommunication network topology, trees (mathematics), random folded clos topologies, datacenter networks, fault tolerance, random regular networks, Jellyfish, unstructured design, RFC, Clos networks, deadlock-free equal-cost multipath routing, orthogonal fat-trees, random regular graphs, synthetic traffics, design requirements, 
* **Abstract**: In datacenter networks, big scale, high performance and faulttolerance, low-cost, and graceful expandability are pursued features. Recently, random regular networks, as the Jellyfish, have been proposed for satisfying these stringent requirements. However, their completely unstructured design entails several drawbacks. As a related alternative, in this paper we propose Random Folded Clos (RFC) networks. They constitute a compromise between total randomness and maintaining some topological structure. As it will be shown, RFCs preserve important properties of Clos networks that provide a straightforward deadlock-free equal-cost multi-path routing and enough randomness to gracefully expanding. These networks are minutely compared, in topological and cost terms, against fat-trees, orthogonal fat-trees and random regular graphs. Also, experiments are carried out to simulate their performance under synthetic traffics that emulate common loads in datacenters. It is shown that RFCs constitute an interesting alternative to currently deployed networks since they appropriately balance all the important design requirements. Moreover, they do that at much lower cost than the fat-tree, their natural competitor. Being able up to connect the same number of compute nodes, saving up to 95% of the cost, and giving similar performance.

---
### Tiny Directory: Efficient Shared Memory in Many-Core Systems with Ultra-Low-Overhead Coherence Tracking.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.24
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920826
* **Key Words**: Coherence, Proposals, Protocols, Oceans, Organizations, Switches, Instruction sets, coherence, shared memory systems, many-core systems, ultra-low-overhead coherence tracking, shared memory abstraction, multicore chip-multiprocessors, many-core chip-multiprocessors, coarse grain, operating system, OS, multigrain coherence, broadcast-based recovery, broadcast-free block-grain coherence, block lastlevel cache, LLC data, tiny sparse directory, LLC space, multithreaded applications, sparse directory, cache coherence, shared memory, 
* **Abstract**: The sparse directory has emerged as a critical component for supporting the shared memory abstraction in multiand many-core chip-multiprocessors. Recent research efforts have explored ways to reduce the number of entries in the sparse directory. These include tracking coherence of private regions at a coarse grain, not tracking blocks that belong to pages identified as private by the operating system (OS), and not tracking a subset of blocks that are speculated to be private by the hardware. These techniques require support for multi-grain coherence, assistance of OS, or broadcast-based recovery on sharing an untracked block that is wrongly speculated as private. In this paper, we design a robust minimally-sized sparse directory that can offer adequate performance while enjoying the simplicity, scalability, and OS-independence of traditional broadcast-free block-grain coherence. We begin our exploration with a naïve design that does not have a sparse directory and the location/sharers of a block are tracked by borrowing a portion of the block's lastlevel cache (LLC) data way. Such a design, however, lengthens the critical path from two transactions to three transactions (two hops to three hops) for the blocks that experience frequent shared read accesses. We address this problem by architecting a tiny sparse directory that dynamically identifies and tracks a selected subset of the blocks that experience a large volume of shared accesses. We augment the tiny directory proposal with an option of selectively spilling into the LLC space for tracking the coherence of the critical shared blocks that the tiny directory fails to accommodate. Detailed simulation-based study on a 128-core system with a large set of multi-threaded applications spanning scientific, general-purpose, and commercial computing shows that our coherence tracking proposal operating with1/32 × to 1/256 × sparse directories offers performance within a percentage of a traditional 2× sparse directory.

---
### Partial Row Activation for Low-Power DRAM System.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.35
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920827
* **Key Words**: Random access memory, Power demand, Bandwidth, Memory management, Decoding, Prefetching, Organizations, DRAM chips, low-power electronics, power aware computing, low-power DRAM system, memory traffic, DRAM bandwidth, I/O power consumption reduction, power efficiency improvement, energy efficiency improvement, partial-row activation scheme, memory writes, row overfetching problem mitigation, row activation power consumption reduction, cache line data, row activation granularity minimization, total DRAM power consumption reduction, 
* **Abstract**: Owing to increasing demand of faster and larger DRAM system, the DRAM system accounts for a large portion of the total power consumption of computing systems. As memory traffic and DRAM bandwidth grow, the row activation and I/O power consumptions are becoming major contributors to total DRAM power consumption. Thus, reducing row activation and I/O power consumptions has big potential for improving the power and energy efficiency of the computing systems. To this end, we propose a partial row activation scheme for memory writes, in which DRAM is rearchitected to mitigate row overfetching problem of modern DRAMs and to reduce row activation power consumption. In addition, accompanying I/O power consumption in memory writes is also reduced by transferring only a part of cache line data that must be written to partially opened rows. In our proposed scheme, partial rows ranging from a one-eighth row to a full row can be activated to minimize row activation granularity for memory writes and the full bandwidth of the conventional DRAM can be maintained for memory reads. Our partial row activation scheme is shown to reduce total DRAM power consumption by up to 32% and 23% on average, which outperforms previously proposed schemes in DRAM power saving with almost no performance loss.

---
### Understanding and Optimizing Power Consumption in Memory Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.60
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920828
* **Key Words**: Memory management, Bandwidth, Network topology, Topology, Random access memory, Pins, Power demand, Big Data, computer centres, input-output programs, parallel processing, power consumption, storage management, power consumption, memory networks, digital data, HPC systems, big data, memory technologies, point-to-point links, data centers, I/O links, Memory Power Management, Hybrid Memory Cube, Point-to-point Network, High-speed Memory I/O, 
* **Abstract**: As the amount of digital data the world generates explodes, data centers and HPC systems that process this big data will require high bandwidth and high capacity main memory. Unfortunately, conventional memory technologies either provide high memory capacity (e.g., DDRx memory) or high bandwidth (GDDRx memory), but not both. Memory networks, which provide both high bandwidth and high capacity memory by connecting memory modules together via a network of pointto-point links, are promising future memory candidates for data centers and HPCs. In this paper, we perform the first exploration to understand the power characteristics of memory networks. We find idle I/O links to be the biggest power contributor in memory networks. Subsequently, we study idle I/O power in more detail. We evaluate well-known circuitlevel I/O power control mechanisms such as rapid on off, variable link width, and DVFS. We also adapt prior works on memory power management to memory networks. The adapted schemes together reduce I/O power by 32% and 21%, on average, for big and small networks, respectively. We also explore novel power management schemes specifically targeting memory networks, which yield another 29% and 17% average I/O power reduction for big and small networks, respectively.

---
### SoftMC: A Flexible and Practical Open-Source Infrastructure for Enabling Experimental DRAM Studies.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.62
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920829
* **Key Words**: DRAM chips, Reliability, Testing, Timing, Open source software, data integrity, DRAM chips, field programmable gate arrays, integrated circuit design, public domain software, SoftMC, open-source infrastructure, data integrity, data latency, high-performance DRAM-based main memory, DRAM chips, publicly-available DRAM testing infrastructure, soft memory controller, FPGA-based testing platform, DDR interface, double data rate, DRAM cells, publicly-available experimental memory testing infrastructures, memory system design, DRAM, FPGA, Memory Characterization, 
* **Abstract**: DRAM is the primary technology used for main memory in modern systems. Unfortunately, as DRAM scales down to smaller technology nodes, it faces key challenges in both data integrity and latency, which strongly affects overall system reliability and performance. To develop reliable and high-performance DRAM-based main memory in future systems, it is critical to characterize, understand, and analyze various aspects (e.g., reliability, latency) of existing DRAM chips. To enable this, there is a strong need for a publicly-available DRAM testing infrastructure that can flexibly and efficiently test DRAM chips in a manner accessible to both software and hardware developers. This paper develops the first such infrastructure, SoftMC (Soft Memory Controller), an FPGA-based testing platform that can control and test memory modules designed for the commonly used DDR (Double Data Rate) interface. SoftMC has two key properties: (i) it provides flexibility to thoroughly control memory behavior or to implement a wide range of mechanisms using DDR commands; and (ii) it is easy to use as it provides a simple and intuitive high-level programming interface for users, completely hiding the low-level details of the FPGA. We demonstrate the capability, flexibility, and programming ease of SoftMC with two example use cases. First, we implement a test that characterizes the retention time of DRAM cells. Experimental results we obtain using SoftMC are consistent with the findings of prior studies on retention time in modern DRAM, which serves as a validation of our infrastructure. Second, we validate two recently-proposed mechanisms, which rely on accessing recently-refreshed or recently-accessed DRAM cells faster than other DRAM cells. Using our infrastructure, we show that the expected latency reduction effect of these mechanisms is not observable in existing DRAM chips, which demonstrates the usefulness of SoftMC in testing new ideas on existing memory modules. We discuss several other u...

---
### Static Bubble: A Framework for Deadlock-Free Irregular On-chip Topologies.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.44
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920830
* **Key Words**: System recovery, Topology, Routing, Network topology, Runtime, System-on-chip, Microarchitecture, network routing, network topology, network-on-chip, trees (mathematics), static bubble, deadlock-free irregular on-chip topologies, SoC, system-on-chip, routing deadlocks, tree construction, NoC resiliency, network-on-chip, power gating, Networks on chip, Deadlocks, NoC Power Gating, NoC Fault tolerance, Irregular topologies, 
* **Abstract**: Future SoCs are expected to have irregular onchip topologies, either at design time due to heterogeneity in the size of core/accelerator tiles, or at runtime due to link/node failures or power-gating of network elements such as routers/router datapaths. A key challenge with irregular topologies is that of routing deadlocks (cyclic dependence between buffers), since conventional XY or turn-model based approaches are no longer applicable. Most prior works in heterogeneous SoC design, resiliency, and power-gating, have addressed the deadlock problem by constructing spanning trees over the physical topology; messages are routed via the root removing cyclic dependencies. However, this comes at a cost of tree construction at runtime, and increased latency and energy for certain flows as they are forced to use non-minimal routes. In this work, we sweep the design space of possible topologies as the number of disconnected components (links/routers) increase, and demonstrate that while most of the resulting topologies are deadlock prone (i.e., have cycles), the injection rates at which they deadlock are often much higher than the injection rates of real applications, making the current solutions highly conservative. We propose a novel framework for deadlock-freedom called Static Bubble, that can be applied at design time to the underlying mesh topology, and guarantees deadlock-freedom for any runtime topology derived from this mesh due to powergating or failure of router/link. We present an algorithm to augment a subset of routers in any n×m mesh (21 routers in a 64-core mesh) with an additional buffer called static bubble, such that any dependence chain has at least one static bubble. We also present the microarchitecture of a low-cost (less than 1% overhead) FSM at every router to activate one static bubble for deadlock recovery. Static Bubble enhances existing solutions for NoC resiliency and power-gating by providing up to 30% less network latency, 4x more throughput and...

---
### Designing Low-Power, Low-Latency Networks-on-Chip by Optimally Combining Electrical and Optical Links.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.23
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920831
* **Key Words**: Optical fiber communication, Power lasers, Delays, Bandwidth, Clocks, Optical resonators, Laser modes, network routing, network-on-chip, optical links, low-power-low-latency network-on-chip design, electrical links, optical on-chip communication, power efficiency improvement, optical loss, laser power, Lego, hybrid mesh-based NoC, local traffic, low-bandwidth optical links, serialization delay, router microarchitecture, synthetic traffic, SPLASH-2/PARSEC workloads, 
* **Abstract**: Optical on-chip communication is considered a promising candidate to overcome latency and energy bottlenecks of electrical interconnects. Although recently proposed hybrid Networks-on-chip (NoCs), which implement both electrical and optical links, improve power efficiency, they often fail to combine these two interconnect technologies efficiently and suffer from considerable laser power overheads caused by high-bandwidth optical links. We argue that these overheads can be avoided by inserting a higher quantity of low-bandwidth optical links in a topology, as this yields lower optical loss and in turn laser power. Moreover, when optimally combined with electrical links for short distances, this can be done without trading off latency. We present the effectiveness of this concept with Lego, our hybrid, mesh-based NoC that provides high power efficiency by utilizing electrical links for local traffic, and low-bandwidth optical links for long distances. Electrical links are placed systematically to outweigh the serialization delay introduced by the optical links, simplify router microarchitecture, and allow to save optical resources. Our routing algorithm always chooses the link that offers the lowest latency and energy. Compared to state-of-the-art proposals, Lego increases throughput-per-watt by at least 40%, and lowers latency by 35% on average for synthetic traffic. On SPLASH-2/PARSEC workloads, Lego improves power efficiency by at least 37% (up to 3.5×).

---
### Near-Ideal Networks-on-Chip for Servers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.16
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920832
* **Key Words**: Servers, Program processors, Spread spectrum communication, Delays, Resource management, Quality of service, Clocks, cache storage, file servers, multiprocessing systems, network-on-chip, resource allocation, near-ideal networks-on-chip, server workloads, manycore processors, massive request-level parallelism, last-level cache, instruction footprints, LLC, low-latency network-on-chip, server processors, lean cores, NOC-induced delays, per-hop resource allocation, proactive resource allocation, PRA, single-cycle multihop networks, 64-core processor, single-cycle multihop mesh NOC, Latency, network-on-chip, resource allocation, server, 
* **Abstract**: Server workloads benefit from execution on manycore processors due to their massive request-level parallelism. A key characteristic of server workloads is the large instruction footprints. While a shared last-level cache (LLC) captures the footprints, it necessitates a low-latency network-on-chip (NOC) to minimize the core stall time on accesses serviced by the LLC. As strict quality-of-service requirements preclude the use of lean cores in server processors, we observe that even state-of-the-art single-cycle multi-hop NOCs are far from ideal because they impose significant NOC-induced delays on the LLC access latency, and diminish performance. Most of the NOC delay is due to per-hop resource allocation. In this paper, we take advantage of proactive resource allocation (PRA) to eliminate per-hop resource allocation time in single-cycle multi-hop networks to reach a near-ideal network for servers. PRA is undertaken during (1) the time interval in which it is known that LLC has the requested data, but the data is not yet ready, and (2) the time interval in which a packet is stalled in a router because the required resources are dedicated to another packet. Through detailed evaluation targeting a 64-core processor and a set of server workloads, we show that our proposal improves system performance by 12% over the state-of-the-art single-cycle multi-hop mesh NOC.

---
### Design and Evaluation of AWGR-Based Photonic NoC Architectures for 2.5D Integrated High Performance Computing Systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.17
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920833
* **Key Words**: Bandwidth, Photonics, Arrayed waveguide gratings, Ports (Computers), Multiprocessor interconnection, Topology, arrayed waveguide gratings, integrated optics, multiprocessing systems, multiprocessor interconnection networks, network-on-chip, parallel machines, AWGR-based photonic NoC architecture design, 2.5D integrated high performance computing systems, supercomputers, 3D/vertical die-stacking, 2.5D/horizontal die-stacking, interconnection network, electronic interconnects, arrayed waveguide grating router, photonic interconnects, silicon interposer, photonic network-on-chip, interposer-based electrical NoC, PARSEC benchmark suite, 64-core system, Gem5 simulation framework, 
* **Abstract**: In future performance improvement of the basic building block of supercomputers has to come through increased integration enabled by 3D (vertical) and 2.5D (horizontal) die-stacking. But to take advantage of this integration we need an interconnection network between the memory and compute die that not only can provide an order of magnitude higher bandwidth but also consume an order of magnitude less power than today's state of the art electronic interconnects. We show how Arrayed Waveguide Grating Router-based photonic interconnects implemented on the silicon interposer can be used to realize a 16 × 16 photonic Network-on-Chip (NoC) with a bisection bandwidth of 16 Tb/s. We propose a baseline network, which consumes 2.57 pJ/bit assuming 100% utilization. We show that the power is dominated by the electro-optical interface of the transmitter, which can be reduced by a more aggressive design that improves the energy per bit to 0.454 pJ/bit at 100% utilization. Compared to recently proposed interposer-based electrical NoC's we show an average performance improvement of 25% on the PARSEC benchmark suite on a 64-core system using the Gem5 simulation framework.

---
### Secure Dynamic Memory Scheduling Against Timing Channel Attacks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.27
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920834
* **Key Words**: Timing, Security, Schedules, Interference, Dynamic scheduling, Random access memory, Cloud computing, processor scheduling, shared memory systems, secure dynamic memory scheduling, timing channel attacks, secure memory controller, quantitative security guarantee, SecMC-NI, memory placements, quantitative information theoretic bound, information leakage, 
* **Abstract**: This paper presents SecMC, a secure memory controller that provides efficient memory scheduling with a strong quantitative security guarantee against timing channel attacks. The first variant, named SecMC-NI, eliminates timing channels while allowing a tight memory schedule by interleaving memory requests that access different banks or ranks. Experimental results show that SecMC-NI significantly (45% on average) improves the performance of the best known scheme that does not rely on restricting memory placements. To further improve the performance, the paper proposes SecMC-Bound, which enables trading-off security for performance with a quantitative information theoretic bound on information leakage. The experimental results show that allowing small information leakage can yield significant performance improvements.

---
### Cold Boot Attacks are Still Hot: Security Analysis of Memory Scramblers in Modern Processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.10
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920835
* **Key Words**: Random access memory, Encryption, Program processors, Registers, Capacitors, Data mining, cryptography, DRAM chips, cold boot attacks, security analysis, modern processors, unencrypted DRAM interfaces, sensitive data, locked devices, data scrambling, signal integrity, power supply noise, DRAM contents, scrambled DDR3 systems, DDR4 memory scramblers, 6th generation Intel Core processors, Skylake processors, DDR4 scramblers, AES keys, disk encryption keys, performance-sensitive applications, memory encryption, integrity checking, replay attack protections, Intel SGX, stream ciphers, ChaCha8, overlap keystream generation, DRAM row buffer access latency, zero exposed latency, nonvolatile DIMM, DDR4 buses, cold boot attack, memory, DRAM, security, cryptography, 
* **Abstract**: Previous work has demonstrated that systems with unencrypted DRAM interfaces are susceptible to cold boot attacks - where the DRAM in a system is frozen to give it sufficient retention time and is then re-read after reboot, or is transferred to an attacker's machine for extracting sensitive data. This method has been shown to be an effective attack vector for extracting disk encryption keys out of locked devices. However, most modern systems incorporate some form of data scrambling into their DRAM interfaces making cold boot attacks challenging. While first added as a measure to improve signal integrity and reduce power supply noise, these scramblers today serve the added purpose of obscuring the DRAM contents. It has previously been shown that scrambled DDR3 systems do not provide meaningful protection against cold boot attacks. In this paper, we investigate the enhancements that have been introduced in DDR4 memory scramblers in the 6 th generation Intel Core (Skylake) processors. We then present an attack that demonstrates these enhanced DDR4 scramblers still do not provide sufficient protection against cold boot attacks. We detail a proof-of-concept attack that extracts memory resident AES keys, including disk encryption keys. The limitations of memory scramblers we point out in this paper motivate the need for strong yet low-overhead fullmemory encryption schemes. Existing schemes such as Intel's SGX can effectively prevent such attacks, but have overheads that may not be acceptable for performance-sensitive applications. However, it is possible to deploy a memory encryption scheme that has zero performance overhead by forgoing integrity checking and replay attack protections afforded by Intel SGX. To that end, we present analyses that confirm modern stream ciphers such as ChaCha8 are sufficiently fast that it is now possible to completely overlap keystream generation with DRAM row buffer access latency, thereby enabling the creation of strongly encrypted DRAMs...

---
### Cooperative Path-ORAM for Effective Memory Bandwidth Sharing in Server Settings.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.9
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920836
* **Key Words**: Random access memory, Servers, Bandwidth, Degradation, Memory management, Cryptography, protocols, random-access storage, cooperative path-ORAM, effective memory bandwidth sharing, oblivious RAM, ORAM protocol, memory access sequences, CP-ORAM, P-Path, R-Path, W-Path, 
* **Abstract**: Path ORAM (Oblivious RAM) is a recently proposed ORAM protocol for preventing information leakage from memory access sequences. It receives wide adoption due to its simplicity, practical efficiency and asymptotic efficiency. However, Path ORAM has extremely large memory bandwidth demand, leading to severe memory competition in server settings, e.g., a server may service one application that uses Path ORAM and one or multiple applications that do not. While Path ORAM synchronously and intensively uses all memory channels, the non-secure applications often exhibit low access intensity and large channel level imbalance. Traditional memory scheduling schemes lead to wasted memory bandwidth to the system and large performance degradation to both types of applications. In this paper, we propose CP-ORAM, a Cooperative Path ORAM design, to effectively schedule the memory requests from both types of applications. CP-ORAM consists of three schemes: P-Path,R-Path, and W-Path. P-Path assigns and enforces scheduling priority for effective memory bandwidth sharing. R-Path maximizes bandwidth utilization by proactively scheduling read operations from the next Path ORAM access. W-Path mitigates contention on busy memory channels with write redirection. We evaluate CP-ORAM and compare it to the state-of-the-art. Our results show that CP-ORAM helps to achieve 20% performance improvement on average over the baseline Path ORAM for the secure application in a four-channel server setting.

---
### Camouflage: Memory Traffic Shaping to Mitigate Timing Attacks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.36
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920837
* **Key Words**: Timing, Security, Hardware, Memory management, Monitoring, Instruction sets, Shape, cloud computing, cryptography, storage management, timing side channels, user security, user privacy, multuser systems, memory timing channels, hardware solution, timing channel attack mitigation, memory system, memory response rate analysis, memory traffic shaping mechanisms, bi-directional traffic, ORAM, information leakage protection, Camouflage security, untrusted parties, cloud providers, co-scheduled clients, hardware, security, memory system, 
* **Abstract**: Information leaks based on timing side channels in computing devices have serious consequences for user security and privacy. In particular, malicious applications in multi-user systems such as data centers and cloud-computing environments can exploit memory timing as a side channel to infer a victim's program access patterns/phases. Memory timing channels can also be exploited for covert communications by an adversary. We propose Camouflage, a hardware solution to mitigate timing channel attacks not only in the memory system, but also along the path to and from the memory system (e.g. NoC, memory scheduler queues). Camouflage introduces the novel idea of shaping memory requests' and responses' interarrival time into a pre-determined distribution for security purposes, even creating additional fake traffic if needed. This limits untrusted parties (either cloud providers or coscheduled clients) from inferring information from another security domain by probing the bus to and from memory, or analyzing memory response rate. We design three different memory traffic shaping mechanisms for different security scenarios by having Camouflage work on requests, responses, and bi-directional (both) traffic. Camouflage is complementary to ORAMs and can be optionally used in conjunction with ORAMs to protect information leaks via both memory access timing and memory access patterns. Camouflage offers a tunable trade-off between system security and system performance. We evaluate Camouflage's security and performance both theoretically and via simulations, and find that Camouflage outperforms state-of-the-art solutions in performance by up to 50%.

---
### SILC-FM: Subblocked InterLeaved Cache-Like Flat Memory Organization.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.20
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920838
* **Key Words**: Bandwidth, Random access memory, Frequency modulation, Hardware, Memory management, Metadata, System-on-chip, cache storage, DRAM chips, storage management, SILC-FM, subblocked interleaved cache-like flat memory organization, DRAM technology, OS-transparent management, data migration, flat address space, CAMEO, PoM, page-level operation, static placement scheme, hardware swapping operations, die-stacked DRMA, subblocked, flat memory, 
* **Abstract**: With current DRAM technology reaching its limit, emerging heterogeneous memory systems have become attractive to continue scaling memory performance. This paper argues for using a small, fast memory closer to the processor as part of a flat address space where the memory system is composed of two or more memory types. OS-transparent management of such memory has been proposed in prior works such as CAMEO and Part of Memory (PoM). Data migration is typically handled either at coarse granularity with high bandwidth overheads (as in PoM) or at fine granularity with low hit rate (as in CAMEO). Prior work uses restricted address mapping from only congruence groups in order to simplify the mapping. At any time, only one page (block) from a congruence group is resident in the fast memory. In this paper, we present a flat address space organization called SILC-FM that uses large granularity but allows subblocks from two pages to coexist in an interleaved fashion in fast memory. Data movement is done at subblocked granularity, avoiding fetching of useless subblocks and consuming less bandwidth compared to migrating the entire large block. SILC-FM can achieve more spatial locality hits than CAMEO and PoM due to page-level operation and interleaving blocks respectively. The interleaved subblock placement improves performance by 55% on average over a static placement scheme without data migration. We also selectively lock hot blocks to prevent them from being involved in hardware swapping operations. Additional features such as locking, associativity and bandwidth balancing improve performance by 11%, 8%, and 8% respectively, resulting in a total of 82% performance improvement over a no migration static placement scheme. Compared to the best state-of-the-art scheme, SILC-FM gets performance improvement of 36% with 13% energy savings.

---
### ATOM: Atomic Durability in Non-volatile Memory through Hardware Logging.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.50
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920839
* **Key Words**: Nonvolatile memory, Hardware, Software, Writing, Data structures, Data models, Computer crashes, memory architecture, random-access storage, ATOM, atomic durability, nonvolatile memory, NVM, hardware logging, fast byte-addressable alternative, persistent data storage, software logging, streaming stores, clflush, mfence, redo log, data movement operation, hardware log manager, performance optimization, microbenchmarks, TPC-C, non-volatile memory, atomicity, durability, 
* **Abstract**: Non-volatile memory (NVM) is emerging as a fast byte-addressable alternative for storing persistent data. Ensuring atomic durability in NVM requires logging. Existing techniques have proposed software logging either by using streaming stores for an undo log; or, by relying on the combination of clflush and mfence for a redo log. These techniques are suboptimal because they waste precious execution cycles to implement logging, which is fundamentally a data movement operation. We propose ATOM, a hardware log manager based on undo logging that performs the logging operation out of the critical path. We present the design principles behind ATOM and two techniques to optimize its performance. Our results show that ATOM achieves an improvement of 27% to 33% for micro-benchmarks and 60% for TPC-C over a baseline undo log design.

---
### KAML: A Flexible, High-Performance Key-Value SSD.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.15
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920840
* **Key Words**: Atomic layer deposition, Databases, Proposals, Protocols, Nonvolatile memory, Random access memory, Flash memories, data mining, DRAM chips, input-output programs, NoSQL databases, transaction processing, KAML, high-performance key-value SSD, solid state drives, block I/O interface, multilog architecture, fine-grained locking, DRAM, online transaction processing, OLTP, NoSQL key-value store applications, 
* **Abstract**: Modern solid state drives (SSDs) unnecessarily confine host programs to the conventional block I/O interface, leading to suboptimal performance and resource under-utilization. Recent attempts to replace or extend this interface with a key-value-oriented interface and/or built-in support for transactions offer some improvements, but the details of their implementations make them a poor match for many applications. This paper presents the key-addressable, multi-log SSD (KAML), an SSD with a key-value interface that uses a novel multi-log architecture and stores data as variable-sized records rather than fixed-sized sectors. Exposing a key-value interface allows applications to remove a layer of indirection between application-level keys (e.g., database record IDs or file inode numbers) and data stored in the SSD. KAML also provides native transaction support tuned to support fine-grained locking, achieving improved performance compared to previous designs that require page-level locking. Finally, KAML includes a caching layer analogous to a conventional page cache that leverages host DRAM to improve performance and provides additional transactional features. We have implemented a prototype of KAML on a commercial SSD prototyping platform, and our results show that compared with existing key-value stores, KAML improves the performance of online transaction processing (OLTP) workloads by 1.1x - 4.0x, and NoSQL key-value store applications by 1.1x - 3.0x.

---
### Balancing Performance and Lifetime of MLC PCM by Using a Region Retention Monitor.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.45
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920841
* **Key Words**: Phase change materials, Resistance, Microprocessors, Memory management, Monitoring, Phase change memory, iterative methods, phase change memories, balancing performance, MLC PCM, region retention monitor, multilevel cell phase change memory, multiple digital bits, resistance drift problem, refresh operations, write scheme, SET iterations, RRM, static short writes, Non-Volatile Memory, Phase Change Memory, Dynamic Trade-off, 
* **Abstract**: Multi Level Cell (MLC) Phase Change Memory (PCM) is an enhancement of PCM technology, which provides higher capacity by allowing multiple digital bits to be stored in a single PCM cell. However, the retention time of MLC PCM is limited by the resistance drift problem and refresh operations are required. Previous work shows that there exists a trade-off between write latency and retention-a write scheme with more SET iterations and smaller current provides a longer retention time but at the cost of a longer write latency. Otherwise, a write scheme with fewer SET iterations achieves high performance for writes but requires a greater number of refresh operations due to its significantly reduced retention time, and this hurts the lifetime of MLC PCM. In this paper, we show that only a small part of memory (i.e., hot memory regions) will be frequently accessed in a given period of time. Based on such an observation, we propose Region Retention Monitor (RRM), a novel structure that records and predicts the write frequency of memory regions. For every incoming memory write operation, RRM select a proper write latency for it. Our evaluations show that RRM helps the system improves the balance between system performance and memory lifetime. On the performance side, the system with RRM bridges 77.2% of the performance gap between systems with long writes and systems with short writes. On the lifetime side, a system with RRM achieves a lifetime of 6.4 years, while systems using only long writes and short writes achieve lifetimes of 10.6 and 0.3 years, respectively. Also, we can easily control the aggressiveness of RRM through an attribute called hot threshold. A more aggressively configured RRM can achieve the performance which is only 3.5% inferior than the system using static short writes, while still achieve a lifetime of 5.78 years.

---
### Reliability-Aware Scheduling on Heterogeneous Multicore Processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.12
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920842
* **Key Words**: Reliability, Multicore processing, Program processors, Measurement, Benchmark testing, Job shop scheduling, Error analysis, multiprocessing systems, power aware computing, processor scheduling, reliability-aware scheduling, heterogeneous multicore processors, high-performance cores, power-efficient cores, system reliability improvement, system-level reliability metric, multiprogram workloads, heterogeneous multicore processor, reliability, scheduling, 
* **Abstract**: Reliability to soft errors is an increasingly important issue as technology continues to shrink. In this paper, we show that applications exhibit different reliability characteristics on big, high-performance cores versus small, power-efficient cores, and that there is significant opportunity to improve system reliability through reliability-aware scheduling on heterogeneous multicore processors. We monitor the reliability characteristics of all running applications, and dynamically schedule applications to the different core types in a heterogeneous multicore to maximize system reliability. Reliability-aware scheduling improves reliability by 25.4% on average (and up to 60.2%) compared to performance-optimized scheduling on a heterogeneous multicore processor with two big cores and two small cores, while degrading performance by 6.3% only. We also introduce a novel system-level reliability metric for multiprogram workloads on (heterogeneous) multicores. We further show that our reliability-aware scheduler is robust across core count, number of big and small cores, and their frequency settings. The hardware cost in support of our reliability-aware scheduler is limited to 296 bytes per core.

---
### Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.13
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920843
* **Key Words**: Quality of service, Power demand, Energy consumption, Servers, Learning (artificial intelligence), Resource management, Throughput, cloud computing, computer centres, electricity, learning (artificial intelligence), multiprocessing systems, quality of service, Hipster, hybrid task manager, latency-critical cloud workloads, data centers, electricity consumption, quality of service, user expectations, power consumption, reinforcement learning, latency-critical workloads, multicores, dynamic voltage, frequency scaling, DVFS, data center utilization, QoS constraints, Web Search, energy consumption, 
* **Abstract**: In 2013, U.S. data centers accounted for 2.2% of the country's total electricity consumption, a figure that is projected to increase rapidly over the next decade. Many important workloads are interactive, and they demand strict levels of quality-of-service (QoS) to meet user expectations, making it challenging to reduce power consumption due to increasing performance demands. This paper introduces Hipster, a technique that combines heuristics and reinforcement learning to manage latency-critical workloads. Hipster's goal is to improve resource efficiency in data centers while respecting the QoS of the latency-critical workloads. Hipster achieves its goal by exploring heterogeneous multi-cores and dynamic voltage and frequency scaling (DVFS). To improve data center utilization and make best usage of the available resources, Hipster can dynamically assign remaining cores to batch workloads without violating the QoS constraints for the latency-critical workloads. We perform experiments using a 64-bit ARM big.LITTLE platform, and show that, compared to prior work, Hipster improves the QoS guarantee for Web-Search from 80% to 96%, and for Memcached from 92% to 99%, while reducing the energy consumption by up to 18%.

---
### Cooper: Task Colocation with Cooperative Games.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.22
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920844
* **Key Words**: Games, Hardware, Game theory, Throughput, Bandwidth, Servers, Stability analysis, computer centres, game theory, resource allocation, cooperative games, task colocation, data center utilization, resource contention, game-theoretic framework, user colocation preferences, Cooper colocations, user performance penalties, cooperative game theory, Datacenter Management, Task Colocation, Interference, Performance Prediction, Fairness, Game Theory, 
* **Abstract**: Task colocation improves datacenter utilization but introduces resource contention for shared hardware. In this setting, a particular challenge is balancing performance and fairness. We present Cooper, a game-theoretic framework for task colocation that provides fairness while preserving performance. Cooper predicts users' colocation preferences and finds stable matches between them. Its colocations satisfy preferences and encourage strategic users to participate in shared systems. Given Cooper's colocations, users' performance penalties are strongly correlated to their contributions to contention, which is fair according to cooperative game theory. Moreover, its colocations perform within 5% of prior heuristics.

---
### MemPod: A Clustered Architecture for Efficient and Scalable Migration in Flat Address Space Multi-level Memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.39
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920845
* **Key Words**: Radiation detectors, Memory management, Random access memory, Hardware, Organizations, Software, Monitoring, Big Data, data analysis, DRAM chips, memory architecture, MemPod, clustered architecture, flat address space multilevel memories, die-stacked DRAM, off-chip memories, hybrid memory systems, stacked memory, memory management mechanism, flat address space hybrid memories, big data analytics algorithm, multiprogrammed workloads, activity tracking approach, Memory architecture, Die-stacked memory, 
* **Abstract**: In the near future, die-stacked DRAM will be increasingly present in conjunction with off-chip memories in hybrid memory systems. Research on this subject revolves around using the stacked memory as a cache or as part of a flat address space. This paper proposes MemPod, a scalable and efficient memory management mechanism for flat address space hybrid memories. MemPod monitors memory activity and periodically migrates the most frequently accessed memory pages to the faster on-chip memory. MemPod's partitioned architectural organization allows for efficient scaling with memory system capabilities. Further, a big data analytics algorithm is adapted to develop an efficient, low-cost activity tracking technique. MemPod improves the average main memory access time of multi-programmed workloads, by up to 29% (9% on average) compared to the state of the art, and that will increase as the differential between memory speeds widens. MemPod's novel activity tracking approach leads to significant cost reduction (~12800x lower storage space requirements) and improved future prediction accuracy over prior work which maintains a separate counter per page.

---
### Exploring Hyperdimensional Associative Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.28
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920846
* **Key Words**: Computer architecture, biomimetics, brain, CMOS memory circuits, cognition, memory architecture, neural net architecture, pattern classification, random-access storage, search problems, vectors, hyperdimensional associative memory, brain-inspired hyperdimensional computing, HD computing, cognition tasks, hypervector computing, large pattern manipulation, associative search, classification, learned hypervectors, query hypervector, distance metric, i.i.d. components, memory-centric architecture, energy efficiency, scalable search operation, nearest Hamming distance, digital CMOS-based HAM, D-HAM, resistive HAM, R-HAM, nonvolatile resistive elements, analog HAM, A-HAM, language recognition, Associative memory, Hyperdimensional computing, Neuromorphic computing, Non-volatile memory, 
* **Abstract**: Brain-inspired hyperdimensional (HD) computing emulates cognition tasks by computing with hypervectors as an alternative to computing with numbers. At its very core, HD computing is about manipulating and comparing large patterns, stored in memory as hypervectors: the input symbols are mapped to a hypervector and an associative search is performed for reasoning and classification. For every classification event, an associative memory is in charge of finding the closest match between a set of learned hypervectors and a query hypervector by using a distance metric. Hypervectors with the i.i.d. components qualify a memory-centric architecture to tolerate massive number of errors, hence it eases cooperation of various methodological design approaches for boosting energy efficiency and scalability. This paper proposes architectural designs for hyperdimensional associative memory (HAM) to facilitate energy-efficient, fast, and scalable search operation using three widely-used design approaches. These HAM designs search for the nearest Hamming distance, and linearly scale with the number of dimensions in the hypervectors while exploring a large design space with orders of magnitude higher efficiency. First, we propose a digital CMOS-based HAM (D-HAM) that modularly scales to any dimension. Second, we propose a resistive HAM (R-HAM) that exploits timing discharge characteristic of nonvolatile resistive elements to approximately compute Hamming distances at a lower cost. Finally, we combine such resistive characteristic with a currentbased search method to design an analog HAM (A-HAM) that results in faster and denser alternative. Our experimental results show that R-HAM and A-HAM improve the energy-delay product by 9.6× and 1347× compared to D-HAM while maintaining a moderate accuracy of 94% in language recognition.

---
### GraphPIM: Enabling Instruction-Level PIM Offloading in Graph Computing Frameworks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.54
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920847
* **Key Words**: Computer architecture, Hardware, Random access memory, Metadata, Software, Complexity theory, Atomic measurements, memory architecture, storage management, GraphPIM, instruction-level PIM offloading, graph computing frameworks, data science, atomic operations, memory subsystem, hybrid memory cube, processing-in-memory functionality, PIM functionality, graph workloads, software mechanisms, HMC 2.0 specification, energy consumption, processing-in-memory, PIM, graph computing, hybrid memory cube, HMC, 
* **Abstract**: With the emergence of data science, graph computing has become increasingly important these days. Unfortunately, graph computing typically suffers from poor performance when mapped to modern computing systems because of the overhead of executing atomic operations and inefficient utilization of the memory subsystem. Meanwhile, emerging technologies, such as Hybrid Memory Cube (HMC), enable the processing-in-memory (PIM) functionality with offloading operations at an instruction level. Instruction offloading to the PIM side has considerable potentials to overcome the performance bottleneck of graph computing. Nevertheless, this functionality for graph workloads has not been fully explored, and its applications and shortcomings have not been well identified thus far. In this paper, we present GraphPIM, a full-stack solution for graph computing that achieves higher performance using PIM functionality. We perform an analysis on modern graph workloads to assess the applicability of PIM offloading and present hardware and software mechanisms to efficiently make use of the PIM functionality. Following the real-world HMC 2.0 specification, GraphPIM provides performance benefits for graph applications without any user code modification or ISA changes. In addition, we propose an extension to PIM operations that can further bring performance benefits for more graph applications. The evaluation results show that GraphPIM achieves up to a 2.4× speedup with a 37% reduction in energy consumption.

---
### High-Bandwidth Low-Latency Approximate Interconnection Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.38
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920848
* **Key Words**: Optical switches, Forward error correction, Bandwidth, Optical packet switching, Optical interconnections, Network topology, discrete event simulation, error correction codes, forward error correction, optical computing, optical interconnections, quadrature amplitude modulation, random-access storage, high-bandwidth low-latency approximate interconnection networks, deterministic roundoff errors, soft errors, nondeterministic bit flips, error correcting codes, ECC, error-free execution, large-scale computing platforms, RAM, CPU, data corruptions, application execution, optical networks, error-free standard, network performance scalability, optical links, multilevel quadrature amplitude modulation, QAM, forward error correction, FEC, symbol mapping coding, discrete-event simulation, Approximate computing, Interconnect and network interface architectures, 
* **Abstract**: Computational applications are subject to various kinds of numerical errors, ranging from deterministic roundoff errors to soft errors caused by non-deterministic bit flips, which do not lead to application failure but corrupt application results. Non-deterministic bit flips are typically mitigated in hardware using various error correcting codes (ECC). But in practice, due to performance and cost concerns, these techniques do not guarantee error-free execution. On large-scale computing platforms, soft errors occur with non-negligible probability in RAM and on the CPU, and it has become clear that applications must tolerate them. For some applications, this tolerance is intrinsic as result quality can remain acceptable even in the presence of soft errors (e.g., data analysis applications, multimedia applications). Tolerance can also be built into the application, resolving data corruptions in software during application execution. By contrast, today's optical networks hold on to a rigid error-free standard, which imposes limits on network performance scalability. In this work we propose high-bandwidth, low-latency approximate networks with the following three features: (1) Optical links that exploit multi-level quadrature amplitude modulation (QAM) for achieving high bandwidth; (2) Avoidance of forward error correction (FEC), which makes optical link error-prone but affords lower latency; and (3) The use of symbol mapping coding between bit sequence and QAM to ensure data integrity that is sufficient for practical soft-error-tolerant applications. Discrete-event simulation results for application benchmarks show that approx networks achieve speedups up to 2.94 when compared to conventional networks.

---
### Compute Caches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.21
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920849
* **Key Words**: Computer architecture, Program processors, Random access memory, Robustness, Geometry, Throughput, Databases, cache storage, memory architecture, performance evaluation, power aware computing, reliability, SRAM chips, Compute Cache architecture, in-place computation, bit-line SRAM circuit technology, cache elements, vector computational units, cache hierarchy, coherence, consistency, reliability, energy reduction, data-centric applications, database query processing, cryptographic kernels, in-memory checkpointing, Compute Cache operations, dynamic energy savings, Cache, Data Movement, Vector Processing, In-Memory Processing, Performance, Energy, 
* **Abstract**: This paper presents the Compute Cache architecture that enables in-place computation in caches. Compute Caches uses emerging bit-line SRAM circuit technology to re-purpose existing cache elements and transforms them into active very large vector computational units. Also, it significantly reduces the overheads in moving data between different levels in the cache hierarchy. Solutions to satisfy new constraints imposed by Compute Caches such as operand locality are discussed. Also discussed are simple solutions to problems in integrating them into a conventional cache hierarchy while preserving properties such as coherence, consistency, and reliability. Compute Caches increase performance by 1.9× and reduce energy by 2.4× for a suite of data-centric applications, including text and database query processing, cryptographic kernels, and in-memory checkpointing. Applications with larger fraction of Compute Cache operations could benefit even more, as our micro-benchmarks indicate (54× throughput, 9× dynamic energy savings).

---
### Boomerang: A Metadata-Free Architecture for Control Flow Delivery.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.53
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920850
* **Key Words**: Prefetching, Servers, Metadata, Pipelines, Complexity theory, cache storage, data flow computing, meta data, high-performance control flow delivery, metadata-free architecture, branch-predictor-directed prefetching, missing instruction cache blocks, core front-end, CMP, LLC access latencies, multiMB server binaries, BTB entries, Boomerang, Server Processors, Microarchitecture, Core Front-end, Instruction Prefetching, BTB Prefetching, Branch-predictor-directed Prefetching, Control Flow Delivery, 
* **Abstract**: Contemporary server workloads feature massive instruction footprints stemming from deep, layered software stacks. The active instruction working set of the entire stack can easily reach into megabytes, resulting in frequent frontend stalls due to instruction cache misses and pipeline flushes due to branch target buffer (BTB) misses. While a number of techniques have been proposed to address these problems, every one of them requires dedicated metadata structures, translating into significant storage and complexity costs. In this paper, we ask the question whether it is possible to achieve high-performance control flow delivery without the metadata costs of prior techniques. We revisit a previously proposed approach of branch-predictor-directed prefetching, which leverages just the branch predictor and BTB to discover and prefetch the missing instruction cache blocks by exploring the program control flow ahead of the core front-end. Contrary to conventional wisdom, we find that this approach can be effective in covering instruction cache misses in modern CMPs with long LLC access latencies and multi-MB server binaries. Our first contribution lies in explaining the reasons for the efficacy of branch-predictor-directed prefetching. Our second contribution is in Boomerang, a metadata-free architecture for control flow delivery. Boomerang leverages a branch-predictor-directed prefetcher to discover and prefill not only the instruction cache blocks, but also the missing BTB entries. Crucially, we demonstrate that the additional hardware cost required to identify and fill BTB misses is negligible. Our experimental evaluation shows that Boomerang matches the performance of the state-of-the-art control flow delivery scheme without the latter's high metadata and complexity overheads.

---
### PABST: Proportionally Allocated Bandwidth at the Source and Target.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.33
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920851
* **Key Words**: Bandwidth, Quality of service, Resource management, Computer architecture, Channel allocation, Software, Regulators, bandwidth allocation, computer centres, memory architecture, quality of service, proportionally allocated bandwidth-at-the-source-and-target, total cost-of-ownership, data center, equipment cost reduction, energy consumption reduction, quality of service, QoS, bandwidth interference, hardware architecture, software-controlled memory bandwidth partitioning, application bandwidth control, request rate throttling, PABST, memory latency, memory controller, TCO, performance isolation, memory-intensive background jobs, quality of service, memory bandwidth, proportional share, manycore, data center, 
* **Abstract**: Higher integration lowers total cost of ownership (TCO) in the data center by reducing equipment cost and lowering energy consumption. However, higher integration also makes it difficult to achieve guaranteed quality of service (QoS) for shared resources. Unlike many other resources, memory bandwidth cannot be finely controlled by software in existing systems. As a result, many systems running critical, bandwidth-sensitive applications remain underutilized to protect against bandwidth interference. In this paper, we propose a novel hardware architecture allowing practical, software-controlled partitioning of memory bandwidth. Proportionally Allocated Bandwidth at the Source and Target (PABST) precisely controls the bandwidth of applications by throttling request rates at the source and prioritizes requests at the target. We show that PABST is work conserving, such that excess bandwidth beyond the requested allocation will not go unused. For applications sensitive to memory latency, we pair PABST with a simple priority scheme at the memory controller. We show that when combined, the system is able to lower TCO by providing performance isolation across a wide range of workloads, even when co-located with memory-intensive background jobs.

---
### SOUP-N-SALAD: Allocation-Oblivious Access Latency Reduction with Asymmetric DRAM Microarchitectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.31
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920852
* **Key Words**: Random access memory, Organizations, Software, Performance evaluation, Memory management, Hardware, DRAM chips, SOUP-N-SALAD, allocation-oblivious access latency reduction, asymmetric DRAM microarchitectures, memory access latency, memory-level parallelism, low-latency DRAM organizations, software stack, nonuniform access latency, microarchitectural techniques, DRAM chip, DRAM device architecture, symmetric access latency, asymmetric DRAM bank organizations, data transfer time, column decoders, I/O transfers, memory clock cycles, DDR4 device, memory-intensive SPEC CPU2006 workloads, software modifications, DRAM, symmetric access latency, asymmetric DRAM bank organizations, skewed and pipelined accesses, 
* **Abstract**: Memory access latency has a significant impact on application performance. Unfortunately, the random access latency of DRAM has been scaling relatively slowly, and often directly affects the critical path of execution, especially for applications with insufficient locality or memory-level parallelism. The existing low-latency DRAM organizations either incur significant area overhead or burden the software stack with non-uniform access latency. This paper proposes two microarchitectural techniques to provide uniformly low access time over the entire DRAM chip. The first technique is SALAD, a new DRAM device architecture that provides symmetric access latency with asymmetric DRAM bank organizations. Because local regions have lower data transfer time due to their proximity to the I/O pads, SALAD applies high aspectratio (i.e., low-latency) mats only to remote regions to offset the difference in data transfer time, resulting in symmetrically low latency across regions. The second technique is SOUP (skewed organization of μbanks with pipelined accesses), which leverages asymmetry in column access latency within a region due to non-uniform distance to the column decoders. By starting I/O transfers as soon as data from near cells arrive, instead of waiting for the entire column data, SOUP further saves two memory clock cycles for column accesses for all regions. The resulting design, called SOUP-N-SALAD, improves IPC and EDP by 9.6% (11.2%) and 18.2% (21.8%) over the baseline DDR4 device, respectively, for memory-intensive SPEC CPU2006 workloads without any software modifications, while incurring only 3% (6%) area overhead.

---
### Transparent and Efficient CFI Enforcement with Intel Processor Trace.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.18
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920853
* **Key Words**: Decoding, Runtime, Hardware, Security, Libraries, Instruments, Registers, fuzzy set theory, microprocessor chips, program debugging, CFI enforcement, Intel processor trace, control flow integrity, instrumenting application executables, shared libraries, runtime overhead, runtime control flow information, FlowGuard, IPT, offline performance analysis, software debugging, CFG edges, fuzzing-like dynamic training, commodity Intel Skylake machine, hardware extensions, Control-flow integrity, Intel processor trace, 
* **Abstract**: Current control flow integrity (CFI) enforcement approaches either require instrumenting application executables and even shared libraries, or are unable to defend against sophisticated attacks due to relaxed security policies, or both; many of them also incur high runtime overhead. This paper observes that the main obstacle of providing transparent and strong defense against sophisticated adversaries is the lack of sufficient runtime control flow information. To this end, this paper describes FlowGuard, a lightweight, transparent CFI enforcement approach by a novel reuse of Intel Processor Trace (IPT), a recent hardware feature that efficiently captures the entire runtime control flow. The main challenge is that IPT is designed for offline performance analysis and software debugging such that decoding collected control flow traces is prohibitively slow on the fly. FlowGuard addresses this challenge by reconstructing applications' conservative control flow graphs (CFG) to be compatible with the compressed encoding format of IPT, and labeling the CFG edges with credits in the help of fuzzing-like dynamic training. At runtime, FlowGuard separates fast and slow paths such that the fast path compares the labeled CFGs with the IPT traces for fast filtering, while the slow path decodes necessary IPT traces for strong security. We have implemented and evaluated FlowGuard on a commodity Intel Skylake machine with IPT support. Evaluation results show that FlowGuard is effective in enforcing CFI for several applications, while introducing only small performance overhead. We also show that, with minor hardware extensions, the performance overhead can be further reduced.

---
### PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.55
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920854
* **Key Words**: Training, Neural networks, Machine learning, Computer architecture, Pipelines, Testing, Kernel, data analysis, graphics processing units, learning (artificial intelligence), neural nets, pipeline processing, resistive RAM, PipeLayer, pipelined ReRAM-based accelerator, convolutional neural networks, CNNs, deep learning applications, PRIME, ISAAC, resistive random access memory, neural memory computations, complex data dependency, pipeline bubbles, ReRAM-based PIM accelerator, data analysis, weight update, training algorithms and, inter-layer parallelism, GPU platform, GPU implementation, 
* **Abstract**: Convolutional neural networks (CNNs) are the heart of deep learning applications. Recent works PRIME [1] and ISAAC [2] demonstrated the promise of using resistive random access memory (ReRAM) to perform neural computations in memory. We found that training cannot be efficiently supported with the current schemes. First, they do not consider weight update and complex data dependency in training procedure. Second, ISAAC attempts to increase system throughput with a very deep pipeline. It is only beneficial when a large number of consecutive images can be fed into the architecture. In training, the notion of batch (e.g. 64) limits the number of images can be processed consecutively, because the images in the next batch need to be processed based on the updated weights. Third, the deep pipeline in ISAAC is vulnerable to pipeline bubbles and execution stall. In this paper, we present PipeLayer, a ReRAM-based PIM accelerator for CNNs that support both training and testing. We analyze data dependency and weight update in training algorithms and propose efficient pipeline to exploit inter-layer parallelism. To exploit intra-layer parallelism, we propose highly parallel design based on the notion of parallelism granularity and weight replication. With these design choices, PipeLayer enables the highly pipelined execution of both training and testing, without introducing the potential stalls in previous work. The experiment results show that, PipeLayer achieves the speedup of 42.45x compared with GPU platform on average. The average energy saving of PipeLayer compared with GPU implementation is 7.17x.

---
### FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.29
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920855
* **Key Words**: Parallel processing, Neurons, Computer architecture, Kernel, Clocks, Pipelines, Biological neural networks, feedforward neural nets, parallel architectures, power aware computing, convolutional neural networks, CNN accelerators, CNN intrinsic parallelism, CNN workloads, resource utilization, flexible dataflow architecture, feature map, neuron, synapse parallelism, performance speedup, power efficiency improvement, FlexFlow, Flexible Dataflow, Complementary Effect, Convolutional Neural Networks, Accelerator, 
* **Abstract**: Convolutional Neural Networks (CNN) are very computation-intensive. Recently, a lot of CNN accelerators based on the CNN intrinsic parallelism are proposed. However, we observed that there is a big mismatch between the parallel types supported by computing engine and the dominant parallel types of CNN workloads. This mismatch seriously degrades resource utilization of existing accelerators. In this paper, we propose a flexible dataflow architecture (FlexFlow) that can leverage the complementary effects among feature map, neuron, and synapse parallelism to mitigate the mismatch. We evaluated our design with six typical practical workloads, it acquires 2-10x performance speedup and 2.5-10x power efficiency improvement compared with three state-of-the-art accelerator architectures. Meanwhile, FlexFlow is highly scalable with growing computing engine scale.

---
### Needle: Leveraging Program Analysis to Analyze and Extract Accelerators from Whole Programs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.59
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920856
* **Key Words**: Needles, Hardware, Acceleration, Out of order, Complexity theory, Compounds, microprocessor chips, program compilers, program diagnostics, program analysis, hardware accelerators, computer architects, program paths, dynamic program behavior, path-based accelerator offloading, dynamic instructions, energy efficiency, diverse control flow behavior, LLVM based compiler framework, dynamic profile information, NEEDLE, program abstraction, dataflow size, coarse grained offloading, CPU core, software frames, accelerator microarchitecture, SPEC, PARSEC, PERFECT workload suites, Synthesis, Hardware Accelerators, Program analysis, LLVM, Speculation, Braids, Hyperblocks, Frames, Traces, Energy Efficiency, 
* **Abstract**: Technology constraints have increasingly led to the adoption of specialized coprocessors, i.e. hardware accelerators. The first challenge that computer architects encounter is identifying “what to specialize in the program”. We demonstrate that this requires precise enumeration of program paths based on dynamic program behavior. We hypothesize that path-based [4] accelerator offloading leads to good coverage of dynamic instructions and improve energy efficiency. Unfortunately, hot paths across programs demonstrate diverse control flow behavior. Accelerators (typically based on dataflow execution), often lack an energy-efficient, complexity effective, and high performance (eg. branch prediction) support for control flow. We have developed NEEDLE, an LLVM based compiler framework that leverages dynamic profile information to identify, merge, and offload acceleratable paths from whole applications. NEEDLE derives insight into what code coverage (and consequently energy reduction) an accelerator can achieve. We also develop a novel program abstraction for offload calledBraid, that merges common code regions across different paths to improve coverage of the accelerator while trading off the increase in dataflow size. This enables coarse grained offloading, reducing interaction with the host CPU core. To prepare the Braids and paths for acceleration, NEEDLE generates software frames. Software frames enable energy efficient speculative execution on accelerators. They are accelerator microarchitecture independent support speculative execution including memory operations. NEEDLE is automated and has been used to analyze 225K paths across 29 workloads. It filtered and ranked 154K paths for acceleration across unmodified SPEC, PARSEC and PERFECT workload suites. We target NEEDLE's offload regions toward a CGRA and demonstrate 34% performance and 20% energy improvement.

---
### Radiation-Induced Error Criticality in Modern HPC Parallel Accelerators.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.41
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920857
* **Key Words**: Reliability, Computer architecture, Measurement, Neutrons, Resilience, Computer crashes, Supercomputers, arithmetic, finite difference methods, iterative methods, parallel processing, radiation-induced error criticality, HPC parallel accelerators, high-performance computing, Intel Xeon Phi, NVIDIA K40, HPC device radiation sensitivity, mean relative error, radiation-induced error magnitude, arithmetic operations, finite difference methods, iterative stencil operations, Radiation-induced errors, HPC sensitivity, Fault Tolerance, 
* **Abstract**: In this paper, we evaluate the error criticality of radiation-induced errors on modern High-Performance Computing (HPC) accelerators (Intel Xeon Phi and NVIDIA K40) through a dedicated set of metrics. We show that, as long as imprecise computing is concerned, the simple mismatch detection is not sufficient to evaluate and compare the radiation sensitivity of HPC devices and algorithms. Our analysis quantifies and qualifies radiation effects on applications' output correlating the number of corrupted elements with their spatial locality. Also, we provide the mean relative error (dataset-wise) to evaluate radiation-induced error magnitude. We apply the selected metrics to experimental results obtained in various radiation test campaigns for a total of more than 400 hours of beam time per device. The amount of data we gathered allows us to evaluate the error criticality of a representative set of algorithms from HPC suites. Additionally, based on the characteristics of the tested algorithms, we draw generic reliability conclusions for broader classes of codes. We show that arithmetic operations are less critical for the K40, while Xeon Phi is more reliable when executing particles interactions solved through Finite Difference Methods. Finally, iterative stencil operations seem the most reliable on both architectures.

---
### Pilot Register File: Energy Efficient Partitioned Register File for GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.47
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920858
* **Key Words**: Registers, Radio frequency, FinFETs, Kernel, Delays, Graphics processing units, Logic gates, graphics processing units, integrated circuit design, MOSFET, performance evaluation, power aware computing, program compilers, pilot register file, energy efficient partitioned register file, general purpose computing, concurrently active threads, power consumption, near-threshold voltage, NTV, MOSFET devices, FinFET devices, chip industry, GPU designs, back gate control, hybrid profiling technique, compiler based profiling, pilot warp profiling technique, RF leakage, dynamic energy, performance overhead, GPUs, Register File, Low power, FinFET, 
* **Abstract**: GPU adoption for general purpose computing has been accelerating. To support a large number of concurrently active threads, GPUs are provisioned with a very large register file (RF). The RF power consumption is a critical concern. One option to reduce the power consumption dramatically is to use near-threshold voltage(NTV) to operate the RF. However, operating MOSFET devices at NTV is fraught with stability and reliability concerns. The adoption of FinFET devices in chip industry is providing a promising path to operate the RF at NTV while satisfactorily tackling the stability and reliability concerns. However, the fundamental problem of NTV operation, namely slow access latency, remains. To tackle this challenge in this paper we propose to build a partitioned RF using FinFET technology. The partitioned RF design exploits our observation that applications exhibit strong preference to utilize a small subset of their registers. One way to exploit this behavior is to cache the RF content as has been proposed in recent works. However, caching leads to unnecessary area overheads since a fraction of the RF must be replicated. Furthermore, we show that caching is not efficient as we increase the number of issued instructions per cycle, which is the expected trend in GPU designs. The proposed partitioned RF splits the registers into two partitions: the highly accessed registers are stored in a small RF that switches between high and low power modes. We use the FinFET's back gate control to provide low overhead switching between the two power modes. The remaining registers are stored in a large RF partition that always operates at NTV. The assignment of the registers to the two partitions will be based on statistics collected by the a hybrid profiling technique that combines the compiler based profiling and the pilot warp profiling technique proposed in this paper. The partitioned FinFET RF is able to save 39% and 54% of the RF leakage and the dynamic energy, respectively, a...

---
### G-Scalar: Cost-Effective Generalized Scalar Execution Architecture for Power-Efficient GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.51
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920859
* **Key Words**: Registers, Graphics processing units, Pipelines, Hardware, Computer architecture, Microarchitecture, Power demand, graphics processing units, power consumption, G-Scalar, cost-effective generalized scalar execution architecture, power-efficient GPU, execution resources, power efficiency, power wall challenge, nondivergent arithmetic/logic instructions, special-function instructions, contemporary GPU applications, register value compression, power consumption reduction, GPU, scalar execution, register file, register file compression, 
* **Abstract**: The GPU has provide higher throughput by integrating more execution resources into a single chip without unduly compromising power efficiency. With the power wall challenge, however, increasing the throughput will require significant improvement in power efficiency. To accomplish this goal, we propose G-Scalar, a cost-effective generalized scalar execution architecture for GPUs in this paper. G-Scalar offers two key advantages over prior architectures supporting scalar execution for only non-divergent arithmetic/logic instructions. First, G-Scalar is more power-efficient as it can also support scalar execution of divergent and special-function instructions, the fraction of which in contemporary GPU applications has notably increased. Second, G-Scalar is less expensive as it can share most of its hardware resources with register value compression, of which adoption has been strongly promoted to reduce high power consumption of accessing the large register file. Compared with the baseline and previous scalar architectures, G-Scalar improves power efficiency by 24% and 15%, respectively, at a negligible cost.

---
### Dynamic GPGPU Power Management Using Adaptive Model Predictive Control.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.34
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920860
* **Key Words**: Kernel, Graphics processing units, Hardware, Niobium, Throughput, Benchmark testing, Performance evaluation, adaptive control, graphics processing units, power aware computing, predictive control, dynamic GPGPU power management, adaptive model predictive control, modern processors, energy efficiency, dynamic voltage and frequency scaling, proactive techniques, MPC, GPU kernels, CPU-GPU heterogeneous processor, execution history, performance overhead, horizon length, low-throughput phases, high-throughput kernels, 
* **Abstract**: Modern processors can greatly increase energy efficiency through techniques such as dynamic voltage and frequency scaling. Traditional predictive schemes are limited in their effectiveness by their inability to plan for the performance and energy characteristics of upcoming phases. To date, there has been little research exploring more proactive techniques that account for expected future behavior when making decisions. This paper proposes using Model Predictive Control (MPC) to attempt to maximize the energy efficiency of GPU kernels without compromising performance. We develop performance and power prediction models for a recent CPU-GPU heterogeneous processor. Our system then dynamically adjusts hardware states based on recent execution history, the pattern of upcoming kernels, and the predicted behavior of those kernels. We also dynamically trade off the performance overhead and the effectiveness of MPC in finding the best configuration by adapting the horizon length at runtime. Our MPC technique limits performance loss by proactively spending energy on the kernel iterations that will gain the most performance from that energy. This energy can then be recovered in future iterations that are less performance sensitive. Our scheme also avoids wasting energy on low-throughput phases when it foresees future high-throughput kernels that could better use that energy. Compared to state-of-the-practice schemes, our approach achieves 24.8% energy savings with a performance loss (including MPC overheads) of 1.8%. Compared to state-of-the-art history-based schemes, our approach achieves 6.6% chip-wide energy savings while simultaneously improving performance by 9.6%.

---
### Efficient Sequential Consistency in GPUs via Relativistic Cache Coherence.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.40
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920861
* **Key Words**: Computer architecture, cache storage, graphics processing units, relativistic cache coherence, sequential consistency, weak memory models, GPU coherence protocol, timestamps, global memory order, GPU SC proposals, invasive core, GPU, coherence, consistency, 
* **Abstract**: Recent work has argued that sequential consistency (SC) in GPUs can perform on par with weak memory models, provided ordering stalls are made less frequent by relaxing ordering for private and read-only data. In this paper, we address the complementary problem of reducing stall latencies for both read-only and read-write data. We find that SC stalls are particularly problematic for workloads with inter-workgroup sharing, and occur primarily due to earlier stores in the same thread; a substantial part of the overhead comes from the need to stall until write permissions are obtained (to ensure write atomicity). To address this, we propose RCC, a GPU coherence protocol which grants write permissions without stalling but can still be used to implement SC. RCC uses logical timestamps to determine a global memory order and L1 read permissions; even though each core may see a different logical "time," SC ordering can still be maintained. Unlike previous GPU SC proposals, our design does not require invasive core changes and additional per-core storage to classify read-only/private data. For workloads with interworkgroup sharing overall performance is 29% better and energy is 25% less than in best previous GPU SC proposals, and within 7% of the best non-SC design.

---
### Processing-in-Memory Enabled Graphics Processors for 3D Rendering.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.37
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920862
* **Key Words**: Three-dimensional displays, Rendering (computer graphics), Graphics processing units, Bandwidth, Memory management, Through-silicon vias, Random access memory, DRAM chips, graphics processing units, rendering (computer graphics), processing-in-memory enabled graphics processors, 3D rendering, graphics processing unit, 3D vector stream, 2D frame, 3D image effects, user gaming experience, modern computer systems, main memory bandwidth, 3D-stacked memory systems, hybrid memory cube, logic controllers, DRAM, HMC, off-chip memory traffic, processing-in-memory based GPU, texture filtering, GPU, Processing-In-Memory, 3D-Stacked Memory, Approximate Computing, 3D Rendering, 
* **Abstract**: The performance of 3D rendering of Graphics Processing Unit that converts 3D vector stream into 2D frame with 3D image effects significantly impacts users gaming experience on modern computer systems. Due to its high texture throughput requirement, main memory bandwidth becomes a critical obstacle for improving the overall rendering performance. 3D-stacked memory systems such as Hybrid Memory Cube provide opportunities to significantly overcome the memory wall by directly connecting logic controllers to DRAM dies. Although recent works have shown promising improvement in performance by utilizing HMC to accelerate special-purpose applications, a critical challenge of how to effectively leverage its high internal bandwidth and computing capability in GPU for 3D rendering remains unresolved. Based on the observation that texel fetches greatly impact off-chip memory traffic, we propose two architectural designs to enable Processing-In-Memory based GPU for efficient 3D rendering. Additionally, we employ camera angles of pixels to control the performance-quality tradeoff of 3D rendering. Extensive evaluation across several real-world games demonstrates that our design can significantly improve the performance of texture filtering and 3D rendering by an average of 3.97X (up to 6.4X) and 43% (up to 65%) respectively, over the baseline GPU. Meanwhile, our design provides considerable memory traffic and energy reduction without sacrificing rendering quality.

---
### Controlled Kernel Launch for Dynamic Parallelism in GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/HPCA.2017.14
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7920863
* **Key Words**: Kernel, Graphics processing units, Instruction sets, Parallel processing, Benchmark testing, Hardware, Performance evaluation, graphics processing units, parallel processing, resource allocation, scheduling, controlled kernel launch, dynamic parallelism, kernel on-demand spawning, GPU kernels, dynamically-generated kernels, SPAWN runtime framework, scheduler, GPU resource utilization, 
* **Abstract**: Dynamic parallelism (DP) is a promising feature for GPUs, which allows on-demand spawning of kernels on the GPU without any CPU intervention. However, this feature has two major drawbacks. First, the launching of GPU kernels can incur significant performance penalties. Second, dynamically-generated kernels are not always able to efficiently utilize the GPU cores due to hardware-limits. To address these two concerns cohesively, we propose SPAWN, a runtime framework that controls the dynamically-generated kernels, thereby directly reducing the associated launch overheads and queuing latency. Moreover, it allows a better mix of dynamically-generated and original (parent) kernels for the scheduler to effectively hide the remaining overheads and improve the utilization of the GPU resources. Our results show that, across 13 benchmarks, SPAWN achieves 69% and 57% speedup over the flat (non-DP) implementation and baseline DP, respectively.
