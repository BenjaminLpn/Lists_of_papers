### micro 2016 | 62 papers.
---
### Keynotes: Internet of Things: History and hype, technology and policy.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783703
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783703
* **Key Words**: data analysis, intelligent sensors, Internet of Things, Internet of Things, IoT, 50B networked devices, embedded sensing, embedded control, vehicle traffic flow, data analysis, natural resources, 
* **Abstract**: Summary form only given. The idea of an emerging Internet of Things (IoT) is currently captivating both technologists and society at large. Although IoT techniques have their roots in ideas that are decades old, their increasingly widespread deployments have made them a hot topic these days, frequently discussed and hyped. As many as 50B networked devices are envisioned by 2020, and proponents of IoTs see a world where embedded sensing and control techniques help vehicle traffic flow more smoothly, where environmental sensing and data analysis facilitates better use of natural resources like water, and where personalized health monitoring helps individuals improve their quality of life. On the other hand, properly addressing policy concerns around security and privacy may play a role in IoT's adoption and success. My talk will discuss key technology and policy challenges for future IoT applications and devices. Overall, I will be drawing from both technical experiences and trends, as well as from policy perspectives gained during a one year fellowship doing technology policy within the U. S. Department of State.

---
### Dictionary sharing: An efficient cache compression scheme for compressed caches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783704
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783704
* **Key Words**: Compaction, Layout, Dictionaries, Encoding, System performance, Electronic mail, Indexes, cache storage, data compression, dictionary sharing, skewed compressed cache, yet another compressed cache, compressed cache layouts, storage overheads, latency overheads, SCC, YACC, multiple contiguous compressed blocks, data entry, cache blocks, cache space, dictionary based cache compression, DISH, 
* **Abstract**: The effectiveness of a compressed cache depends on three features: i) the compression scheme, ii) the compaction scheme, and iii) the cache layout of the compressed cache. Skewed compressed cache (SCC) and yet another compressed cache (YACC) are two recently proposed compressed cache layouts that feature minimal storage and latency overheads, while achieving comparable performance over more complex compressed cache layouts. Both SCC and YACC use compression techniques to compress individual cache blocks, and then a compaction technique to compact multiple contiguous compressed blocks into a single data entry. The primary attribute used by these techniques for compaction is the compression factor of the cache blocks, and in this process, they waste cache space. In this paper, we propose dictionary sharing (DISH), a dictionary based cache compression scheme that reduces this wastage. DISH compresses a cache block by keeping in mind that the block is a potential candidate for the compaction process. DISH encodes a cache block with a dictionary that stores the distinct 4-byte chunks of a cache block and the dictionary is shared among multiple neighboring cache blocks. The simple encoding scheme of DISH also provides a single cycle decompression latency and it does not change the cache layout of compressed caches. Compressed cache layouts that use DISH outperforms the compression schemes, such as BDI and CPACK+Z, in terms of compression ratio (from 1.7X to 2.3X), system performance (from 7.2% to 14.3%), and energy efficiency (from 6% to 16%).

---
### Perceptron learning for reuse prediction.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783705
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783705
* **Key Words**: Radiation detectors, Optimization, Marine vehicles, Prediction algorithms, Indexes, Feature extraction, History, cache storage, learning (artificial intelligence), multiprogramming, optimisation, perceptrons, reuse prediction, perceptron learning, last-level cache latencies, memory latencies, cache management policies, cache blocks, cache performance, cache efficiency, multiprogrammed workloads, average false positive rate, sampling dead block prediction, signature-based hit prediction, single-thread workloads, last-level cache, bypass optimization, geometric mean speedup, SHiP, SDBP, SPEC CPU 2006 benchmarks, LRU, geometric mean normalized weighted speedup, 
* **Abstract**: The disparity between last-level cache and memory latencies motivates the search for efficient cache management policies. Recent work in predicting reuse of cache blocks enables optimizations that significantly improve cache performance and efficiency. However, the accuracy of the prediction mechanisms limits the scope of optimization. This paper proposes perceptron learning for reuse prediction. The proposed predictor greatly improves accuracy over previous work. For multi-programmed workloads, the average false positive rate of the proposed predictor is 3.2%, while sampling dead block prediction (SDBP) and signature-based hit prediction (SHiP) yield false positive rates above 7%. The improvement in accuracy translates directly into performance. For single-thread workloads and a 4MB last-level cache, reuse prediction with perceptron learning enables a replacement and bypass optimization to achieve a geometric mean speedup of 6.1%, compared with 3.8% for SHiP and 3.5% for SDBP on the SPEC CPU 2006 benchmarks. On a memory-intensive subset of SPEC, perceptron learning yields 18.3% speedup, versus 10.5% for SHiP and 7.7% for SDBP. For multi-programmed workloads and a 16MB cache, the proposed technique doubles the efficiency of the cache over LRU and yields a geometric mean normalized weighted speedup of 7.4%, compared with 4.4% for SHiP and 4.2% for SDBP.

---
### pTask: A smart prefetching scheme for OS intensive applications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783706
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783706
* **Key Words**: Prefetching, Benchmark testing, Hardware, Web servers, Databases, operating systems (computers), storage management, pTask, smart prefetching, OS intensive applications, instruction prefetching, operating system intensive workloads, Web servers, file servers, database servers, PIF, RDIP, hardware structures, system call handlers, OS event, application codes, instruction prefetching techniques, 
* **Abstract**: Instruction prefetching is a standard approach to improve the performance of operating system (OS) intensive workloads such as web servers, file servers and database servers. Sophisticated instruction prefetching techniques such as PIF [12] and RDIP [17] record the execution history of a program in dedicated hardware structures and use this information for prefetching if a known execution pattern is repeated. The storage overheads of the additional hardware structures are prohibitively high (64-200 KB per core). This makes it difficult for the deployment of such schemes in real systems. We propose a solution that uses minimal hardware modifications to tackle this problem. We notice that the execution of server applications keeps switching between tasks such as the application, system call handlers, and interrupt handlers. Each task has a distinct instruction footprint, and is separated by a special OS event. We propose a sophisticated technique to capture the instruction stream in the vicinity of such OS events; the captured information is then compressed significantly and is stored in a process's virtual address space. Special OS routines then use this information to prefetch instructions for the OS and the application codes. Using modest hardware support (4 registers per core), we report an increase in instruction throughput of 2-14% (mean: 7%) over state of the art instruction prefetching techniques for a suite of 8 popular OS intensive applications.

---
### Register sharing for equality prediction.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783707
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783707
* **Key Words**: Registers, Benchmark testing, Radiation detectors, Complexity theory, Optimization, Transforms, Microarchitecture, benchmark testing, optimising compilers, parallel programming, program processors, register sharing, value prediction, instruction level parallelism, ILP, static instructions, inflight instruction, register equality prediction, distance prediction, speculative memory bypassing, short-circuiting def-store-load-use chains, renaming-based register equality prediction, SPEC benchmarks, 
* **Abstract**: Recently, Value Prediction (VP) has been gaining renewed traction in the research community. VP speculates on the result of instructions to increase Instruction Level Parallelism (ILP). In most embodiments, VP requires large tables to track predictions for many static instructions. However, in many cases, it is possible to detect that the result of an instruction is produced by an older inflight instruction, but not to predict the result itself. Consequently it is possible to rely on predicting register equality and handle speculation through the renamer. To do so, we propose to use Distance Prediction, a technique that was previously used to perform Speculative Memory Bypassing (short-circuiting def-store-load-use chains). Distance Prediction attempts to determine how many instructions separate the instruction of interest and the most recent older instruction that produced the same result. With this information, the physical register identifier of the older instruction can be retrieved from the ROB and provided to the renamer. In this paper, we first quantify the performance gains brought by renaming-based register equality prediction and regular VP on SPEC benchmarks. Second, we study the overlap between the two different schemes and show that those mechanisms often capture different behavior.

---
### Data-centric execution of speculative parallel programs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783708
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783708
* **Key Words**: Parallel processing, Logic gates, Hardware, Integrated circuit modeling, Scalability, Runtime, Multicore processing, abstract data types, multiprocessing systems, parallel architectures, parallel programming, data-centric execution, speculative parallel programs, multicore systems, data movement minimization, locality-aware parallelism, nonspeculative systems, spatial hints, program knowledge, balancing tasks, near-linear scalability, hint-oblivious scheduling, thread-level speculation, TLS, hardware transactional memory, HTM, 
* **Abstract**: Multicore systems must exploit locality to scale, scheduling tasks to minimize data movement. While locality-aware parallelism is well studied in non-speculative systems, it has received little attention in speculative systems (e.g., HTM or TLS), which hinders their scalability. We present spatial hints, a technique that leverages program knowledge to reveal and exploit locality in speculative parallel programs. A hint is an abstract integer, given when a speculative task is created, that denotes the data that the task is likely to access. We show it is easy to modify programs to convey locality through hints. We design simple hardware techniques that allow a state-of-the-art, tiled speculative architecture to exploit hints by: (i) running tasks likely to access the same data on the same tile, (ii) serializing tasks likely to conflict, and (iii) balancing tasks across tiles in a locality-aware fashion. We also show that programs can often be restructured to make hints more effective. Together, these techniques make speculative parallelism practical on large-scale systems: at 256 cores, hints achieve near-linear scalability on nine challenging applications, improving performance over hint-oblivious scheduling by 3.3× gmean and by up to 16×. Hints also make speculation far more efficient, reducing wasted work by 6.4× and traffic by 3.5× on average.

---
### SABRes: Atomic object reads for in-memory rack-scale computing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783709
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783709
* **Key Words**: Software, Hardware, Semantics, Atomic measurements, Servers, Fabrics, Aerospace electronics, cache storage, distributed processing, DRAM chips, graphics processing units, in-memory rack-scale computing, in-memory services, distributed object stores, random accesses, remote memory accesses, distributed memory frameworks, one-sided operations, RDMA one-sided operations, single cache block, software mechanisms, integrated rack-scale systems, DRAM latency, technology-triggered paradigm shift calls, SABRes, atomic remote object reads, LightSABRes, lightweight hardware accelerator, atomicity-associated software overheads, 
* **Abstract**: Modern in-memory services rely on large distributed object stores to achieve the high scalability essential to ser-vice thousands of requests concurrently. The independent and unpredictable nature of incoming requests results in random accesses to the object store, triggering frequent remote memory accesses. State-of-the-art distributed memory frameworks leverage the one-sided operations offered by RDMA technology to mitigate the traditionally high cost of remote memory access. Unfortunately, the limited semantics of RDMA one-sided operations bound remote memory access atomicity to a single cache block; therefore, atomic remote object access relies on software mechanisms. Emerging highly integrated rack-scale systems that reduce the latency of one-sided operations to a small multiple of DRAM latency expose the overhead of these software mechanisms as a major latency contributor. This technology-triggered paradigm shift calls for new one-sided operations with stronger semantics. We take a step in that direction by proposing SABRes, a new one-sided operation that provides atomic remote object reads in hardware. We then present LightSABRes, a lightweight hardware accelerator for SABRes that removes all atomicity-associated software overheads. Compared to a state-of-the-art software atomicity mechanism, LightSABRes improve the throughput of a microbenchmark atomically accessing 128B-8KB objects from remote memory by 15-97%, and the throughput of a modern in-memory distributed object store by 30-60%.

---
### A cloud-scale acceleration architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783710
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783710
* **Key Words**: Field programmable gate arrays, Servers, Acceleration, Computer architecture, Hardware, Production, Cloud computing, cloud computing, computer centres, field programmable gate arrays, network servers, reconfigurable architectures, cloud-scale acceleration architecture, hyperscale data center providers, reconfigurable logic, network plane functions, configurable cloud architecture, network switches, production server, Web search ranking, network acceleration, data encryption, secondary rack-scale networks, interFPGA communication, direct FPGA-to-FPGA messages, direct interFPGA messaging, Microsoft production datacenters, 
* **Abstract**: Hyperscale datacenter providers have struggled to balance the growing need for specialized hardware (efficiency) with the economic benefits of homogeneity (manageability). In this paper we propose a new cloud architecture that uses reconfigurable logic to accelerate both network plane functions and applications. This Configurable Cloud architecture places a layer of reconfigurable logic (FPGAs) between the network switches and the servers, enabling network flows to be programmably transformed at line rate, enabling acceleration of local applications running on the server, and enabling the FPGAs to communicate directly, at datacenter scale, to harvest remote FPGAs unused by their local servers. We deployed this design over a production server bed, and show how it can be used for both service acceleration (Web search ranking) and network acceleration (encryption of data in transit at high-speeds). This architecture is much more scalable than prior work which used secondary rack-scale networks for inter-FPGA communication. By coupling to the network plane, direct FPGA-to-FPGA messages can be achieved at comparable latency to previous work, without the secondary network. Additionally, the scale of direct inter-FPGA messaging is much larger. The average round-trip latencies observed in our measurements among 24, 1000, and 250,000 machines are under 3, 9, and 20 microseconds, respectively. The Configurable Cloud architecture has been deployed at hyperscale in Microsoft's production datacenters worldwide.

---
### Towards efficient server architecture for virtualized network function deployment: Implications and implementations.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783711
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783711
* **Key Words**: Pipelines, Software, Handheld computers, Switches, Firewalls (computing), Protocols, computer networks, dynamic programming, resource allocation, scheduling, telecommunication traffic, virtualisation, NUMA-based SHVS, remote packet transmission, optimal thread mapping, dynamic programming-based method, NFV traffic flow, mapping scheme, resource allocation, network functions virtualization, HSP, heterogeneous software pipeline, virtualized network function deployment:, efficient server architecture, NFV, Networking, thread scheduling, NUMA, 
* **Abstract**: Recent years have seen a revolution in network infrastructure brought on by the ever-increasing demands for data volume. One promising proposal to emerge from this revolution is Network Functions Virtualization (NFV), which has been widely adopted by service and cloud providers. The essence of NFV is to run network functions as virtualized workloads on commodity Standard High Volume Servers (SHVS), which is the industry standard. However, our experience using NFV when deployed on modern NUMA-based SHVS paints a frustrating picture. Due to the complexity in the NFV data plane and its service function chain feature, modern NFV deployment on SHVS exhibits a unique processing pattern - heterogeneous software pipeline (HSP), in which the NFV traffic flows must be processed by heterogeneous software components sequentially from the NIC to the end re-ceiver. Since the end-to-end performance of flows is cooperatively determined by the performance of each processing stage, the resource allocation/mapping scheme in NUMA-based SHVS must consider a thread-dependence scheduling to tradeoff the impact of co-located contention and remote packet transmission. In this paper, we develop a thread scheduling mechanism that collaboratively places threads of HSP to minimize the end-to-end performance slowdown for NFV traffic flow. It employs a dynamic programming-based method to search for the optimal thread mapping with negligible overhead. To serve this mechanism, we also develop a performance slowdown estimation model to accurately estimate the performance slowdown at each stage of HSP. We implement our collaborative thread scheduling mechanism on a real system and evaluate it using real workloads. On average, our algorithm outperforms state-of-the-art NUMA-aware and contention-aware scheduling policies by at least 7% on CPU utilization and 23% on traffic throughput with negligible computational overhead (less than 1 second).

---
### Bridging the I/O performance gap for big data workloads: A new NVDIMM-based approach.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783712
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783712
* **Key Words**: Nonvolatile memory, Random access memory, Throughput, Performance evaluation, Interference, Big Data, flash memories, random-access storage, resource allocation, storage management, I-O performance, Big Data workloads, I-O latency, data-intensive applications, nonvolatile dual in-line memory module, nonvolatile NAND flash memory, storage media, dual in-line memory module slots, DIMM slots, NVDIMM devices, memory bus, memory access, flash read-write operations, garbage collection, I-O-derived memory traffic, data access criticality, resource utilization, memory controller, NAND flash controller, NVDIMM controller, 
* **Abstract**: The long I/O latency posts significant challenges for many data-intensive applications, such as the emerging big data workloads. Recently, the NVDIMM (Non-Volatile Dual In-line Memory Module) technologies provide a promising solution to this problem. By employing non-volatile NAND flash memory as storage media and connecting them via DIMM (Dual Inline Memory Module) slots, the NVDIMM devices are exposed to memory bus so the access latencies due to going through I/O controllers can be significantly mitigated. However, placing NVDIMM on the memory bus introduces new challenges. For instance, by mixing I/O and memory traffic, NVDIMM can cause severe performance degradation on memory-intensive applications. Besides, there exists a speed mismatch between fast memory access and slow flash read/write operations. Moreover, garbage collection (GC) in NAND flash may cause up to several millisecond latency. This paper presents novel, enabling mechanisms that allow NVDIMM to more effectively bridge the I/O performance gap for big data workloads. To address the workload heterogeneity challenge, we develop a scheduling scheme in memory controller to minimize the interference between the native and the I/O-derived memory traffic by exploiting both data access criticality and resource utilization. For NVDIMM controller, several mechanisms are designed to better orchestrate traffic between the memory controller and NAND flash to alleviate the speed discrepancy issue. To mitigate the lengthy GC period, we propose a proactive GC scheme for the NVDIMM controller and flash controller to intelligently synchronize and transfer data involving in forthcoming GC operations. We present detailed evaluation and analysis to quantify how well our techniques fit with the NVDIMM design. Our experimental results show that overall the proposed techniques yield 10%~35% performance improvements over the state-of-the-art baseline schemes.

---
### NeSC: Self-virtualizing nested storage controller.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783713
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783713
* **Key Words**: Virtual machine monitors, Bandwidth, Performance evaluation, Software, Hardware, Virtualization, Computer architecture, field programmable gate arrays, network interfaces, virtual machines, virtual storage, virtualisation, self-virtualizing nested storage controller, high-speed multiGB/s storage devices, storage virtualization, hypervisor software layers, virtual machine, direct device assignment, networking interfaces, SR-IOV specification, virtual interfaces, virtual PCIe device interface, data isolation, file system-agnostic protection mechanism, virtual PCIe storage devices, 1GB/s NeSC controller, Virtex-7 FPGA development board, NeSC virtual devices, 
* **Abstract**: The emergence of high-speed, multi GB/s storage devices has shifted the performance bottleneck of storage virtualization to the software layers of the hypervisor. The hypervisor overheads can be avoided by allowing the virtual machine (VM) to directly access the storage device (a method known as direct device assignment), but this method voids all protection guarantees provided by filesystem permissions, since the device has no notion of client isolation. Recently, following the introduction of 10Gbs and higher networking interfaces, the PCIe specification was extended to include the SR-IOV specification for self-virtualizing devices, which allows a single physical device to present multiple virtual interfaces on the PCIe interconnect. Using SR-IOV, a hypervisor can directly assign a virtual PCIe device interface to each of its VMs. However, as networking interfaces simply multiplex packets sent from/to different clients, the specification does not dictate the semantics of a virtual storage device and how to maintain data isolation in a self-virtualizing device. In this paper we present the self-virtualizing, nested storage controller (NeSC) architecture, which includes a filesystem-agnostic protection mechanism that enables the physical device to export files as virtual PCIe storage devices. The protection mechanism maps file offsets to physical blocks and thereby offloads the hypervisor's storage layer functionality to hardware. Using NeSC, a hypervisor can securely expose its files as virtual PCIe devices and directly assign them to VMs. We have prototyped a 1GB/s NeSC controller using a Virtex-7 FPGA development board connected to the PCIe interconnect. Our evaluation of NeSC on a real system shows that NeSC virtual devices enable VMs to access their data with near-native performance (in terms of both throughput and latency).

---
### MIMD synchronization on SIMT architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783714
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783714
* **Key Words**: System recovery, Synchronization, Hardware, Graphics processing units, Instruction sets, Manuals, Programming, flow graphs, multi-threading, program compilers, program debugging, program diagnostics, synchronisation, MIMD synchronization, SIMT architectures, single-instruction multiple-thread execution model, SIMT hardware implementations, thread execution serialization, debuggability, static instruction, synchronization scope limitations, compiler transformation, compiler analysis, adaptive hardware reconvergence mechanism, LLVM compiler, CFG transformation, control flow graph, SIMT deadlock detection, static analysis, SIMT hardware, SIMT scheduling constraints, SIMT machine, multiple-instruction multiple-data architecture, deadlock-free program, 
* **Abstract**: In the single-instruction multiple-threads (SIMT) execution model, small groups of scalar threads operate in lockstep. Within each group, current SIMT hardware implementations serialize the execution of threads that follow different paths, and to ensure efficiency, revert to lockstep execution as soon as possible. These constraints must be considered when adapting algorithms that employ synchronization. A deadlock-free program on a multiple-instruction multiple-data (MIMD) architecture may deadlock on a SIMT machine. To avoid this, programmers need to restructure control flow with SIMT scheduling constraints in mind. This requires programmers to be familiar with the underlying SIMT hardware. In this paper, we propose a static analysis technique that detects SIMT deadlocks by inspecting the application control flow graph (CFG). We further propose a CFG transformation that avoids SIMT deadlocks when synchronization is local to a function. Both the analysis and the transformation algorithms are implemented as LLVM compiler passes. Finally, we propose an adaptive hardware reconvergence mechanism that supports MIMD synchronization without changing the application CFG, but which can leverage our compiler analysis to gain efficiency. The static detection has a false detection rate of only 4%-5%. The automated transformation has an average performance overhead of 8.2%-10.9% compared to manual transformation. Our hardware approach performs on par with the compiler transformation, however, it avoids synchronization scope limitations, static instruction and register overheads, and debuggability challenges that are present in the compiler only solution.

---
### Efficient kernel synthesis for performance portable programming.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783715
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783715
* **Key Words**: Computational modeling, Computer architecture, Kernel, Performance evaluation, Containers, Programming, Algorithm design and analysis, parallel processing, program compilers, software performance evaluation, custom code generator, generic design space composition, generic design space exploration, extensible architectural model, high-performance kernels, architecture-neutral computations, kernel synthesis framework, TANGRAM, hierarchical organization, performance portability, performance portable programming system, substantial software re-development cost, heterogeneous computing systems, microarchitecture designs, performance portable programming, 
* **Abstract**: The diversity of microarchitecture designs in heterogeneous computing systems allows programs to achieve high performance and energy efficiency, but results in substantial software re-development cost for each type or generation of hardware. To mitigate this cost, a performance portable programming system is required. One fundamental difference between architectures that makes performance portability challenging is the hierarchical organization of their computing elements. To address this challenge, we introduce TANGRAM, a kernel synthesis framework that composes architecture-neutral computations and composition rules into high-performance kernels customized for different architectural hierarchies. TANGRAM is based on an extensible architectural model that can be used to specify a variety of architectures. This model is coupled with a generic design space exploration and composition algorithm that can generate multiple composition plans for any specified architecture. A custom code generator then compiles these plans for the target architecture while performing various optimizations such as data placement and tuning. We show that code synthesized by TANGRAM for different types and generations of devices achieves no less than 70% of the performance of highly optimized vendor libraries such as Intel MKL and NVIDIA CUBLAS/CUSPARSE.

---
### KLAP: Kernel launch aggregation and promotion for optimizing dynamic parallelism.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783716
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783716
* **Key Words**: Kernel, Parallel processing, Graphics processing units, Heuristic algorithms, Synchronization, Hardware, Fuses, graphics processing units, parallel processing, program compilers, dynamic parallelism optimization, KLAP, GPU architectures, kernel launch aggregation and promotion, compiler technique set, single aggregated kernel, generation mitigating, limited depth problem, 
* **Abstract**: Dynamic parallelism on GPUs simplifies the programming of many classes of applications that generate paral-lelizable work not known prior to execution. However, modern GPUs architectures do not support dynamic parallelism efficiently due to the high kernel launch overhead, limited number of simultaneous kernels, and limited depth of dynamic calls a device can support. In this paper, we propose Kernel Launch Aggregation and Promotion (KLAP), a set of compiler techniques that improve the performance of kernels which use dynamic parallelism. Kernel launch aggregation fuses kernels launched by threads in the same warp, block, or kernel into a single aggregated kernel, thereby reducing the total number of kernels spawned and increasing the amount of work per kernel to improve occupancy. Kernel launch promotion enables early launch of child kernels to extract more parallelism between parents and children, and to aggregate kernel launches across generations mitigating the problem of limited depth. We implement our techniques in a real compiler and show that kernel launch aggregation obtains a geometric mean speedup of 6.58x over regular dynamic parallelism. We also show that kernel launch promotion enables cases that were not originally possible, improving throughput by a geometric mean of 30.44 x.

---
### Cache-emulated register file: An integrated on-chip memory architecture for high performance GPGPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783717
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783717
* **Key Words**: Radio frequency, System-on-chip, Registers, Kernel, Memory architecture, Instruction sets, memory architecture, parallel processing, storage management chips, cache-emulated register file, integrated on-chip memory architecture, on-chip memory design, CPU architecture, high performance GPGPU computing, microarchitecture, 
* **Abstract**: The on-chip memory design is critical to the GPGPU performance because it serves between the massive threads and the huge external memory as a low-latency and high-throughput data communication point. However, the existing on-chip memory hierarchy is inherited from the conventional CPU architecture and is oftentimes sub-optimal to the SIMT (single instruction, multiple threads) execution. In this study, we surpass the traditional memory hierarchy design and reform the on-chip memory into an integrated architecture with the cache-emulated register file (RF) capability tailored for high performance GPGPU computing. With the lightweight support from ISA, compiler and the modified microarchitecture, this integrated architecture can dynamically emulate a variable-sized RF and a cache in a uniform way. Evaluation results demonstrate that this novel architecture can deliver better performance and energy efficiency with smaller on-chip memory size. For example, it can gain an average of 50% performance improvement for the cache-sensitive applications.

---
### Zorua: A holistic approach to resource virtualization in GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783718
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783718
* **Key Words**: Graphics processing units, Virtualization, Instruction sets, System-on-chip, Resource management, Hardware, Parallel processing, graphics processing units, multi-threading, virtualisation, holistic approach, Zorua, programmer-specified resource usage, resource virtualization framework, on-chip hardware resources, dynamic management, multiple on-chip resources, registers, scratchpad memory, thread slots, hardware-software codesign, thread-level parallelism, latency overheads, capacity overheads, swap space usage, controlled virtualization, coordinated virtualization, hardware resources, fine-grained resource sharing, multiple kernels, low-latency preemption, GPU programs, graphics processing units, runtime system, compiler, programming ease, 
* **Abstract**: This paper introduces a new resource virtualization framework, Zorua, that decouples the programmer-specified resource usage of a GPU application from the actual allocation in the on-chip hardware resources. Zorua enables this decoupling by virtualizing each resource transparently to the programmer. The virtualization provided by Zorua builds on two key concepts - dynamic allocation of the on-chip resources and their oversubscription using a swap space in memory. Zorua provides a holistic GPU resource virtualization strategy, designed to (i) adaptively control the extent of oversubscription, and (ii) coordinate the dynamic management of multiple on-chip resources (i.e., registers, scratchpad memory, and thread slots), to maximize the effectiveness of virtualization. Zorua employs a hardware-software code-sign, comprising the compiler, a runtime system and hardware-based virtualization support. The runtime system leverages information from the compiler regarding resource requirements of each program phase to (i) dynamically allocate/deallocate the different resources in the physically available on-chip resources or their swap space, and (ii) manage the tradeoffbetween higher thread-level parallelism due to virtualization versus the latency and capacity overheads of swap space usage. We demonstrate that by providing the illusion of more resources than physically available via controlled and coordinated virtualization, Zorua offers several important benefits: (i) Programming Ease. Zorua eases the burden on the programmer to provide code that is tuned to efficiently utilize the physically available on-chip resources. (ii) Portability. Zorua alleviates the necessity of re-tuning an application's resource usage when porting the application across GPU generations. (iii) Performance. By dynamically allocating resources and carefully oversubscribing them when necessary, Zorua improves or retains the performance of applications that are already highly tuned to best utilize th...

---
### GRAPE: Minimizing energy for GPU applications with performance requirements.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783719
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783719
* **Key Words**: Pipelines, Hardware, Graphics processing units, Random access memory, Energy consumption, Control systems, energy conservation, energy consumption, graphics processing units, hardware description languages, minimisation, performance evaluation, power aware computing, GRAPE, GPU applications, performance requirements, resource usage, energy consumption, unconstrained optimization, constrained optimization, GPU energy minimization, warp action, GPGPU-Sim, VHDL, user-specified performance, memory speed, core speed, wavefront action, core usage, hardware control system, externally defined performance constraint, 
* **Abstract**: Many applications have performance requirements (e.g., real-time deadlines or quality-of-service goals) and we can save tremendous energy by tailoring resource usage so the application just meets its performance using the minimal resources. This problem is a classic constrained optimization: the performance goal is the constraint and energy consumption is the objective to be optimized. While several existing hardware approaches solve unconstrained optimizations (i.e., maximizing performance or minimizing energy), we are not aware of a hardware approach that minimizes GPU energy under an externally defined performance constraint. Therefore, we propose GRAPE, a hardware control system for GPUs that coordinates core usage, wavefront/warp action, core speed, and memory speed to deliver user-specified performance while minimizing energy. We implement GRAPE in VHDL (to demonstrate feasibility) and as an extension to GPGPU-Sim (for performance and power measurement). We find that GRAPE can be implemented with very low hardware overhead; however, compared to the no-overhead approach of race-to-idle, GRAPE reduces energy by 9-26% (depending on the performance goal), while meeting performance goals with an average error of 0.75%.

---
### From high-level deep neural models to FPGAs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783720
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783720
* **Key Words**: Field programmable gate arrays, Hardware, Schedules, Weaving, Acceleration, System-on-chip, Computational modeling, compiler generators, data flow graphs, feedforward neural nets, field programmable gate arrays, learning (artificial intelligence), logic design, optimisation, high-level deep neural models, FPGA, deep neural networks, DNN acceleration, compute-intensive learning models, memory footprint, limited on-chip storage, DnnWeaver compiler, synthesizable accelerator generation, high-level specification, Caffe, hand-optimized design templates, macro dataflow graph, optimization algorithm, data reuse maximization, Xilinx Zynq, Altera Stratix V, Altera Arria 10, 
* **Abstract**: Deep Neural Networks (DNNs) are compute-intensive learning models with growing applicability in a wide range of domains. FPGAs are an attractive choice for DNNs since they offer a programmable substrate for acceleration and are becoming available across different market segments. However, obtaining both performance and energy efficiency with FPGAs is a laborious task even for expert hardware designers. Furthermore, the large memory footprint of DNNs, coupled with the FPGAs' limited on-chip storage makes DNN acceleration using FPGAs more challenging. This work tackles these challenges by devising DnnWeaver, a framework that automatically generates a synthesizable accelerator for a given (DNN, FPGA) pair from a high-level specification in Caffe [1]. To achieve large benefits while preserving automation, DNNWEAVER generates accelerators using hand-optimized design templates. First, DnnWeaver translates a given high-level DNN specification to its novel ISA that represents a macro dataflow graph of the DNN. The DnnWeaver compiler is equipped with our optimization algorithm that tiles, schedules, and batches DNN operations to maximize data reuse and best utilize target FPGA's memory and other resources. The final result is a custom synthesizable accelerator that best matches the needs of the DNN while providing high performance and efficiency gains for the target FPGA. We use DnnWeaver to generate accelerators for a set of eight different DNN models and three different FPGAs, Xilinx Zynq, Altera Stratix V, and Altera Arria 10. We use hardware measurements to compare the generated accelerators to both multicore CPUs (ARM Cortex A15 and Xeon E3) and many-core GPUs (Tegra K1, GTX 650Ti, and Tesla K40). In comparison, the generated accelerators deliver superior performance and efficiency without requiring the programmers to participate in the arduous task of hardware design.

---
### vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783721
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783721
* **Key Words**: Memory management, Graphics processing units, Neural networks, Training, Resource management, Backpropagation, Feature extraction, DRAM chips, feedforward neural nets, graphics processing units, learning (artificial intelligence), neural net architecture, parallel processing, performance evaluation, virtualisation, vDNN, virtualized deep neural networks, scalable memory-efficient neural network design, machine learning, memory usage, DRAM capacity, network architecture, parallel processing, runtime memory manager, CPU memory, GPU memory usage, virtualized DNN, AlexNet, OverFeat, GoogLeNet, memory requirements, VGG-16, NVIDIA Titan X GPU card, 
* **Abstract**: The most widely used machine learning frameworks require users to carefully tune their memory usage so that the deep neural network (DNN) fits into the DRAM capacity of a GPU. This restriction hampers a researcher's flexibility to study different machine learning algorithms, forcing them to either use a less desirable network architecture or parallelize the processing across multiple GPUs. We propose a runtime memory manager that virtualizes the memory usage of DNNs such that both GPU and CPU memory can simultaneously be utilized for training larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory usage of AlexNet by up to 89%, OverFeat by 91%, and GoogLeNet by 95%, a significant reduction in memory requirements of DNNs. Similar experiments on VGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the memory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256 (requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card containing 12 GB of memory, with 18% performance loss compared to a hypothetical, oracular GPU with enough memory to hold the entire DNN.

---
### Stripes: Bit-serial deep neural network computing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783722
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783722
* **Key Words**: Neurons, Bandwidth, Parallel processing, Performance evaluation, Three-dimensional displays, Neural networks, Computers, image classification, neural nets, numerical analysis, bit-serial deep neural network computing, hardware accelerator, DNN, execution time scales, numerical representation, image classification, 
* **Abstract**: Motivated by the variance in the numerical precision requirements of Deep Neural Networks (DNNs) [1], [2], Stripes (STR), a hardware accelerator is presented whose execution time scales almost proportionally with the length of the numerical representation used. STR relies on bit-serial compute units and on the parallelism that is naturally present within DNNs to improve performance and energy with no accuracy loss. In addition, STR provides a new degree of adaptivity enabling on-the-fly trade-offs among accuracy, performance, and energy. Experimental measurements over a set of DNNs for image classification show that STR improves performance over a state-of-the-art accelerator [3] from 1.30x to 4.51x and by 1.92x on average with no accuracy loss. STR is 57% more energy efficient than the baseline at a cost of 32% additional area. Additionally, by enabling configurable, per-layer and per-bit precision control, STR allows the user to trade accuracy for further speedup and energy efficiency.

---
### Cambricon-X: An accelerator for sparse neural networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783723
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783723
* **Key Words**: Artificial neural networks, Neurons, Biological neural networks, Computer architecture, Graphics processing units, Architecture, Feature extraction, graphics processing units, neural net architecture, Cambricon-X, sparse neural network accelerator, memory intensive NN, computationally intensive NN, NN accelerators, regular networks, synaptic weight reduction, PE-based architecture, processing elements, indexing module, bandwidth requirement, 
* **Abstract**: Neural networks (NNs) have been demonstrated to be useful in a broad range of applications such as image recognition, automatic translation and advertisement recommendation. State-of-the-art NNs are known to be both computationally and memory intensive, due to the ever-increasing deep structure, i.e., multiple layers with massive neurons and connections (i.e., synapses). Sparse neural networks have emerged as an effective solution to reduce the amount of computation and memory required. Though existing NN accelerators are able to efficiently process dense and regular networks, they cannot benefit from the reduction of synaptic weights. In this paper, we propose a novel accelerator, Cambricon-X, to exploit the sparsity and irregularity of NN models for increased efficiency. The proposed accelerator features a PE-based architecture consisting of multiple Processing Elements (PE). An Indexing Module (IM) efficiently selects and transfers needed neurons to connected PEs with reduced bandwidth requirement, while each PE stores irregular and compressed synapses for local computation in an asynchronous fashion. With 16 PEs, our accelerator is able to achieve at most 544 GOP/s in a small form factor (6.38 mm 2 and 954 mW at 65 nm). Experimental results over a number of representative sparse networks show that our accelerator achieves, on average, 7.23x speedup and 6.43x energy saving against the state-of-the-art NN accelerator.

---
### NEUTRAMS: Neural network transformation and co-design under neuromorphic hardware constraints.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783724
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783724
* **Key Words**: Neurons, Hardware, Artificial neural networks, Neuromorphics, Biological neural networks, Training, Computer architecture, hardware-software codesign, multilayer perceptrons, program processors, recurrent neural nets, software tools, NEUTRAMS toolset, neural network transformation mapping and simulation, neural network co-design, neuromorphic hardware constraints, neuromorphic computing, neuromorphic chip fabrication, neural network transformation algorithm, configurable clock-driven simulator, optimized runtime tool, resource utilization, spiking neural networks, artificial neural networks, convolutional neural networks, multilayer perceptrons, recurrent neural networks, hardware software co-design, network error-rates, 
* **Abstract**: With the recent reincarnations of neuromorphic computing comes the promise of a new computing paradigm, with a focus on the design and fabrication of neuromorphic chips. A key challenge in design, however, is that programming such chips is difficult. This paper proposes a systematic methodology with a set of tools to address this challenge. The proposed toolset is called NEUTRAMS (Neural network Transformation, Mapping and Simulation), and includes three key components: a neural network (NN) transformation algorithm, a configurable clock-driven simulator of neuromorphic chips and an optimized runtime tool that maps NNs onto the target hardware for better resource utilization. To address the challenges of hardware constraints on implementing NN models (such as the maximum fan-in/fan-out of a single neuron, limited precision, and various neuron models), the transformation algorithm divides an existing NN into a set of simple network units and retrains each unit iteratively, to transform the original one into its counterpart under such constraints. It can support both spiking neural networks (SNNs) and traditional artificial neural networks (ANNs), including convolutional neural networks (CNNs) and multilayer perceptrons (MLPs) and recurrent neural networks (RNNs). With the combination of these tools, we have explored the hardware/software co-design space of the correlation between network error-rates and hardware constraints and consumptions. Doing so provides insights which can support the design of future neuromorphic architectures. The usefulness of such a toolset has been demonstrated with two different designs: a real Complementary Metal-Oxide-Semiconductor (CMOS) neuromorphic chip for both SNNs and ANNs and a processing-in-memory architecture design for ANNs.

---
### Fused-layer CNN accelerators.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783725
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783725
* **Key Words**: System-on-chip, Bandwidth, Random access memory, Neural networks, Convolution, Field programmable gate arrays, Data transfer, data flow computing, feedforward neural nets, field programmable gate arrays, graphics processing units, fused-layer CNN accelerators, deep convolutional neural networks, pervasive machine learning tasks, computer vision, iterative CNN layer processing, CNN accelerators, convolutional layers, VGGNet-E network, Xilinx Virtex-7 FPGA, on-chip storage, off-chip feature map data transfer, 
* **Abstract**: Deep convolutional neural networks (CNNs) are rapidly becoming the dominant approach to computer vision and a major component of many other pervasive machine learning tasks, such as speech recognition, natural language processing, and fraud detection. As a result, accelerators for efficiently evaluating CNNs are rapidly growing in popularity. The conventional approaches to designing such CNN accelerators is to focus on creating accelerators to iteratively process the CNN layers. However, by processing each layer to completion, the accelerator designs must use off-chip memory to store intermediate data between layers, because the intermediate data are too large to fit on chip. In this work, we observe that a previously unexplored dimension exists in the design space of CNN accelerators that focuses on the dataflow across convolutional layers. We find that we are able to fuse the processing of multiple CNN layers by modifying the order in which the input data are brought on chip, enabling caching of intermediate data between the evaluation of adjacent CNN layers. We demonstrate the effectiveness of our approach by constructing a fused-layer CNN accelerator for the first five convolutional layers of the VGGNet-E network and comparing it to the state-of-the-art accelerator implemented on a Xilinx Virtex-7 FPGA. We find that, by using 362KB of on-chip storage, our fused-layer accelerator minimizes off-chip feature map data transfer, reducing the total transfer by 95%, from 77MB down to 3.6MB per image.

---
### Continuous shape shifting: Enabling loop co-optimization via near-free dynamic code rewriting.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783726
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783726
* **Key Words**: Runtime environment, Shape, Microarchitecture, Optimization, Jacobian matrices, Servers, cache storage, iterative methods, program compilers, program control structures, rewriting systems, continuous shape shifting, loop co-optimization, near-free dynamic code rewriting, loop interaction space, improved cache locality, cache reusing, cache tiling, cache blocking, runtime environment, microarchitectural environment, datacenter environments, iteration spaces, cache conditions, dynamic rewriting, ShapeShifter, loop iteration space reshaping, near-optimal loop tile configurations, oracle static loop tiling baseline, 
* **Abstract**: The class of optimizations characterized by manipulating a loop's interaction space for improved cache locality and reuse (i.e, cache tiling/blocking/strip mine and interchange) are static optimizations requiring a priori information about the microarchitectural and runtime environment of an application binary. However, particularly in datacenter environments, deployed applications face numerous dynamic environments over their lifetimes. As a result, this class of optimizations can result in sub-optimal performance due to the inability to flexibly adapt iteration spaces as cache conditions change at runtime. This paper introduces continuous shape shifiting, a compilation approach that removes the risks of cache tiling optimizations by dynamically rewriting (and reshaping) deployed, running application code. To realize continuous shape shifting, we present ShapeShifter, a framework for continuous monitoring of co-running applications and their runtime environments to reshape loop iteration spaces and pinpoint near-optimal loop tile configurations. Upon identifying a need for reshaping, a new tiling approach is quickly constructed for the application, new code is dynamically generated and is then seamlessly stitched into the running application with near-zero overhead. Our evaluation on a wide spectrum of runtime scenarios demonstrates that ShapeShifter achieves an average of 10-40% performance improvement (up to 2.4 χ) on real systems depending on the runtime environment compared to an oracle static loop tiling baseline.

---
### CrystalBall: Statically analyzing runtime behavior via deep sequence learning.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783727
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783727
* **Key Words**: Runtime, Measurement, Software, Optimization, Debugging, Testing, Vehicle dynamics, learning (artificial intelligence), program compilers, program debugging, program diagnostics, program testing, sensitivity analysis, source code (software), CrystalBall, runtime behavior static analysis, deep sequence learning, dynamic program behavior, software development lifecycle, software optimization, software debugging, software testing, software security, static dynamic program behavior prediction, input sequences, program execution, compiler intermediate representation, source code, language-independence, area under the receiver operating characteristic curve, AUROC, SPEC CPU2006 benchmarks, 
* **Abstract**: Understanding dynamic program behavior is critical in many stages of the software development lifecycle, for purposes as diverse as optimization, debugging, testing, and security. This paper focuses on the problem of predicting dynamic program behavior statically. We introduce a novel technique to statically identify hot paths that leverages emerging deep learning techniques to take advantage of their ability to learn subtle, complex relationships between sequences of inputs. This approach maps well to the problem of identifying the behavior of sequences of basic blocks in program execution. Our technique is also designed to operate on the compiler's intermediate representation (IR), as opposed to the approaches taken by prior techniques that have focused primarily on source code, giving our approach language-independence. We describe the pitfalls of conventional metrics used for hot path prediction such as accuracy, and motivate the use of Area Under the Receiver Operating Characteristic curve (AUROC). Through a thorough evaluation of our technique on complex applications that include the SPEC CPU2006 benchmarks, we show that our approach achieves an AUROC of 0.85.

---
### Low-cost soft error resilience with unified data verification and fine-grained recovery for acoustic sensor based detection.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783728
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783728
* **Key Words**: Registers, Acoustic sensors, Buffer storage, Detectors, Hardware, Resilience, Acoustics, checkpointing, electronic engineering computing, error detection, program compilers, radiation hardening (electronics), low-cost soft error resilience, unified data verification, fine-grained recovery, acoustic sensor based detection, Turnstile, hardware/software cooperative technique, acoustic sensor based soft error detection, bounded detection latency, compilers, verifiable region, store instruction, register inputs, register state, region boundary, memory checkpoints, register checkpoints, store queue, error detection latency, memory systems, architectural states, memory states, recovery point, unverified entries, checkpointed register values, verified region boundary, program control, Soft Error Resilience, Fine-Grained Recovery, Acoustic Sensor, Compiler, Region Boundary Buffer, 
* **Abstract**: This paper presents Turnstile, a hardware/software cooperative technique for low-cost soft error resilience. Leveraging the recent advance of acoustic sensor based soft error detection, Turnstile achieves guaranteed recovery by taking into account the bounded detection latency. The compiler forms verifiable regions and selectively inserts store instructions to checkpoint their register inputs so that Turnstile can verify the register/memory states with regard to a region boundary in a unified way without expensive register file protection. At runtime, for each region, Turnstile regards any stores (to both memory and register checkpoints) as unverified, and thus holds them in a store queue until the region ends and spends the time of the error detection latency. If no error is detected during the time, the verified stores are merged into memory systems, and registers are checkpointed. When all the stores including checkpointing stores prior to a region boundary are verified, the architectural and memory states with regard to the boundary are verified, thus it can serve as a recovery point. In this way, Turnstile contains the errors within the core without extra memory buffering. When an error is detected, Turnstile invalidates unverified entries in the store queue and restores the checkpointed register values to get the architectural and memory states back to what they were at the most recently verified region boundary. Then, Turnstile simply redirects program control to the verified region boundary and continues execution. The experimental results demonstrate that Turnstile can offer guaranteed soft error recovery with low performance overhead (<;8% on average).

---
### Lazy release consistency for GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783729
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783729
* **Key Words**: Synchronization, Graphics processing units, Coherence, Instruction sets, Copper, Data models, Computational modeling, graph theory, graphics processing units, storage management, lazy release consistency, GPU, heterogeneous-race-free memory model, HRF memory model, Heterogeneous System Architecture Foundation, HSA, OpenCL, atomic memory operations, dynamic data sharing patterns, work-stealing graph analysis applications, DRF memory model, hLRC, DeNovo-like mechanism, synchronization variable ownership, robust performance improvement, graphics processing unit (GPU), memory model, lazy release consistency, scope promotion, scoped synchronization, work stealing, 
* **Abstract**: The heterogeneous-race-free (HRF) memory model has been embraced by the Heterogeneous System Architecture (HSA) Foundation and OpenCL TM because it clearly and precisely defines the behavior of current GPUs. However, compared to the simpler SC for DRF memory model, HRF has two shortcomings. The first is that HRF requires programmers to label atomic memory operations with the correct scope of synchronization. This explicit labeling can save significant coherence overhead when synchronization is local, but it is tedious and error-prone. The second shortcoming is that HRF restricts important dynamic data sharing patterns like work stealing. Prior work on remote-scope promotion (RSP) attempted to resolve the second shortcoming. However, RSP further complicates the memory model and no scalable implementation of RSP has been proposed. For example, we found that the previously proposed RSP implementation actually results in slowdowns of up to 30% on large GPUs, compared to a naïve baseline system that forgoes work stealing and scopes. Meanwhile, DeNovo has been shown to offer efficient synchronization with an SC for DRF memory model, performing on average 21% better than our baseline system, but it introduces additional overheads to maintain ownership of all modified data. To resolve these deficiencies, we propose to adapt lazy release consistency - previously only proposed for homogeneous CPU systems - to a heterogeneous system. Our approach, called hLRC, uses a DeNovo-like mechanism to track ownership of synchronization variables, lazily performing coherence actions only when a synchronization variable changes locations. hLRC allows GPU programmers to use the simpler SC for DRF memory model without tracking ownership for all modified data. Our evaluation shows that lazy release consistency provides robust performance improvement across a set of work-stealing graph analysis applications - 29% on average versus the baseline system.

---
### Improving energy efficiency of DRAM by exploiting half page row access.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783730
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783730
* **Key Words**: Random access memory, Wires, Recycling, Buffer storage, Organizations, Electric breakdown, Mobile computing, DRAM chips, multiprocessing systems, energy efficiency improvement, half page row access, DRAM operations, row activate, row refresh, wordline connections, row buffer, data wire connections, multicore systems, quad-core systems, self-refresh energy, autorefresh energy, refresh scheduling scheme, temperature 85 degC, 
* **Abstract**: DRAM energy is an important component to optimize in modern computing systems. One outstanding source of DRAM energy is the energy to fetch data stored on cells to the row buffer, which occurs during two DRAM operations, row activate and refresh. This work exploits previously proposed half page row access, modifying the wordline connections within a bank to halve the number of cells fetched to the row buffer, to save energy in both cases. To accomplish this, we first change the data wire connections in the sub-array to reduce the cost of row buffer overfetch in multi-core systems which yields a 12% energy savings on average and a slight performance improvement in quad-core systems. We also propose charge recycling refresh, which reuses charge left over from a prior half page refresh to refresh another half page. Our charge recycling scheme is capable of reducing both auto- and self-refresh energy, saving more than 15% of refresh energy at 85°C, and provides even shorter refresh cycle time. Finally, we propose a refresh scheduling scheme that can dynamically adjust the number of charge recycled half pages, which can save up to 30% of refresh energy at 85°C.

---
### OSCAR: Orchestrating STT-RAM cache traffic for heterogeneous CPU-GPU architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783731
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783731
* **Key Words**: Graphics processing units, Multicore processing, Random access memory, Nonvolatile memory, System-on-chip, Central Processing Unit, cache storage, graphics processing units, microprocessor chips, multiprocessing systems, random-access storage, OSCAR, STT-RAM cache traffic, heterogeneous CPU-GPU architectures, shared last-level cache, spin-transfer torque RAM, near-zero leakage power, heterogeneous multicores, shared network bandwidth, network resource allocation, 
* **Abstract**: As we integrate data-parallel GPUs with general-purpose CPUs on a single chip, the enormous cache traffic generated by GPUs will not only exhaust the limited cache capacity, but also severely interfere with CPU requests. Such heterogeneous multicores pose significant challenges to the design of shared last-level cache (LLC). This problem can be mitigated by replacing SRAM LLC with emerging non-volatile memories like Spin-Transfer Torque RAM (STT-RAM), which provides larger cache capacity and near-zero leakage power. However, without careful design, the slow write operations of STT-RAM may offset the capacity benefit, and the system may still suffer from contention in the shared LLC and on-chip interconnects. While there are cache optimization techniques to alleviate such problems, we reveal that the true potential of STT-RAM LLC may still be limited because now that the cache hit rate has been improved by the increased capacity, the on-chip network can become a performance bottleneck. CPU and GPU packets contend with each other for the shared network bandwidth. Moreover, the mixed-criticality read/write packets to STT-RAM add another layer of complexity to the network resource allocation. Therefore, being aware of the disparate latency tolerance of CPU/GPU applications and the asymmetric read/write latency of STT-RAM, we propose OSCAR to Orchestrate STT-RAM Caches traffic for heterogeneous ARchitectures. Specifically, an integration of asynchronous batch scheduling and priority based allocation for on-chip interconnect is proposed to maximize the potential of STT-RAM based LLC. Simulation results on a 28-GPU and 14-CPU system demonstrate an average of 17.4% performance improvement for CPUs, 10.8% performance improvement for GPUs, and 28.9% LLC energy saving compared to SRAM based LLC design.

---
### A unified memory network architecture for in-memory computing in commodity servers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783732
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783732
* **Key Words**: Sockets, Random access memory, Servers, Memory management, Sparks, Fabrics, Big Data, DRAM chips, file servers, graph theory, network-on-chip, performance evaluation, power aware computing, unified memory network architecture, in-memory computing, commodity servers, data-intensive processing, DRAM, main memory system, discrete memory modules, memory pool, hybrid memory cubes, HMC, intermemory network, multihop latency, power consumption, intramemory network, NoC, memory network structure, network switches, distance-aware selective compression, light-weight power-gating algorithm, connected graph, deadlock-free routing, event-driven simulator, Big-Data workloads, average memory access latency reduction, total memory energy saving, 
* **Abstract**: In-memory computing is emerging as a promising paradigm in commodity servers to accelerate data-intensive processing by striving to keep the entire dataset in DRAM. To address the tremendous pressure on the main memory system, discrete memory modules can be networked together to form a memory pool, enabled by recent trends towards richer memory interfaces (e.g. Hybrid Memory Cubes, or HMCs). Such an inter-memory network provides a scalable fabric to expand memory capacity, but still suffers from long multi-hop latency, limited bandwidth, and high power consumption - problems that will continue to exacerbate as the gap between interconnect and transistor performance grows. Moreover, inside each memory module, an intra-memory network (NoC) is typically employed to connect different memory partitions. Without careful design, the back-pressure inside the memory modules can further propagate to the inter-memory network to cause a performance bottleneck. To address these problems, we propose co-optimization of intra- and inter-memory network. First, we re-organize the intra-memory network structure, and provide a smart I/O interface to reuse the intra-memory NoC as the network switches for inter-memory communication, thus forming a unified memory network. Based on this architecture, we further optimize the inter-memory network for both high performance and lower energy, including a distance-aware selective compression scheme to drastically reduce communication burden, and a light-weight power-gating algorithm to turn off under-utilized links while guaranteeing a connected graph and deadlock-free routing. We develop an event-driven simulator to model our proposed architectures. Experiment results based on both synthetic traffic and real big-data workloads show that our unified memory network architecture can achieve 75.1% average memory access latency reduction and 22.1% total memory energy saving.

---
### Contention-based congestion management in large-scale networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783733
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783733
* **Key Words**: Routing, Adaptive systems, Topology, Multiprocessor interconnection, Network topology, Throughput, Robustness, computer network management, telecommunication network routing, telecommunication traffic, contention-based congestion management, large-scale networks, adversarial traffic patterns, load-balance network channels, global adaptive routing, greedy flow, hotspot traffic, supercomputer networks, data center networks, 
* **Abstract**: Global adaptive routing exploits non-minimal paths to improve performance on adversarial traffic patterns and load-balance network channels in large-scale networks. However, most prior work on global adaptive routing have assumed admissible traffic pattern where no endpoint node is oversubscribed. In the presence of a greedy flow or hotspot traffic, we show how exploiting path diversity with global adaptive routing can spread network congestion and degrade performance. When global adaptive routing is combined with congestion management, the two types of congestion - network congestion that occurs within the interconnection network channels and endpoint congestion that occurs from oversubscribed endpoint nodes - are not properly differentiated. As a result, previously proposed congestion management mechanisms that are effective in addressing endpoint congestion are not necessarily effective when global adaptive routing is also used in the network. Thus, we propose a novel, low-cost contention-based congestion management (CBCM) to identify endpoint congestion based on the contention within the intermediate routers and at the endpoint nodes. While contention also occurs for network congestion, the endpoint nodes or the destination determines whether the congestion is endpoint congestion or network congestion. If it is only network congestion, CBCM ignores the network congestion and adaptive routing is allowed to minimize network congestion. However, if endpoint congestion occurs, CBCM throttles the hotspot senders and minimally route the traffic through a separate VC. Our evaluation across different traffic patterns and network sizes demonstrates that our approach is more robust in identifying endpoint congestion in the network while complementing global adaptive routing to avoid network congestion.

---
### Dynamic error mitigation in NoCs using intelligent prediction techniques.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783734
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783734
* **Key Words**: Transistors, Delays, Predictive models, Data models, Prediction algorithms, Error correction codes, decision trees, error correction codes, error detection, error handling, fault tolerance, learning (artificial intelligence), multiprocessing systems, network-on-chip, radiation hardening (electronics), timing, dynamic error mitigation, NoC, intelligent prediction, network-on-chips, communication fabric, multicore systems, process-voltage-temperature variations, timing constraints, fault handling, fault-tolerant schemes, machine learning, decision tree, error correction codes, ECC, relaxed timing transmission, timing errors, static single error correction and double error detection, SECDED, retransmitted packet reduction, 
* **Abstract**: Network-on-chips (NoCs) are quickly becoming the standard communication fabric for multi-core systems. As technology continues to scale down into the nanometer regime, device behavior will become increasingly unreliable due to a combination of aging, soft errors, aggressive transistor design, and process-voltage-temperature variations. Further, stringent timing constraints in NoCs are designed so that data can be pushed faster. The net result is an increase in errors which must be mitigated by the NoC. Typical techniques for handling faults are often reactive as they respond to faults after the error has occurred, making the recovery process inefficient in energy and time. In this paper, we take a different approach wherein we propose to use proactive, fault-tolerant schemes to be employed before the fault affects the system. We propose to utilize machine learning techniques to train a decision tree which can be used to predict faults efficiently in the network. Based on the prediction model, we dynamically mitigate these predicted faults through error correction codes (ECC) and relaxed timing transmission. Our results indicate that, on average, we can accurately predict timing errors 60.6% better than a static single error correction and double error detection (SECDED) technique resulting in an average 26.8% reduction in retransmitted packets, a average net speedup of 3.31 x, and an average energy savings of 60.0% over other designs for real traffic patterns.

---
### Reducing data movement energy via online data clustering and encoding.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783735
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783735
* **Key Words**: Encoding, Energy consumption, Random access memory, Standards, Hamming distance, Receivers, System-on-chip, pattern clustering, storage management chips, data movement energy, online data clustering, data encoding technique, binary representations, 
* **Abstract**: Modern computer systems expend significant amounts of energy on transmitting data over long and highly capacitive interconnects. A promising way of reducing the data movement energy is to design the interconnect such that the transmission of 0s is considerably cheaper than that of 1s. Given such an interconnect with asymmetric transmission costs, data movement energy can be reduced by encoding the transmitted data such that the number of 1s in each transmitted codeword is minimized. This paper presents a new data encoding technique based on online data clustering that exploits this opportunity. The transmitted data blocks are dynamically clustered based on the similarities between their binary representations. Each data block is expressed as the bitwise XOR between one of multiple cluster centers and a residual with a small number of 1s. The data movement energy is minimized by sending the residual along with an identifier that specifies which cluster center to use in decoding the transmitted data. At runtime, the proposed approach continually updates the cluster centers based on the observed data to adapt to phase changes. The proposed technique is compared to three previously proposed energy-efficient data encoding techniques on a set of 14 applications. The results indicate respective energy savings of 5%, 9%, and 12% in DDR4, LPDDR3, and last level cache subsystems as compared to the best existing baseline encoding technique.

---
### Racer: TSO consistency via race detection.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783736
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783736
* **Key Words**: Protocols, Synchronization, Coherence, Semantics, Software, Buffer storage, Proposals, cache storage, program diagnostics, programming language semantics, Racer, TSO consistency, program DRF semantics, SC-for-DRF semantics, self-invalidation/write-through approach, memory consistency model, read-after-write race detection, racing reader cache, coalescing store buffer, store reordering, signature-based mechanism, directory protocol, acquire-release annotations, network traffic reduction, memory-system energy consumption reduction, program data-race-free semantics, 
* **Abstract**: Several recent efforts aim to simplify coherence and its associate costs (e.g., directory size, complexity) in multicores. The bulk of these efforts rely on program data-race-free (DRF) semantics to eliminate explicit invalidations and use self-invalidation instead. While such protocols are simple, they require software cooperation. This is acceptable only for (correct) software that abides by the SC-for-DRF semantics defined in many modern programming language standards (e.g., C++11, Java, the latest C standards) but many are unwilling to trust coherence that relies solely on program semantics for its correctness. To address this important issue, this work proposes Racer, an efficient self-invalidation/write-through approach that guarantees the memory consistency model of the most common family of processors (TSO-x86), and at the same time maintains the relaxed-ordering advantages of SC-for-DRF protocols. Lacking a directory and explicit invalidations, Racer achieves this by detecting read-after-write races and causing self-invalidation on the racing reader's cache. Racer also uses a coalescing store buffer (at the L1 level) that allows coalescing and reordering of stores but upon detecting a race, delays the racing read until all its stores appear in order to the read. Race detection is performed using an efficient signature-based mechanism at the level of the shared cache. Racer performs significantly better than a traditional non-scalable directory-based protocol that does not allow reordering at the protocol level (14.2% in time and 26.4% in energy), a directory protocol for TSO (1.9% in time and 15.5% in energy), and state-of-the-art SC-for-DRF protocol that relies on acquire-release annotations in the programs (6.7% in time and 9.5% in energy). Racer self-invalidates less than program-level annotations as it only enforces ordering on dynamically detected races and provides significant reductions in network traffic and memory-system energy consumption.

---
### Exploiting semantic commutativity in hardware speculation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783737
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783737
* **Key Words**: Semantics, Software, Coherence, Protocols, Hardware, Radiation detectors, Concurrent computing, concurrency control, parallel programming, protocols, storage management, semantic commutativity, hardware speculative execution schemes, software speculation scheme, concurrent operations, run-time overheads, COMMTM, HTM, coherence protocol, user-defined commutative operations, top-K insertions, transactional memory, 
* **Abstract**: Hardware speculative execution schemes such as hardware transactional memory (HTM) enjoy low run-time overheads but suffer from limited concurrency because they rely on reads and writes to detect conflicts. By contrast, software speculation schemes can exploit semantic knowledge of concurrent operations to reduce conflicts. In particular, they often exploit that many operations on shared data, like insertions into sets, are semantically commutative: they produce semantically equivalent results when reordered. However, software techniques often incur unacceptable run-time overheads. To solve this dichotomy, we present COMMTM, an HTM that exploits semantic commutativity. CommTM extends the coherence protocol and conflict detection scheme to support user-defined commutative operations. Multiple cores can perform commutative operations to the same data concurrently and without conflicts. CommTM preserves transactional guarantees and can be applied to arbitrary HTMs. CommTM scales on many operations that serialize in conventional HTMs, like set insertions, reference counting, and top-K insertions, and retains the low overhead of HTMs. As a result, at 128 cores, CommTM outperforms a conventional eager-lazy HTM by up to 3.4 χ and reduces or eliminates aborts.

---
### CANDY: Enabling coherent DRAM caches for multi-node systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783738
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783738
* **Key Words**: Coherence, Protocols, Program processors, Bandwidth, Buffer storage, DRAM chips, cache storage, DRAM chips, SRAM chips, coherent DRAM caches, multinode systems, memory-side cache, MSC, small on-die caches, inter-node network latency, giga-scale CDC, read-write shared data, DRAM-cache coherence buffer, on-die coherence directory, sharing-aware bypass, 4-node system, CANDY, 64MB SRAM directory per node, zero cache latency, 
* **Abstract**: This paper investigates the use of DRAM caches for multi-node systems. Current systems architect the DRAM cache as Memory-Side Cache (MSC), restricting the DRAM cache to cache only the local data, and relying on only the small on-die caches for the remote data. As MSC keeps only the local data, it is implicitly coherent and obviates the need of any coherence support. Unfortunately, as accessing the data in the remote node incurs a significant inter-node network latency, MSC suffers from such latency overhead on every on-die cache miss to the remote data. A desirable alternative is to allow the DRAM cache to cache both the local and the remote data. However, as data blocks can be cached in multiple DRAM caches, this design requires coherence support for DRAM caches to ensure correctness, and is termed Coherent DRAM Cache (CDC). We identify two key challenges in architecting giga-scale CDC. First, the coherence directory can be as large as few tens of MB. Second, cache misses to the read-write shared data in CDC cause longer delay due to the need to access the DRAM cache. To address both problems, this paper proposes CANDY, a low-cost and scalable solution that consists of two techniques for these challenges. First, CANDY places the coherence directory in 3D DRAM to avoid SRAM storage overhead, and re-purposes the existing on-die coherence directory as a DRAM-cache Coherence Buffer to cache recently accessed directory entries. Second, we propose Sharing-Aware Bypass, which dynamically detects the read-write shared data at run-time and enforces such data to bypass the DRAM cache. Our experiment on a 4-node system with 1GB DRAM cache per node shows that CANDY outperforms MSC by 25%, while incurring a negligible overhead of 8KB per node. CANDY is within 5% of an impractical system that has a 64MB SRAM directory per node, and zero cache latency to access the read-write shared data.

---
### C3D: Mitigating the NUMA bottleneck via coherent DRAM caches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783739
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783739
* **Key Words**: Random access memory, Sockets, Coherence, Servers, Bandwidth, System-on-chip, Protocols, DRAM chips, memory protocols, microprocessor chips, parallel processing, shared memory systems, C3D, NUMA bottleneck mitigation, clean coherent DRAM caches, massive datasets, high-performance computing, scale-out computing, enterprise computing, memory capacities-per-node, memory demands, performance-per-unit cost maximization, commodity HPC, server nodes, multisocket shared memory NUMA organizations, remote memory accesses, intersocket communication latencies, coherence protocols, non inclusive on-chip directory, quad-socket system, 
* **Abstract**: Massive datasets prevalent in scale-out, enterprise, and high-performance computing are driving a trend toward ever-larger memory capacities per node. To satisfy the memory demands and maximize performance per unit cost, today's commodity HPC and server nodes tend to feature multi-socket shared memory NUMA organizations. An important problem in these designs is the high latency of accessing memory on a remote socket that results in degraded performance in workloads with large shared data working sets. This work shows that emerging DRAM caches can help mitigate the NUMA bottleneck by filtering up to 98% of remote memory accesses. To be effective, these DRAM caches must be private to each socket to allow caching of remote memory, which comes with the challenge of ensuring coherence across multiple sockets and GBs of DRAM cache capacity. Moreover, the high access latency of DRAM caches, combined with high inter-socket communication latencies, can make hits to remote DRAM caches slower than main memory accesses. These features challenge existing coherence protocols optimized for on-chip caches with fast hits and modest storage capacity. Our solution to these challenges relies on two insights. First, keeping DRAM caches clean avoids the need to ever access a remote DRAM cache on a read. Second, a non-inclusive on-chip directory that avoids tracking blocks in the DRAM cache enables a light-weight protocol for guaranteeing coherence without the staggering directory costs. Our design, called Clean Coherent DRAM Caches (C 3 D), leverages these insights to improve performance by 6.4-50.7% in a quad-socket system versus a baseline without DRAM caches.

---
### Quantifying and improving the efficiency of hardware-based mobile malware detectors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783740
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783740
* **Key Words**: Malware, Resists, Detectors, Mobile communication, Payloads, Androids, Humanoid robots, discrete wavelet transforms, invasive software, mobile computing, trusted computing, unsupervised learning, hardware-based mobile malware detectors, trustworthy systems, malicious adversaries, battery-efficient line of defense, white-box methodology, malware suite, HMD operating range, malware actions, discrete-wavelet transform based unsupervised HMD, AUC metric, supervised HMD, Sherlock diverse malware dataset, ad-hoc malware subsets, 
* **Abstract**: Hardware-based malware detectors (HMDs) are a key emerging technology to build trustworthy systems, especially mobile platforms. Quantifying the efficacy of HMDs against malicious adversaries is thus an important problem. The challenge lies in that real-world malware adapts to defenses, evades being run in experimental settings, and hides behind benign applications. Thus, realizing the potential of HMDs as a small and battery-efficient line of defense requires a rigorous foundation for evaluating HMDs. We introduce Sherlock - a white-box methodology that quantifies an HMD's ability to detect malware and identify the reason why. Sherlock first deconstructs malware into atomic, orthogonal actions to synthesize a diverse malware suite. Sherlock then drives both malware and benign programs with real user-inputs, and compares their executions to determine an HMD's operating range, i.e., the smallest malware actions an HMD can detect. We show three case studies using Sherlock to not only quantify HMDs' operating ranges but design better detectors. First, using information about concrete malware actions, we build a discrete-wavelet transform based unsupervised HMD that outperforms prior work based on power transforms by 24.7% (AUC metric). Second, training a supervised HMD using Sherlock's diverse malware dataset yields 12.5% better HMDs than past approaches that train on ad-hoc subsets of malware. Finally, Sherlock shows why a malware instance is detectable. This yields a surprising new result - obfuscation techniques used by malware to evade static analyses makes them more detectable using HMDs.

---
### PoisonIvy: Safe speculation for secure memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783741
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783741
* **Key Words**: Radiation detectors, Program processors, Encryption, Toxicology, Metadata, cryptography, PoisonIvy, safe speculation, secure memory, encryption, integrity trees, physical attacks, integrity verification latency, address-based side-channels, memory intensive workloads, 
* **Abstract**: Encryption and integrity trees guard against physical attacks, but harm performance. Prior academic work has speculated around the latency of integrity verification, but has done so in an insecure manner. No industrial implementations of secure processors have included speculation. This work presents PoisonIvy, a mechanism which speculatively uses data before its integrity has been verified while preserving security and closing address-based side-channels. PoisonIvy reduces performance overheads from 40% to 20% for memory intensive workloads and down to 1.8%, on average.

---
### ReplayConfusion: Detecting cache-based covert channel attacks using record and replay.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783742
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783742
* **Key Words**: Trojan horses, Receivers, Hardware, Security, Protocols, Timing, Indexes, cache storage, cryptographic protocols, invasive software, cache-based covert channel attacks, trojan, spy process, malicious accesses, ReplayConfusion, record and deterministic replay, RnR, program execution, cache miss rate timelines, 
* **Abstract**: Cache-based covert channel attacks use highly-tuned shared-cache conflict misses to pass information from a trojan to a spy process. Detecting such attacks is very challenging. State of the art detection mechanisms do not consider the general characteristics of such attacks and, instead, focus on specific communication protocols. As a result, they fail to detect attacks using different protocols and, hence, have limited coverage. In this paper, we make the following observation about these attacks: not only are the malicious accesses highly tuned to the mapping of addresses to the caches; they also follow a distinctive cadence as bits are being received. Changing the mapping of addresses to the caches substantially disrupts the conflict miss patterns, but retains the cadence. This is in contrast to benign programs. Based on this observation, we propose a novel, high-coverage approach to detect cache-based covert channel attacks. It is called ReplayConfusion, and is based on Record and deterministic Replay (RnR). After a program's execution is recorded, it is deterministically replayed using a different mapping of addresses to the caches. We then analyze the difference between the cache miss rate timelines of the two runs. If the difference function is both sizable and exhibits a periodic pattern, it indicates that there is an attack. This paper also introduces a new taxonomy of cache-based covert channel attacks, and shows that ReplayConfusion uncovers examples from all the categories. Finally, ReplayConfusion only needs simple hardware.

---
### Jump over ASLR: Attacking branch predictors to bypass ASLR.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783743
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783743
* **Key Words**: Kernel, Process control, Hardware, Layout, Timing, Reliability, buffer storage, cryptography, Linux, Linux, Haswell processor, kernel ASLR recovery, attacker code, kernel execution, user-level victim process, branch instructions, BTB collisions, branch target buffer, side-channel attack, user-level ASLR, control flow, code objects, virtual memory, key program segments, address space layout randomization, attacking branch predictors, Address Space Layout Randomization, Bypass, Side Channel, Timing Channel, Timing Attacks, Kernel Vulnerabilities, Exploit Mitigation, 
* **Abstract**: Address Space Layout Randomization (ASLR) is a widely-used technique that protects systems against a range of attacks. ASLR works by randomizing the offset of key program segments in virtual memory, making it difficult for an attacker to derive the addresses of specific code objects and consequently redirect the control flow to this code. In this paper, we develop an attack to derive kernel and user-level ASLR offset using a side-channel attack on the branch target buffer (BTB). Our attack exploits the observation that an adversary can create BTB collisions between the branch instructions of the attacker process and either the user-level victim process or on the kernel executing on its behalf. These collisions, in turn, can impact the timing of the attacker's code, allowing the attacker to identify the locations of known branch instructions in the address space of the victim process or the kernel. We demonstrate that our attack can reliably recover kernel ASLR in about 60 milliseconds when performed on a real Haswell processor running a recent version of Linux. Finally, we describe several possible protection mechanisms, both in software and in hardware.

---
### Concise loads and stores: The case for an asymmetric compute-memory architecture for approximation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783744
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783744
* **Key Words**: Hardware, Computer architecture, Bandwidth, Data mining, Approximate computing, Buildings, Redundancy, approximation theory, data mining, learning (artificial intelligence), memory architecture, asymmetric compute-memory architecture, cache capacity, memory bandwidth, machine learning, numerical analysis, data mining, approximate computing technique, ACME, 
* **Abstract**: Cache capacity and memory bandwidth play critical roles in application performance, particularly for data-intensive applications from domains that include machine learning, numerical analysis, and data mining. Many of these applications are also tolerant to imprecise inputs and have loose constraints on the quality of output, making them ideal candidates for approximate computing. This paper introduces a novel approximate computing technique that decouples the format of data in the memory hierarchy from the format of data in the compute subsystem to significantly reduce the cost of storing and moving bits throughout the memory hierarchy and improve application performance. This asymmetric compute-memory extension to conventional architectures, ACME, adds two new instruction classes to the ISA - load-concise and store-concise - along with three small functional units to the micro-architecture to support these instructions. ACME does not affect exact execution of applications and comes into play only when concise memory operations are used. Through detailed experimentation we find that ACME is very effective at trading result accuracy for improved application performance. Our results show that ACME achieves a 1.3x speedup (up to 1.8x) while maintaining 99% accuracy, or a 1.1x speedup while maintaining 99.999% accuracy. Moreover, our approach incurs negligible area and power overheads, adding just 0.005% area and 0.1% power to a conventional modern architecture.

---
### Approxilyzer: Towards a systematic framework for instruction-level approximate computing and its application to hardware resiliency.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783745
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783745
* **Key Words**: Measurement, Approximate computing, Registers, Degradation, Hardware, Software, Computational modeling, approximation theory, error statistics, software quality, approxilyzer, instruction-level approximate computing, hardware resiliency, single-bit error, general-purpose programs, 
* **Abstract**: Approximate computing environments trade off computational accuracy for improvements in performance, energy, and resiliency cost. For widespread adoption of approximate computing, a fundamental requirement is to understand how perturbations to a computation affect the outcome of the execution in terms of its output quality. This paper presents a framework for approximate computing, called Approxilyzer, that quantifies the quality impact of a single-bit error in all dynamic instructions of an execution with high accuracy (95% on average). We demonstrate two uses of Approxilyzer. First, we show how Approxilyzer can be used to quantitatively tune output quality vs. resiliency vs. overhead to enable ultra-low cost resiliency solutions (with a single bit error model). For example, we show that Approxilyzer determines that a very small loss in output quality (1%) can yield large resiliency overhead reduction (up to 55%) for 99% resiliency coverage. Second, we show how Approxilyzer can be used to provide a first-order estimate of the approximation potential of general-purpose programs. It does so in an automated way while requiring minimal user input and no program modifications. This enables programmers or other tools to focus on the promising subset of approximable instructions for further analysis.

---
### The Bunker Cache for spatio-value approximation.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783746
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783746
* **Key Words**: Signal to noise ratio, Cache storage, Hardware, Indexes, Memory management, Complexity theory, cache storage, data structures, energy conservation, Bunker Cache, spatio-value approximation, data moving, data storage, computer architects, data handling, spatio-value similarity, data elements, spatial regularity, real-world application data values, data structures, cache storage location, memory address, energy savings, reduced off-chip memory accesses, reduced cache misses, cache storage requirements, cache indexing hardware, commodity systems, 
* **Abstract**: The cost of moving and storing data is still a fundamental concern for computer architects. Inefficient handling of data can be attributed to conventional architectures being oblivious to the nature of the values that these data bits carry. We observe the phenomenon of spatio-value similarity, where data elements that are approximately similar in value exhibit spatial regularity in memory. This is inherent to 1) the data values of real-world applications, and 2) the way we store data structures in memory. We propose the Bunker Cache, a design that maps similar data to the same cache storage location based solely on their memory address, sacrificing some application quality loss for greater efficiency. The Bunker Cache enables performance gains (ranging from 1.08x to 1.19x) via reduced cache misses and energy savings (ranging from 1.18x to 1.39x) via reduced off-chip memory accesses and lower cache storage requirements. The Bunker Cache requires only modest changes to cache indexing hardware, integrating easily into commodity systems.

---
### HARE: Hardware accelerator for regular expressions.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783747
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783747
* **Key Words**: Automata, Hardware, Pattern matching, Bandwidth, Pipelines, Software, Business, application specific integrated circuits, field programmable gate arrays, storage management, text analysis, hardware accelerator for regular expressions, text data rapid processing, software-based tools, I/O, memory bandwidth, in-memory logs, stall-free hardware pipeline, 1GHz 32-character-wide HARE design, ASIC implementation, scaled-down FPGA proof-of-concept, 4-wide parallelism, regular expression matching, text processing, finite automata, 
* **Abstract**: Rapidly processing text data is critical for many technical and business applications. Traditional software-based tools for processing large text corpora use memory bandwidth inefficiently due to software overheads and thus fall far short of peak scan rates possible on modern memory systems. Prior hardware designs generally target I/O rather than memory bandwidth. In this paper, we present HARE, a hardware accelerator for matching regular expressions against large in-memory logs. HARE comprises a stall-free hardware pipeline that scans input data at a fixed rate, examining multiple characters from a single input stream in parallel in a single accelerator clock cycle. We describe a 1GHz 32-character-wide HARE design targeting ASIC implementation that processes data at 32 GB/s - matching modern memory bandwidths. This ASIC design outperforms software solutions by as much as two orders of magnitude. We further demonstrate a scaled-down FPGA proof-of-concept that operates at 100MHz with 4-wide parallelism (400 MB/s). Even at this reduced rate, the prototype outperforms grep by 1.5-20x on commonly used regular expressions.

---
### The microarchitecture of a real-time robot motion planning accelerator.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783748
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783748
* **Key Words**: Planning, Collision avoidance, Robot kinematics, Real-time systems, Program processors, Acceleration, field programmable gate arrays, hardware-software codesign, mobile robots, parallel processing, path planning, power consumption, real-time robot motion planning accelerator microarchitecture, hardware accelerator, FPGA, power consumption, hardware/software co-design, collision data precomputation, parallelism, 
* **Abstract**: We have developed a hardware accelerator for motion planning, a critical operation in robotics. In this paper, we present the microarchitecture of our accelerator and describe a prototype implementation on an FPGA. We experimentally show that the accelerator improves performance by three orders of magnitude and improves power consumption by more than one order of magnitude. These gains are achieved through careful hardware/software co-design. We modify conventional motion planning algorithms to aggressively precompute collision data, as well as implement a microarchitecture that leverages the parallelism present in the problem.

---
### Efficient data supply for hardware accelerators with prefetching and access/execute decoupling.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783749
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783749
* **Key Words**: Prefetching, Hardware, Pipelines, Manuals, Computer architecture, Schedules, Engines, computer architecture, high level synthesis, logic design, program slicing, storage management, system-on-chip, data supply, architecture framework, hardware accelerators design, memory latency, data transfer, external memory, automated program analysis, high-level synthesis tools, accelerator memory access, hardware prefetching, data preloading, decoupled access/execute architecture, performance improvement, HLS-generated accelerators, energy consumption reduction, program slicing, 
* **Abstract**: This paper presents an architecture framework to easily design hardware accelerators that can effectively tolerate long and variable memory latency using prefetching and access/execute decoupling. Hardware accelerators are becoming increasingly popular in modern computing systems as a promising approach to achieve higher performance and energy efficiency when technology scaling is slowing down. However, today's high-performance accelerators require significant manual efforts to design, in large part due to the need to carefully orchestrate data transfers between external memory and an accelerator. Instead, the proposed framework utilizes automated program analysis along with High-Level Synthesis (HLS) tools to enable prefetching and access/execute decoupling with minimal manual efforts. The framework adds tags to accelerator memory accesses so that hardware prefetching can effectively preload data for accesses with regular patterns. To handle irregular memory accesses, the framework generates an accelerator with decoupled access/execute architecture using program slicing. Experimental results show that the proposed optimizations can significantly improve performance of HLS-generated accelerators (average speedup of 2.28x across eight accelerators) and often reduce energy consumption (average of 15%).

---
### An ultra low-power hardware accelerator for automatic speech recognition.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783750
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783750
* **Key Words**: Viterbi algorithm, Speech recognition, Speech, Hardware, Acoustics, Graphics processing units, Acoustic beams, microprocessor chips, parallel architectures, power aware computing, power consumption, search problems, speech recognition, storage management, ultra low-power hardware accelerator, automatic speech recognition, mobile segment, mobile devices, power consumption, ASR systems, large-vocabulary speaker-independent continuous speech recognition, Viterbi search algorithm, memory subsystem, prefetching, memory latency, memory accesses, off-chip memory accesses, CPU, highly optimized CUDA implementation, high-end Geforce GTX 980 GPU, 
* **Abstract**: Automatic Speech Recognition (ASR) is becoming increasingly ubiquitous, especially in the mobile segment. Fast and accurate ASR comes at a high energy cost which is not affordable for the tiny power budget of mobile devices. Hardware acceleration can reduce power consumption of ASR systems, while delivering high-performance. In this paper, we present an accelerator for large-vocabulary, speaker-independent, continuous speech recognition. It focuses on the Viterbi search algorithm, that represents the main bottleneck in an ASR system. The proposed design includes innovative techniques to improve the memory subsystem, since memory is identified as the main bottleneck for performance and power in the design of these accelerators. We propose a prefetching scheme tailored to the needs of an ASR system that hides main memory latency for a large fraction of the memory accesses with a negligible impact on area. In addition, we introduce a novel bandwidth saving technique that removes 20% of the off-chip memory accesses issued during the Viterbi search. The proposed design outperforms software implementations running on the CPU by orders of magnitude and achieves 1.7x speedup over a highly optimized CUDA implementation running on a high-end Geforce GTX 980 GPU, while reducing by two orders of magnitude (287x) the energy required to convert the speech into text.

---
### Co-designing accelerators and SoC interfaces using gem5-Aladdin.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783751
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783751
* **Key Words**: Hardware, Acceleration, Engines, Space exploration, Memory management, Random access memory, Microarchitecture, integrated circuit design, power aware computing, system-on-chip, codesigning accelerators, SoC interfaces, gem5-Aladdin, power-efficient, high-performance computing, hardware accelerators, Systems on Chip, accelerator microarchitecture codesign, data movement, total accelerator runtime, SoC simulator, energy delay product, EDP, accelerator microarchitecture, 
* **Abstract**: Increasing demand for power-efficient, high-performance computing has spurred a growing number and diversity of hardware accelerators in mobile and server Systems on Chip (SoCs). This paper makes the case that the co-design of the accelerator microarchitecture with the system in which it belongs is critical to balanced, efficient accelerator microarchitectures. We find that data movement and coherence management for accelerators are significant yet often unaccounted components of total accelerator runtime, resulting in misleading performance predictions and inefficient accelerator designs. To explore the design space of accelerator-system co-design, we develop gem5-Aladdin, an SoC simulator that captures dynamic interactions between accelerators and the SoC platform, and validate it to within 6% against real hardware. Our co-design studies show that the optimal energy-delay-product (EDP) of an accelerator microarchitecture can improve by up to 7.4× when system-level effects are considered compared to optimizing accelerators in isolation.

---
### Chainsaw: Von-neumann accelerators to leverage fused instruction chains.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783752
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783752
* **Key Words**: Registers, Hardware, Parallel processing, Out of order, Pipelines, Computer architecture, Fabrics, data flow computing, graphics processing units, processor scheduling, program compilers, Von-Neumann accelerators, central tenet, program execution, behavior-specialized architectures, microarchitecture, acceleratable code region, dataflow-based accelerators, instruction parallelism, compiler fused instruction sequences, producer-consumer locality, multilane architecture, LLVM-based compiler prototype, 8-lane Chainsaw, dataflow architecture, energy consumption, 4-way OOO processor, 
* **Abstract**: A central tenet behind accelerators is to partition a program execution into regions with different behavior (e.g., SIMD, Irregular, Compute-Intensive) and then use behavior-specialized architectures [1] for each region. It is unclear whether the gains in efficiency arise from recognizing that a simpler microarchitecture is sufficient for the acceleratable code region or the actual microarchitecture, or a combination of both. Many proposals [2], [3] seem to choose dataflow-based accelerators which encounters challenges with fabric utilization and static power when the available instruction parallelism is below the peak operation parallelism available [4]. In this paper, we develop, Chainsaw, a Von-Neumann based accelerator and demonstrate that many of the fundamental overheads (e.g., fetch-decode) can be amortized by adopting the appropriate instruction abstraction. The key insight is the notion of chains, which are compiler fused sequences of instructions. chains adapt to different acceleration behaviors by varying the length of the chains and the types of instructions that are fused into a chain. Chains convey the producer-consumer locality between dependent instructions, which the Chainsaw architecture then captures by temporally scheduling such operations on the same execution unit and uses pipeline registers to forward the values between dependent operations. Chainsaw is a generic multi-lane architecture (4-stage pipeline per lane) and does not require any specialized compound function units; it can be reloaded enabling it to accelerate multiple program paths. We have developed a complete LLVM-based compiler prototype and simulation infrastructure and demonstrated that a 8-lane Chainsaw is within 73% of the performance of an ideal dataflow architecture, while reducing the energy consumption by 45% compared to a 4-way OOO processor.

---
### Chameleon: Versatile and practical near-DRAM acceleration architecture for large memory systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783753
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783753
* **Key Words**: Random access memory, Pins, Bandwidth, Memory management, Data transfer, Servers, Standards, DRAM chips, graphics processing units, Chameleon, near-DRAM acceleration architecture, large memory systems, computer system performance, memory channel bandwidth, package power constraints, package pin constraints, NDA architectures, 3D-stacking technology, 2.5D-stacking technology, HBM-HMC-like DRAM devices, data buffers, load-reduced DIMM, LRDIMM, near-DRAM accelerators, geo-mean performance, 
* **Abstract**: The performance of computer systems is often limited by the bandwidth of their memory channels, but further increasing the bandwidth is challenging under the stringent pin and power constraints of packages. To further increase performance under these constraints, various near-DRAM acceleration (NDA) architectures, which tightly integrate accelerators with DRAM devices using 3D/2.5D-stacking technology, have been proposed. However, they have not prevailed yet because they often rely on expensive HBM/HMC-like DRAM devices which also suffer from limited capacity, whereas the scalability of memory capacity is critical for some computing segments such as servers. In this paper, we first demonstrate that data buffers in a load-reduced DIMM (LRDIMM), which is originally developed to support large memory systems for servers, are supreme places to integrate near-DRAM accelerators. Second, we propose Chameleon, an NDA architecture that can be realized without relying on 3D/2.5D-stacking technology and seamlessly integrated with large memory systems for servers. Third, we explore three microarchitectures that abate constraints imposed by taking LRDIMM architecture for NDA. Our experiment demonstrates that a Chameleon-based system can offer 2.13 χ higher geo-mean performance while consuming 34% lower geo-mean data transfer energy than a system that integrates the same accelerator logic within the processor.

---
### A patch memory system for image processing and computer vision.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783754
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783754
* **Key Words**: Two dimensional displays, Tensile stress, Three-dimensional displays, IP networks, Computer architecture, Image processing, Prefetching, cache storage, computer vision, energy conservation, mobile computing, storage management, patch memory system, image processing, computer vision, mobile systems, energy efficiency, 1D memory access, multidimensional locality characteristics, PMEM, image data caching, image data prefetching, 
* **Abstract**: From self-driving cars to high dynamic range (HDR) imaging, the demand for image-based applications is growing quickly. In mobile systems, these applications place particular strain on performance and energy efficiency. As traditional memory systems are optimized for 1D memory access, they are unable to efficiently exploit the multi-dimensional locality characteristics of image-based applications which often operate on sub-regions of 2D and 3D image data. We have developed a new Patch Memory System (PMEM) tailored to application domains that process 2D and 3D data streams. PMEM supports efficient multidimensional addressing, automatic handling of image boundaries, and efficient caching and prefetching of image data. In addition to an optimized cache, PMEM includes hardware for offloading structured address calculations from processing units. We improve average energy-delay by 26% compared to EVA, a memory system for computer vision applications. Compared to a traditional cache, our results show that PMEM can reduce processor energy by 34% for a selection of CV and IP applications, leading to system performance improvement of up to 32% and energy-delay product improvement of 48-86% on the applications in this study.

---
### Evaluating programmable architectures for imaging and vision applications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783755
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783755
* **Key Words**: Computer architecture, Kernel, Image processing, Random access memory, Shift registers, Imaging, Hardware, computer vision, field buses, field programmable gate arrays, network routing, parallel processing, reconfigurable architectures, SRAM chips, programmable architecture evaluation, computational imaging, computer vision, image signal processors, flexible ISP architectures, Darkroom image processing language, programmable-in-time class, programmable-in-space class, coarse grain reconfigurable array architectures, CGRA, register file partitioning, SIMD, pipelined wires, line buffer variations, energy efficiency, compute density, FPGA, area efficiency, bus based routing, 
* **Abstract**: Algorithms for computational imaging and computer vision are rapidly evolving, and hardware must follow suit: the next generation of image signal processors (ISPs) must be “programmable” to support new algorithms created with high-level frameworks. In this work, we compare flexible ISP architectures, using applications written in the Darkroom image processing language. We target two fundamental architecture classes: programmable in time, as represented by SIMD, and programmable in space, as typified by coarse grain reconfigurable array architectures (CGRA). We consider several optimizations on these two base architectures, such as register file partitioning for SIMD, bus based routing and pipelined wires for CGRA, and line buffer variations. After these optimizations on average, CGRA provides 1.6x better energy efficiency and 1.4x better compute density versus a SIMD solution, and 1.4x the energy efficiency and 3.1x the compute density of an FPGA. However the cost of providing general programmability is still high: compared to an ASIC, CGRA has 6x worse energy and area efficiency, and this ratio would be roughly 10x if memory dominated applications were excluded.

---
### Redefining QoS and customizing the power management policy to satisfy individual mobile users.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783756
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783756
* **Key Words**: Quality of service, Mobile communication, Batteries, Androids, Humanoid robots, Mobile handsets, Color, human factors, mobile computing, mobile handsets, power aware computing, quality of service, power management policy customization, mobile user satisfaction, mobile devices, mobile platform, user-specific optimal QoS, QoS measurement, online personality-guided user satisfaction prediction model, 
* **Abstract**: Delivering an excellent use experience to the customers is the top challenge faced by today's mobile device designers and producers. There have been multiple studies on achieving the good trade-offs between QoS and energy to enhance the user experience, however, they generally lack a comprehensive and accurate understanding of QoS, and ignore the fact that each individual user has his/her own preference between QoS and energy. In this study, we overcome these two drawbacks and propose a customized power management policy that dynamically configures the mobile platform to achieve the user-specific optimal QoS and energy trade-offs and hence, satisfy each individual mobile user. We first introduce a novel and comprehensive definition of QoS, and propose the accurate QoS measurement and management methodologies. We then observe that user's personality greatly determines his/her preferences between QoS and energy, and propose an online personality-guided user satisfaction prediction model based on the QoS and energy, guided by the user personality inferred from his/her device usage history. Our validation proves our model can achieve very high prediction accuracy. Finally, we propose our customized power management policy based on the prediction model for individual users. The experiment results show that our technique can improve the user experience by around 36% compared with the state-of-the-art power management policies.

---
### Snatch: Opportunistically reassigning power allocation between processor and memory in 3D stacks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783757
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783757
* **Key Words**: Pins, System-on-chip, Three-dimensional displays, Voltage control, Resource management, Multicore processing, microprocessor chips, multiprocessing systems, performance evaluation, power aware computing, voltage control, Snatch, 3D stacks, processor-memory power allocation, chip package, pin count, 3D processor-memory designs, power ground pins, P-G pins, power delivery networks, PDN, bidirectional on-chip voltage regulator, memory power requirements, processor power requirements, 8-core multicore simulations, memory dies, compute-intensive codes, turbo-boosting, memory-intensive codes, 
* **Abstract**: The pin count largely determines the cost of a chip package, which is often comparable to the cost of a die. In 3D processor-memory designs, power and ground (P/G) pins can account for the majority of the pins. This is because packages include separate pins for the disjoint processor and memory power delivery networks (PDNs). Supporting separate PDNs and P/G pins for processor and memory is inefficient, as each set has to be provisioned for the worst-case power delivery requirements. In this paper, we propose to reduce the number of P/G pins of both processor and memory in a 3D design, and dynamically and opportunistically divert some power between the two PDNs on demand. To perform the power transfer, we use a small bidirectional on-chip voltage regulator that connects the two PDNs. Our concept, called Snatch, is effective. It allows the computer to execute code sections with high processor or memory power requirements without having to throttle performance. We evaluate Snatch with simulations of an 8-core multicore stacked with two memory dies. In a set of compute-intensive codes, the processor snatches memory power for 30% of the time on average, speeding-up the codes by up to 23% over advanced turbo-boosting; in memory-intensive codes, the memory snatches processor power. Alternatively, Snatch can reduce the package cost by about 30%.

---
### Ti-states: Processor power management in the temperature inversion region.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783758
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783758
* **Key Words**: Temperature measurement, Temperature sensors, Temperature control, Voltage measurement, Timing, Temperature, Graphics processing units, field effect transistors, microprocessor chips, processor power management, temperature inversion region, transistor-level effect, AMD A10-8700P processor, power management technique, FD-SOI, FinFET, timing margin, temperature inversion, power management, reliability, technology scaling, 
* **Abstract**: Temperature inversion is a transistor-level effect that can improve performance when temperature increases. It has largely been ignored in the past because it does not occur in the typical operating region of a processor, but temperature inversion is becoming increasing important in current and future technologies. In this paper, we study temperature inversion's implications on architecture design, and power and performance management. We present the first public comprehensive measurement-based analysis on the effects of temperature inversion on a real processor, using the AMD A10-8700P processor as our system under test. We show that the extra timing margin introduced by temperature inversion can provide more than 5% V dd reduction benefit, and this improvement increases to more than 8% when operating in the near-threshold, low-voltage region. To harness this opportunity, we present Ti-states, a power management technique that sets the processor's voltage based on real-time silicon temperature to improve power efficiency. T i -states lead to 6% to 12% measured power saving across a range of different temperatures compared to a fixed margin. As technology scales to FD-SOI and FinFET, we show there is an ideal operating temperature for various workloads to maximize the benefits of temperature inversion. The key is to counterbalance leakage power increase at higher temperatures with dynamic power reduction by the Ti-states. The projected optimal temperature is typically around 60°C and yields 8% to 9% chip power saving. The optimal high-temperature can be exploited to reduce design cost and runtime operating power for overall cooling. Our findings are important for power and thermal management in future chips and process technologies.

---
### Graphicionado: A high-performance and energy-efficient accelerator for graph analytics.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783759
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783759
* **Key Words**: Software, Programming, Software algorithms, Machine learning algorithms, System-on-chip, Pipelines, Memory management, data structures, energy conservation, graph theory, parallel processing, power aware computing, high-performance energy-efficient accelerator, Graphicionado, graph graph processing, general purpose processors, domain-specific accelerator, data structure-centric datapath specialization, memory subsystem specialization, vertex programming, general-purpose CPU, software graph analytics, 16-core Haswell Xeon processor, 
* **Abstract**: Graphs are one of the key data structures for many real-world computing applications and the importance of graph analytics is ever-growing. While existing software graph processing frameworks improve programmability of graph analytics, underlying general purpose processors still limit the performance and energy efficiency of graph analytics. We architect a domain-specific accelerator, Graphicionado, for high-performance, energy-efficient processing of graph analytics workloads. For efficient graph analytics processing, Graphicionado exploits not only data structure-centric datapath specialization, but also memory subsystem specialization, all the while taking advantage of the parallelism inherent in this domain. Graphicionado augments the vertex programming paradigm, allowing different graph analytics applications to be mapped to the same accelerator framework, while maintaining flexibility through a small set of reconfigurable blocks. This paper describes Graphicionado pipeline design choices in detail and gives insights on how Graphicionado combats application execution inefficiencies on general-purpose CPUs. Our results show that Graphicionado achieves a 1.76-6.54x speedup while consuming 50-100x less energy compared to a state-of-the-art software graph analytics processing framework executing 32 threads on a 16-core Haswell Xeon processor.

---
### Improving bank-level parallelism for irregular applications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783760
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783760
* **Key Words**: Parallel processing, Multicore processing, Random access memory, Bars, Indexes, Hardware, Barium, iterative methods, multiprocessing systems, multi-threading, processor scheduling, irregular applications, bank-level parallelism, multithreaded applications, data access patterns, low memory bank-level parallelism, loop iteration scheduling, inspector-executor paradigm, intercore perspective, intracore perspective, multicore system, average BLP improvement, 
* **Abstract**: Observing that large multithreaded applications with irregular data access patterns exhibit very low memory bank-level parallelism (BLP) during their execution, we propose a novel loop iteration scheduling strategy built upon the inspector-executor paradigm. A unique characteristic of this strategy is that it considers both bank-level parallelism (from an inter-core perspective) and bank reuse (from an intra-core perspective) in a unified framework. Its primary goal is to improve bank-level parallelism, and bank reuse is taken into account only if doing so does not hurt bank-level parallelism. Our experiments with this strategy using eight application programs on both a simulator and a real multicore system show an average BLP improvement of 46.8% and an average execution time reduction of 18.3%.

---
### Delegated persist ordering.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783761
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783761
* **Key Words**: Semantics, Data structures, Random access memory, Benchmark testing, Coherence, Phase change materials, Instruction sets, cache storage, data structures, persistent objects, delegated persist ordering, load-store interface, persistent memory, in-memory persistent data structures, persistent data structure recoverability, 3D XPoint memory, Intel, data persistence programmer control, synchronous ordering, PM write latency, execution critical path, PM-write-intensive benchmarks, recoverable software systems, cache management, volatile execution, persistent memory, memory persistency, relaxed consistency, delegated ordering, 
* **Abstract**: Systems featuring a load-store interface to persistent memory (PM) are expected soon, making in-memory persistent data structures feasible. Ensuring persistent data structure recoverability requires constraints on the order PM writes become persistent. But, current memory systems reorder writes, providing no such guarantees. To complement their upcoming 3D XPoint memory, Intel has announced new instructions to enable programmer control of data persistence. We describe the semantics implied by these instructions, an ordering model we call synchronous ordering. Synchronous ordering (SO) enforces order by stalling execution when PM write ordering is required, exposing PM write latency on the execution critical path. It incurs an average slowdown of 7.21x over volatile execution without ordering in PM-write-intensive benchmarks. SO tightly couples enforcing order and flushing writes to PM, but this tight coupling is unneeded in many recoverable software systems. Instead, we propose delegated ordering, wherein ordering requirements are communicated explicitly to the PM controller, fully decoupling PM write ordering from volatile execution and cache management. We demonstrate that delegated ordering can bring performance within 1.93x of volatile execution, improving over SO by 3.73x.

---
### Spectral profiling: Observer-effect-free profiling by monitoring EM emanations.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783762
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783762
* **Key Words**: Time-frequency analysis, Training, Instruments, Runtime, Program processors, Signal resolution, Monitoring, program diagnostics, spectral analysis, spectral profiling, observer-effect-free profiling, EM emanation monitoring, program execution profiling, spectral spike frequency, periodic program activity, system monitoring, program profiling, MiBench suite, cycle-accurate simulator, program runtime behavior, 
* **Abstract**: This paper presents Spectral Profiling, a new method for profiling program execution without instrumenting or otherwise affecting the profiled system. Spectral Profiling monitors EM emanations unintentionally produced by the profiled system, looking for spectral “spikes” produced by periodic program activity (e.g. loops). This allows Spectral Profiling to determine which parts of the program have executed at what time. By analyzing the frequency and shape of the spectral “spike”, Spectral Profiling can obtain additional information such as the per-iteration execution time of a loop. The key advantage of Spectral Profiling is that it can monitor a system as-is, without program instrumentation, system activity, etc. associated with the profiling itself, i.e. it completely eliminates the “Observer's Effect” and allows profiling of programs whose execution is performance-dependent and/or programs that run on even the simplest embedded systems that have no resources or support for profiling. We evaluate the effectiveness of Spectral Profiling by applying it to several benchmarks from MiBench suite on a real system, and also on a cycle-accurate simulator. Our results confirm that Spectral Profiling yields useful information about the runtime behavior of a program, allowing Spectral Profiling to be used for profiling in systems where profiling infrastructure is not available, or where profiling overheads may perturb the results too much (“Observer's Effect”).

---
### Path confidence based lookahead prefetching.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783763
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783763
* **Key Words**: Prefetching, History, Hardware, Indexes, Memory management, Bandwidth, Prediction algorithms, paged storage, path confidence based lookahead prefetching, system performance maximization, signature path prefetcher design, SPP, compressed history based scheme, complex address pattern prediction, physical page boundaries, performance improvement, physical address space, 
* **Abstract**: Designing prefetchers to maximize system performance often requires a delicate balance between coverage and accuracy. Achieving both high coverage and accuracy is particularly challenging in workloads with complex address patterns, which may require large amounts of history to accurately predict future addresses. This paper describes the Signature Path Prefetcher (SPP), which offers effective solutions for three classic challenges in prefetcher design. First, SPP uses a compressed history based scheme that accurately predicts complex address patterns. Second, unlike other history based algorithms, which miss out on many prefetching opportunities when address patterns make a transition between physical pages, SPP tracks complex patterns across physical page boundaries and continues prefetching as soon as they move to new pages. Finally, SPP uses the confidence it has in its predictions to adaptively throttle itself on a per-prefetch stream basis. In our analysis, we find that SPP improves performance by 27.2% over a no-prefetching baseline, and outperforms the state-of-the-art Best Offset prefetcher by 6.4%. SPP does this with minimal overhead, operating strictly in the physical address space, and without requiring any additional processor core state, such as the PC.

---
### Continuous runahead: Transparent hardware acceleration for memory intensive workloads.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2016.7783764
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7783764
* **Key Words**: Prefetching, Proposals, Hardware, Pipelines, Engines, Performance gain, cache storage, multiprocessing systems, continuous runahead, transparent hardware acceleration, memory intensive workloads, realistic system configuration, memory intensive SPEC CPU2006 benchmarks, global history buffer, GHB, stream prefetcher, continuous runahead engine, CRE, shared multicore accelerator, memory controller, runahead buffer, 
* **Abstract**: Runahead execution pre-executes the application's own code to generate new cache misses. This pre-execution results in prefetch requests that are overwhelmingly accurate (95% in a realistic system configuration for the memory intensive SPEC CPU2006 benchmarks), much more so than a global history buffer (GHB) or stream prefetcher (by 13%/19%). However, we also find that current runahead techniques are very limited in coverage: they prefetch only a small fraction (13%) of all runahead-reachable cache misses. This is because runahead intervals are short and limited by the duration of each full-window stall. In this work, we explore removing the constraints that lead to these short intervals. We dynamically filter the instruction stream to identify the chains of operations that cause the pipeline to stall. These operations are renamed to execute speculatively in a loop and are then migrated to a Continuous Runahead Engine (CRE), a shared multi-core accelerator located at the memory controller. The CRE runs ahead with the chain continuously, increasing prefetch coverage to 70% of runahead-reachable cache misses. The result is a 43.3% weighted speedup gain on a set of memory intensive quad-core workloads and a significant reduction in system energy consumption. This is a 21.9% performance gain over the Runahead Buffer, a state-of-the-art runahead proposal and a 13.2%/13.5% gain over GHB/stream prefetching. When the CRE is combined with GHB prefetching, we observe a 23.5% gain over a baseline with GHB prefetching alone.
