### micro 2018 | 74 papers.
---
### Exploiting Locality in Graph Analytics through Hardware-Accelerated Traversal Scheduling.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00010
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574527
* **Key Words**: Schedules, Prefetching, Layout, Arrays, Software algorithms, Runtime, cache storage, graph theory, multiprocessing systems, scheduling, online locality-aware scheduling strategy, graph processing accelerators, software graph processing, random memory references, hardware-accelerated traversal scheduling, bounded depth-first scheduling, HATS, community structure, BDFS, locality-oblivious scheduling, online traversal scheduling strategies, graph analytics, graph analytics, multicore, caches, locality, scheduling, prefetching, 
* **Abstract**: Graph processing is increasingly bottlenecked by main memory accesses. On-chip caches are of little help because the irregular structure of graphs causes seemingly random memory references. However, most real-world graphs offer significant potential locality-it is just hard to predict ahead of time. In practice, graphs have well-connected regions where relatively few vertices share edges with many common neighbors. If these vertices were processed together, graph processing would enjoy significant data reuse. Hence, a graph's traversal schedule largely determines its locality. This paper explores online traversal scheduling strategies that exploit the community structure of real-world graphs to improve locality. Software graph processing frameworks use simple, locality-oblivious scheduling because, on general-purpose cores, the benefits of locality-aware scheduling are outweighed by its overheads. Software frameworks rely on offline preprocessing to improve locality. Unfortunately, preprocessing is so expensive that its costs often negate any benefits from improved locality. Recent graph processing accelerators have inherited this design. Our insight is that this misses an opportunity: Hardware acceleration allows for more sophisticated, online locality-aware scheduling than can be realized in software, letting systems significantly improve locality without any preprocessing. To exploit this insight, we present bounded depth-first scheduling (BDFS), a simple online locality-aware scheduling strategy. BDFS restricts each core to explore one small, connected region of the graph at a time, improving locality on graphs with good community structure. We then present HATS, a hardware-accelerated traversal scheduler that adds just 0.4% area and 0.2% power over general-purpose cores. We evaluate BDFS and HATS on several algorithms using large real-world graphs. On a simulated 16-core system, BDFS reduces main memory accesses by up to 2.4x and by 30% on average. However, BDF...

---
### Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00011
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574528
* **Key Words**: Synapses, Neurons, Image coding, Quantization (signal), Convergence, Biological neural networks, learning (artificial intelligence), neural nets, software-based coarse-grained pruning technique, network compression ratio, hardware accelerator, Cambricon-S, image recognition, speech recognition, natural language processing, cooperative software-hardware approach, sparse neural network accelerator, local quantization, neural networks, sparsity, accelerator, 
* **Abstract**: Neural networks have become the dominant algorithms rapidly as they achieve state-of-the-art performance in a broad range of applications such as image recognition, speech recognition and natural language processing. However, neural networks keep moving towards deeper and larger architectures, posing a great challenge to the huge amount of data and computations. Although sparsity has emerged as an effective solution for reducing the intensity of computation and memory accesses directly, irregularity caused by sparsity (including sparse synapses and neurons) prevents accelerators from completely leveraging the benefits; it also introduces costly indexing module in accelerators. In this paper, we propose a cooperative software/hardware approach to address the irregularity of sparse neural networks efficiently. Initially, we observe the local convergence, namely larger weights tend to gather into small clusters during training. Based on that key observation, we propose a software-based coarse-grained pruning technique to reduce the irregularity of sparse synapses drastically. The coarse-grained pruning technique, together with local quantization, significantly reduces the size of indexes and improves the network compression ratio. We further design a hardware accelerator, Cambricon-S, to address the remaining irregularity of sparse synapses and neurons efficiently. The novel accelerator features a selector module to filter unnecessary synapses and neurons. Compared with a state-of-the-art sparse neural network accelerator, our accelerator is 1.71× and 1.37× better in terms of performance and energy efficiency, respectively.

---
### CSE: Parallel Finite State Machines with Convergence Set Enumeration.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00012
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574529
* **Key Words**: Convergence, Automata, Hardware, Optimization, Software, Encoding, Prediction algorithms, finite state machines, probability, set theory, CSE, parallel finite state machines, convergence set enumeration, Finite State Machine, unknown starting state, state transitions, enumeration path, state state computation overhead, specific state state mappings, convergence sets, one-hot encoding, convergence set prediction, random input based profiling, global re-execution algorithm, lookback enumeration, LBE, parallel automata processor, PAP, enumerative FSM, N, S, Finite State Machine, Accelerator, Parallelism, Speculation, 
* **Abstract**: Finite State Machine (FSM) is known to be “embarrassingly sequential” because the next state depends on the current state and input symbol. Enumerative FSM breaks the data dependencies by cutting the input symbols into segments and processing all segments in parallel. With unknown starting state (except the first segment), each segment needs to calculate the state transitions, i.e., state state, for all states, each one is called an enumeration path. The current software and hardware implementations suffer from two drawbacks: 1) large amount of state state computation overhead for the enumeration paths; and 2) the optimizations are restricted by the need to correctly performing state state and only achieve limited improvements. This paper proposes CSE, a Convergence Set based Enumeration based parallel FSM. Unlike prior approaches, CSE is based on a novel computation primitive set(N) set(M), which maps N states to M states without giving the specific state state mappings (which state is mapped to which). The set(N) set(M) has two key properties: 1) if M is equal to 1, i.e., all N states are mapped to the same state, the state state for all the N states are computed; 2) using one-hot encoding, the hardware implementation cost of state state is the same as set(N) set(M). The convergence property ensures that M is always less than N. The key idea of CSE is to partition the original all S states into n state sets CS 1 ,CS 2 ,...,CS n , i.e., convergence sets. Using set(N) set(M) to process each CS, if the states converge to a single state, then we have successfully computed the enumeration path for each state in CS; otherwise, we may need to re-execute the stage when the outcome of the previous stage falls in CS. CSE is realized by two techniques: convergence set prediction, which generates the convergence sets with random input based profiling that maximizes the probability of each CS z converging to one state; global re-execution algorithm, which ensures the correctne...

---
### Inter-Thread Communication in Multithreaded, Reconfigurable Coarse-Grain Arrays.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00013
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574530
* **Key Words**: Message systems, Instruction sets, Synchronization, Convolution, Computational modeling, Graphics processing units, Computer architecture, DRAM chips, graphics processing units, multiprocessing systems, multi-threading, reconfigurable architectures, producer threads, consumer threads, memory bandwidth, scratchpad memory, direct inter-thread communication, massively multithreaded CGRAs, reconfigurable coarse-grain arrays, von Neumann GPGPUs, barrier synchronization, DRAM bandwidth, Rodinia benchmarks, coarse grained reconfigurable GPGPU architecture, CUDA, CGRA, dataflow, GPGPU, SIMD, MPI, reconfigurable -architectures, non-von Neumann-architectures, inter-thread communication, 
* **Abstract**: Traditional von Neumann GPGPUs only allow threads to communicate through memory on a group-to-group basis. In this model, a group of producer threads writes intermediate values to memory, which are read by a group of consumer threads after a barrier synchronization. To alleviate the memory bandwidth imposed by this method of communication, GPGPUs provide a small scratchpad memory that prevents intermediate values from overloading DRAM bandwidth. In this paper we introduce direct inter-thread communications for massively multithreaded CGRAs, where intermediate values are communicated directly through the compute fabric on a point-to-point basis. This method avoids the need to write values to memory, eliminates the need for a dedicated scratchpad, and avoids workgroup global barriers. We introduce our proposed extensions to the programming model (CUDA) and execution model, as well as the hardware primitives that facilitate the communication. Our simulations of Rodinia benchmarks running on the new system show that direct inter-thread communication provides an average speedup of 2.8x (10.3x max) and reduces system power by an average of 5x (22x max), when compared to an equivalent Nvidia GPGPU.

---
### An Architectural Framework for Accelerating Dynamic Parallel Algorithms on Reconfigurable Hardware.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00014
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574531
* **Key Words**: Task analysis, Parallel processing, Hardware, Field programmable gate arrays, Computational modeling, Heuristic algorithms, Computer architecture, field programmable gate arrays, multiprocessing systems, parallel algorithms, reconfigurable architectures, resource allocation, scheduling, accelerator architecture, dynamic task generation, data-dependent task spawning, static parallel patterns, high-performance accelerators, reconfigurable hardware, application-specific parallel accelerators, task-based computation model, static parallelism, high-level design frameworks, task scheduling, parallel algorithms, ParallelXL, load balancing, fork-join, task nesting, task recursion, FPGA, accelerator, FPGA, framework, parallel, programming, 
* **Abstract**: In this paper, we propose ParallelXL, an architectural framework for building application-specific parallel accelerators with low manual effort. The framework introduces a task-based computation model with explicit continuation passing to support dynamic parallelism in addition to static parallelism. In contrast, today's high-level design frameworks for accelerators focus on static data-level or thread-level parallelism that can be identified and scheduled at design time. To realize the new computation model, we develop an accelerator architecture that efficiently handles dynamic task generation and scheduling as well as load balancing through work stealing. The architecture is general enough to support many dynamic parallel constructs such as fork-join, data-dependent task spawning, and arbitrary nesting and recursion of tasks, as well as static parallel patterns. We also introduce a design methodology that includes an architectural template that allows easily creating parallel accelerators from high-level descriptions. The proposed framework is studied through an FPGA prototype as well as detailed simulations. Evaluation results show that the framework can generate high-performance accelerators targeting FPGAs for a wide range of parallel algorithms and achieve an average of 4.0x speedup over an eight-core out-of-order processor (24.1x over a single core), while being 11.8x more energy efficient.

---
### Composable Building Blocks to Open up Processor Design.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00015
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574532
* **Key Words**: Out of order, Hardware, Timing, Microarchitecture, Multicore processing, Rockets, application specific integrated circuits, field programmable gate arrays, Linux, logic design, microprocessor chips, ARM processors, Linux, out-of-order RISC-V processor, ASIC design flows, FPGA, RTL, out-of-order processor design, composable modular design, composable building blocks, atomic rules, interface method, processor design, processor design, composability, modularity, RISC V, out of order processor, guarded atomic actions, Bluespec, 
* **Abstract**: We present a framework called Composable Modular Design (CMD) to facilitate the design of out-of-order (OOO) processors. In CMD, (1) The interface methods of modules provide instantaneous access and perform atomic updates to the state elements inside the module; (2) Every interface method is guarded, i.e., it cannot be applied unless it is ready; and (3) Modules are composed together by atomic rules which call interface methods of different modules. A rule either successfully updates the state of all the called modules or it does nothing. CMD designs are compiled into RTL which can be run on FPGAs or synthesized using standard ASIC design flows. The atomicity properties of interfaces in CMD ensures composability when selected modules are refined selectively. We show the efficacy of CMD by building a parameterized out-of-order RISC-V processor which boots Linux and runs on FPGAs at 25 MHz to 40 MHz. We also synthesized several variants of it in a 32 nm technology to run at 1 GHz to 1.1 GHz. Performance evaluation shows that our processor beats in-order processors in terms of IPC but will require more architectural work to compete with wider superscalar commercial ARM processors. Modules designed under the CMD framework (e.g., ROB, reservation stations, load store unit) can be used and refined by other implementations. We believe that this realistic framework can revolutionize architectural research and practice as the library of reusable components grows.

---
### Performance Improvement by Prioritizing the Issue of the Instructions in Unconfident Branch Slices.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00016
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574533
* **Key Words**: Pipelines, Clocks, Estimation, Registers, Microarchitecture, Degradation, Task analysis, microprocessor chips, parallel architectures, performance evaluation, pipeline processing, program compilers, performance improvement, instructions, single-thread performance, branch misprediction, state recovery penalty, branch execution, branch slice unconfident, branch prediction, issue queue, IQ, PUBS scheme, processor state, superscalar processor, branch misprediction penalty, issue queue, 
* **Abstract**: Single-thread performance has hardly improved for more than a decade. One of the largest problems for performance improvements is branch misprediction. There are two approaches to reduce the penalty caused by this. One is to reduce the frequency of misprediction, and the other is to reduce the cycles consumed because of misprediction. Improving branch predictors is the former approach, and many studies on this topic have been done for several decades. However, the latter approach has been rarely studied. The present paper hence explores the latter approach. The cycles consumed because of misprediction are divided into the following two parts. The first part is the state recovery penalty, which consists of cycles consumed for rolling back the processor state. The second part is the misspeculation penalty, which are cycles consumed during useless speculative execution from the fetch of a mispredicted branch until the completion of the branch execution. We focus on reducing the misspeculation penalty. For this, we propose a scheme called PUBS, which allows the instructions in unconfident branch slices to be issued with highest priority from the issue queue (IQ). Here, a branch slice is a set consisting of a branch and the instructions this branch directly or indirectly depends on, and we call the branch slice unconfident if the associated branch prediction cannot be sufficiently trusted. By issuing instructions in unconfident branch slices as early as possible, the wait cycles of these instructions in the IQ are minimized and thus the misspeculation penalty is minimized. Our evaluation results using SPEC2006 benchmark programs show that the PUBS scheme improves the performance of the programs with difficult branch prediction by 7.8% on average (a maximum of 19.2%) using only 4.0KB hardware cost.

---
### The Superfluous Load Queue.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00017
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574534
* **Key Words**: Memory management, Out of order, Buffer storage, Load modeling, Coherence, Search problems, Delays, cache storage, multiprocessing systems, parallel processing, superfluous load queue, store queue, store buffer, speculative reordered loads, nonspeculative load reordering, SQ-SB system, program execution, parallel code, serial code, multiprocessor ordering, uniprocessor ordering, CAM structures, nonassociative LQ, sequential consistency, TSO load, Memory consistency, Out-of-Order Execution, Total Store Order, 
* **Abstract**: In an out-of-order core, the load queue (LQ), the store queue (SQ), and the store buffer (SB) are responsible for ensuring: i) correct forwarding of stores to loads and ii) correct ordering among loads (with respect to external stores). The first requirement safeguards the sequential semantics of program execution and applies to both serial and parallel code; the second requirement safeguards the semantics of coherence and consistency (e.g., TSO). In particular, loads search the SQ/SB for the latest value that may have been produced by a store, and stores and invalidations search the LQ to find speculative loads in case they violate uniprocessor or multiprocessor ordering. To meet timing constraints the LQ and SQ/SB system is composed of CAM structures that are frequently searched. This results in high complexity, cost, and significant difficulty to scale, but is the current state of the art. Prior research demonstrated the feasibility of a non-associative LQ by replaying loads at commit. There is a steep cost however: a significant increase in L1 accesses and contention for L1 ports. This is because prior work assumes Sequential Consistency and completely ignores the existence of a SB in the system. In contrast, we intentionally delay stores in the SB to achieve a total management of stores and loads in a core, while still supporting TSO. Our main result is that we eliminate the LQ without burdening the L1 with extra accesses. Store forwarding is achieved by delaying our own stores until speculatively issued loads are validated on commit, entirely in-core; TSO load→load ordering is preserved by delaying remote external stores in their SB until our own speculative reordered loads commit. While the latter is inspired by recent work on non-speculative load reordering, our contribution here is to show that this can be accomplished without having a load queue. Eliminating the LQ results in both energy savings and performance improvement from the elimination of LQ-induce...

---
### Architectural Support for Probabilistic Branches.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00018
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574535
* **Key Words**: Probabilistic logic, Prediction algorithms, Hardware, Program processors, Optimization, Bars, Biological system modeling, computer architecture, data handling, pipeline processing, probability, statistical analysis, inherent probabilistic nature, probabilistic codes, branch mispredictions, PBS, probabilistic values, mispredicted probabilistic branches, fine-tuning branch predictors, probabilistic computation, probabilistic branch support, Branch Prediction, Probabilistic Branches, Probabilistic Programs, 
* **Abstract**: A plethora of research efforts have focused on fine-tuning branch predictors to increasingly higher levels of accuracy. However, several important optimization, financial, and statistical data analysis algorithms rely on probabilistic computation. These applications draw random values from a distribution and steer control flow based on those values. Such probabilistic branches are challenging to predict because of their inherent probabilistic nature. As a result, probabilistic codes significantly suffer from branch mispredictions. This paper proposes Probabilistic Branch Support (PBS), a hardware/software cooperative technique that leverages the observation that the outcome of probabilistic branches needs to be correct only in a statistical sense. PBS stores the outcome and the probabilistic values that lead to the outcome of the current execution to direct the next execution of the probabilistic branch, thereby completely removing the penalty for mispredicted probabilistic branches. PBS relies on marking probabilistic branches in software for hardware to exploit. Our evaluation shows that PBS improves MPKI by 45% on average (and up to 99%) and IPC by 6.7% (up to 17%) over the TAGE-SC-L predictor. PBS requires 193 bytes of hardware overhead and introduces statistically negligible algorithmic inaccuracy.

---
### STRAIGHT: Hazardless Processor Architecture Without Register Renaming.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00019
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574536
* **Key Words**: Registers, Computer architecture, Microarchitecture, Pipelines, Instruction sets, Hardware, Scalability, instruction sets, microprocessor chips, parallel architectures, pipeline processing, power consumption, OoO microarchitecture, assembler, linker, cycle-accurate simulator, power consumption reduction, instruction set architecture design, hazardless processor architecture, CPUs, ISA design, general purpose OoO processors, RTL power analysis, instruction window size, power reduction, compiler, sequential execution semantics, STRAIGHT machine code, compilation algorithm, computer architecture, out-of-order execution, conventional superscalar processor, single-thread performance, microprocessor, instruction-level-parallelism, out-of-order execution, register renaming, computer architecture, compiler, power efficiency, 
* **Abstract**: The single-thread performance of a processor improves the capability of the entire system by reducing the critical path latency of programs. Typically, conventional superscalar processors improve this performance by introducing out-of-order (OoO) execution with register renaming. However, it is also known to increase the complexity and affect the power efficiency. This paper realizes a novel computer architecture called "STRAIGHT" to resolve this dilemma. The key feature is a unique instruction format in which the source operand is given based on the distance from the producer instruction. By leveraging this format, register renaming is completely removed from the pipeline. This paper presents the practical Instruction Set Architecture (ISA) design, the novel efficient OoO microarchitecture, and the compilation algorithm for the STRAIGHT machine code. Because the ISA has sequential execution semantics, as in general CPUs, and is provided with a compiler, programming for the architecture is as easy as that of conventional CPUs. A compiler, an assembler, a linker, and a cycle-accurate simulator are developed to measure the performance. Moreover, an RTL description of STRAIGHT is developed to estimate the power reduction. The evaluation using standard benchmarks shows that the performance of STRAIGHT is 18.8% better than the conventional superscalar processor of the same issue-width and instruction window size. This improvement is achieved by STRAIGHT's rapid miss-recovery. Compilation technology for resolving the possible overhead of the ISA is also revealed. The RTL power analysis shows that the architecture reduces the power consumption by removing the power for renaming. The revealed performance and efficiencies support that STRAIGHT is a novel viable alternative for designing general purpose OoO processors.

---
### Diffy: a Déjà vu-Free Differential Deep Neural Network Accelerator.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00020
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574537
* **Key Words**: Correlation, Imaging, Microsoft Windows, Convolution, Computational modeling, Entropy, Task analysis, convolution, feedforward neural nets, image classification, image resolution, on-chip energy, on-chip storage, off-chip memory bandwidth, HD resolution images, Diffy boosts, hardware accelerator, agnostic accelerator, image classification, differential convolution, CNN accelerator, computational imaging, deep convolutional neural network, deep neural network accelerator, neural networks, deep learning, differential convolution, computational imaging, accelerator, 
* **Abstract**: We show that Deep Convolutional Neural Network (CNN) implementations of computational imaging tasks exhibit spatially correlated values. We exploit this correlation to reduce the amount of computation, communication, and storage needed to execute such CNNs by introducing Diffy, a hardware accelerator that performs Differential Convolution. Diffy stores, communicates, and processes the bulk of the activation values as deltas. Experiments show that, over five state-of-the-art CNN models and for HD resolution inputs, Diffy boosts the average performance by 7.1× over a baseline value-agnostic accelerator [1] and by 1.41× over a state-of-the-art accelerator that processes only the effectual content of the raw activation values [2]. Further, Diffy is respectively 1.83× and 1.36× more energy efficient when considering only the on-chip energy. However, Diffy requires 55% less on-chip storage and 2.5× less off-chip bandwidth compared to storing the raw values using profiled per-layer precisions [3]. Compared to using dynamic per group precisions [4], Diffy requires 32% less storage and 1.43× less off-chip memory bandwidth. More importantly, Diffy provides the performance necessary to achieve real-time processing of HD resolution images with practical configurations. Finally, Diffy is robust and can serve as a general CNN accelerator as it improves performance even for image classification models.

---
### Beyond the Memory Wall: A Case for Memory-Centric HPC System for Deep Learning.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00021
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574538
* **Key Words**: Training, Bandwidth, Performance evaluation, Systems architecture, Memory management, Proposals, learning (artificial intelligence), memory architecture, parallel processing, shared memory systems, storage management, memory modules, device-side interconnect, transparent memory capacity expansion, conventional systems, system-wide memory capacity, memory wall, memory-centric HPC system, deep learning models scale, system architects, memory capacity bottleneck, physical memory, accelerator device, memory-centric deep learning system, fast inter-device communication, parallel training, system architecture, HPC, machine learning, 
* **Abstract**: As the models and the datasets to train deep learning (DL) models scale, system architects are faced with new challenges, one of which is the memory capacity bottleneck, where the limited physical memory inside the accelerator device constrains the algorithm that can be studied. We propose a memory-centric deep learning system that can transparently expand the memory capacity available to the accelerators while also providing fast inter-device communication for parallel training. Our proposal aggregates a pool of memory modules locally within the device-side interconnect, which are decoupled from the host interface and function as a vehicle for transparent memory capacity expansion. Compared to conventional systems, our proposal achieves an average 2.8x speedup on eight DL applications and increases the system-wide memory capacity to tens of TBs.

---
### Towards Memory Friendly Long-Short Term Memory Networks (LSTMs) on Mobile GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00022
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574539
* **Key Words**: Logic gates, Optimization, Recurrent neural networks, Natural language processing, Kernel, Bandwidth, Memory management, learning (artificial intelligence), natural language processing, recurrent neural nets, telecommunication computing, mobile GPU, intelligent personal assistants, LSTM cells, GPU architecture, input data, LSTM exhibit, NLP-based IPA, memory friendly long-short term memory networks, weight matrices, intra-cell level optimizations, limited off-chip memory bandwidth, data locality, inter-cell level optimizations, off-chip memory accesses, memory friendly LSTM, redundant data movements, inefficient memory access pattern, privacy issues, data transmission, local processing, core machine learning technique, recurrent neural networks, mobile devices, natural language processing, Approximate Computing, GPGPU/GPU, Mobile and Embedded Architectures, 
* **Abstract**: Intelligent Personal Assistants (IPAs) with the capability of natural language processing (NLP) are increasingly popular in today's mobile devices. Recurrent neural networks (RNNs), especially one of their forms - Long-Short Term Memory networks (LSTMs), are becoming the core machine learning technique applied in the NLP-based IPAs. With the continuously improved performance of mobile GPUs, local processing has become a promising solution to the large data transmission and privacy issues induced by the cloud-centric computations of IPAs. However, LSTMs exhibit quite inefficient memory access pattern when executed on mobile GPUs due to the redundant data movements and limited off-chip bandwidth. In this study, we aim to explore the memory friendly LSTM on mobile GPUs by hierarchically reducing the off-chip memory accesses. To address the redundant data movements, we propose inter-cell level optimizations that intelligently parallelize the originally sequentially executed LSTM cells (basic units in RNNs, corresponding to neurons in CNNs) to improve the data locality across cells with negligible accuracy loss. To relax the pressure on limited off-chip memory bandwidth, we propose intra-cell level optimizations that dynamically skip the loads and computations of rows in the weight matrices with trivial contribution to the outputs. We also introduce a light-weighted module to the GPUs architecture for the runtime row skipping in weight matrices. Moreover, our techniques are equipped with thresholds which provide a unique tunning space for performance-accuracy trade-offs directly guided by the user preferences. The experimental results show our optimizations achieves substantial improvements on both performance and power with user-imperceptible accuracy loss. And our optimizations exhibit the strong scalability with the increasing input data set. Our user study also shows that our designed system delivers the excellent user experience.

---
### A Network-Centric Hardware/Algorithm Co-Design to Accelerate Distributed Training of Deep Neural Networks.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00023
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574540
* **Key Words**: Training, Neural networks, Hardware, Acceleration, Complexity theory, Compression algorithms, Distributed databases, data compression, gradient methods, network interfaces, neural nets, leveraging distributed systems, worker-aggregator nodes, gradient updates, significant communication cost, data compression accelerators, in-network acceleration, algorithmic innovations, aggressive compression, existing training algorithms, floating-point gradients, gradient communication, aggregator-free training algorithm, distributed manner, aggregation operator, in-network accelerators, communication time, conventional training system, network interface cards, in-network computing, exchange gradients, process training information, distributed training algorithms, real-world deep neural networks, Network-centric hardware-algorithm, accelerate distributed training, Accelerators, Domain Specific Architectures, Reconfigurable Architectures and FPGA, DNN Training, 
* **Abstract**: Training real-world Deep Neural Networks (DNNs) can take an eon (i.e., weeks or months) without leveraging distributed systems. Even distributed training takes inordinate time, of which a large fraction is spent in communicating weights and gradients over the network. State-of-the-art distributed training algorithms use a hierarchy of worker-aggregator nodes. The aggregators repeatedly receive gradient updates from their allocated group of the workers, and send back the updated weights. This paper sets out to reduce this significant communication cost by embedding data compression accelerators in the Network Interface Cards (NICs). To maximize the benefits of in-network acceleration, the proposed solution, named INCEPTIONN (In-Network Computing to Exchange and Process Training Information Of Neural Networks), uniquely combines hardware and algorithmic innovations by exploiting the following three observations. (1) Gradients are significantly more tolerant to precision loss than weights and as such lend themselves better to aggressive compression without the need for the complex mechanisms to avert any loss. (2) The existing training algorithms only communicate gradients in one leg of the communication, which reduces the opportunities for in-network acceleration of compression. (3) The aggregators can become a bottleneck with compression as they need to compress/decompress multiple streams from their allocated worker group. To this end, we first propose a lightweight and hardware-friendly lossy-compression algorithm for floating-point gradients, which exploits their unique value characteristics. This compression not only enables significantly reducing the gradient communication with practically no loss of accuracy, but also comes with low complexity for direct implementation as a hardware block in the NIC. To maximize the opportunities for compression and avoid the bottleneck at aggregators, we also propose an aggregator-free training algorithm that exchanges gradien...

---
### PermDNN: Efficient Compressed DNN Architecture with Permuted Diagonal Matrices.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00024
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574541
* **Key Words**: Sparse matrices, Computational modeling, Computer architecture, Hardware, Task analysis, Urban areas, Artificial intelligence, computational complexity, data compression, matrix algebra, multiprocessing systems, neural nets, parallel architectures, permuted diagonal matrices, deep neural network, network sparsification approach, heuristic nature, indexing overhead, input sparsity, unstructured sparsification approach, nonheuristic compression effects, circulant structure-imposing approach, computational complexity, PermDNN architecture, matrix-based approach, compression ratio, arithmetic computation, sparse DNN models, complex arithmetic computation, compressed DNN architecture, CMOS, artificial intelligent, Deep Learning, Mold compression, VLSI, 
* **Abstract**: Deep neural network (DNN) has emerged as the most important and popular artificial intelligent (AI) technique. The growth of model size poses a key energy efficiency challenge for the underlying computing platform. Thus, model compression becomes a crucial problem. However, the current approaches are limited by various drawbacks. Specifically, network sparsification approach suffers from irregularity, heuristic nature and large indexing overhead. On the other hand, the recent structured matrix-based approach (i.e., CirCNN) is limited by the relatively complex arithmetic computation (i.e., FFT), less flexible compression ratio, and its inability to fully utilize input sparsity. To address these drawbacks, this paper proposes PermDNN, a novel approach to generate and execute hardware-friendly structured sparse DNN models using permuted diagonal matrices. Compared with unstructured sparsification approach, PermDNN eliminates the drawbacks of indexing overhead, non-heuristic compression effects and time-consuming retraining. Compared with circulant structure-imposing approach, PermDNN enjoys the benefits of higher reduction in computational complexity, flexible compression ratio, simple arithmetic computation and full utilization of input sparsity. We propose PermDNN architecture, a multi-processing element (PE) fully-connected (FC) layer-targeted computing engine. The entire architecture is highly scalable and flexible, and hence it can support the needs of different applications with different model configurations. We implement a 32-PE design using CMOS 28nm technology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout, 5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher throughput and 3.89x better energy efficiency.

---
### Rethinking the Memory Hierarchy for Modern Languages.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00025
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574542
* **Key Words**: Memory management, Layout, Hardware, Arrays, Java, Software, Resource management, cache storage, Java, optimisation, program debugging, storage management, memory hierarchy, Hotpads, modern memory-safe languages, memory layout, memory corruption bugs, automatic memory management, directly addressed memories, memory allocation, Java, programmer, software, flat address space abstraction, associative caches, hierarchical garbage collection, data placement, optimizations, Memory hierarchy, cache, scratchpad, memory safe languages, managed languages, garbage collection, 
* **Abstract**: We present Hotpads, a new memory hierarchy designed from the ground up for modern, memory-safe languages like Java, Go, and Rust. Memory-safe languages hide the memory layout from the programmer. This prevents memory corruption bugs and enables automatic memory management. Hotpads extends the same insight to the memory hierarchy: it hides the memory layout from software and takes control over it, dispensing with the conventional flat address space abstraction. This avoids the need for associative caches. Instead, Hotpads moves objects across a hierarchy of directly addressed memories. It rewrites pointers to avoid most associative lookups, provides hardware support for memory allocation, and unifies hierarchical garbage collection and data placement. As a result, Hotpads improves memory performance and efficiency substantially, and unlocks many new optimizations.

---
### Harmonizing Speculative and Non-Speculative Execution in Architectures for Ordered Parallelism.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00026
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574543
* **Key Words**: Task analysis, Parallel processing, Synchronization, Hardware, Instruction sets, Databases, Logic gates, multiprocessing systems, parallel programming, transaction processing, hardware transactional memory, multicore systems, HTM, speculative parallelism, nonspeculative parallelism, nonspeculative execution, software-managed speculative actions, ordered parallelism, unordered parallelism, multicore, speculative parallelism, ordered parallelism, fine-grain parallelism, transactional memory, thread level speculation, speculative forwarding, synchronization, 
* **Abstract**: Multicore systems should support both speculative and non-speculative parallelism. Speculative parallelism is easy to use and is crucial to scale many challenging applications, while non-speculative parallelism is more efficient and allows parallel irrevocable actions (e.g., parallel I/O). Unfortunately, prior techniques are far from this goal. Hardware transactional memory (HTM) systems support speculative (transactional) and non-speculative (non-transactional) work, but lack coordination mechanisms between the two, and are limited to unordered parallelism. Prior work has extended HTMs to avoid the limitations of speculative execution, e.g., through escape actions and open-nested transactions. But these mechanisms are incompatible with systems that exploit ordered parallelism, which parallelize a broader range of applications and are easier to use. We contribute two techniques that enable seamlessly composing and coordinating speculative and non-speculative work in the context of ordered parallelism: (i) a task-based execution model that efficiently coordinates concurrent speculative and non-speculative ordered tasks, allowing them to create tasks of either kind and to operate on shared data; and (ii) a safe way for speculative tasks to invoke software-managed speculative actions that avoid hardware version management and conflict detection. These contributions improve efficiency and enable new capabilities. Across several benchmarks, they allow the system to dynamically choose whether to execute tasks speculatively or non-speculatively, avoid needless conflicts among speculative tasks, and allow speculative tasks to safely invoke irrevocable actions.

---
### Sampler: PMU-Based Sampling to Detect Memory Errors Latent in Production Software.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00027
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574544
* **Key Words**: Phasor measurement units, Hardware, Software, Production, Tools, Security, Metadata, meta data, program debugging, quality of service, security of data, storage management, PMU-based sampling, system reliability, memory accesses, production software, memory bugs, performance monitoring unit, system security, hardware-based sampling, heap allocator design, quality of service, QoS, metadata lookup, Sampling, PMU, Vulnerability Detection, 
* **Abstract**: Deployed software is still faced with numerous in-production memory errors. They can significantly affect system reliability and security, causing application crashes, erratic execution behavior, or security attacks. Unfortunately, existing tools cannot be deployed in the production environment, since they either impose significant performance/memory overhead, or can only detect partial errors. This paper presents Sampler, a library that employs the combination of hardware-based SAMPLing and novel heap allocator design to efficiently identify a range of memory ERrors, including buffer overflows, use-after-frees, invalid frees, and double-frees. Due to the stringent Quality of Service (QoS) requirement of production services, Sampler proposes to trade detection effectiveness for performance on each execution. Rather than inspecting every memory access, Sampler proposes the use of the Performance Monitoring Unit (PMU) hardware to sample memory accesses, and only checks the validity of sampled accesses. At the same time, Sampler proposes a novel dynamic allocator supporting fast metadata lookup, and a solution to prevent false alarms potentially caused by sampling. The sampling-based approach, although it may lead to reduced effectiveness on each execution, is suitable for in-production software, since software is generally employed by a large number of individuals, and may be executed many times or over a long period of time. By randomizing the start of the sampling, different executions may sample different sequences of memory accesses, working together to enable effective detection. Experimental results demonstrate that Sampler detects all known memory bugs inside real applications, without any false positive. Sampler only imposes negligible performance overhead (2.4% on average). Sampler is the first work that simultaneously satisfies efficiency, preciseness, completeness, accuracy, and transparency, making it a practical tool for in-production deployment.

---
### TAPAS: Generating Parallel Accelerators from Parallel Programs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00028
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574545
* **Key Words**: Task analysis, Parallel processing, Tools, Hardware, Concurrent computing, Field programmable gate arrays, Dynamic scheduling, high level synthesis, microprocessor chips, parallel programming, system-on-chip, TAPAS, parallel accelerators, parallel programs, high-level-synthesis tools, software programs, parallel hardware, heterogeneous parallelism, fine-grain parallelism, accelerator, Intel-Altera DE1-SoC, Arria-10, High-level Synthesis, LLVM, Chisel, HLS, Cilk, TAPAS, Hardware accelerator, Power efficiency, Dynamic parallelism, FPGA, 
* **Abstract**: High-level-synthesis (HLS) tools generate accelerators from software programs to ease the task of building hardware. Unfortunately, current HLS tools have limited support for concurrency, which impacts the speedup achievable with the generated accelerator. Current approaches only target fixed static patterns (e.g., pipeline, data-parallel kernels). This constraints the ability of software programmers to express concurrency. Moreover, the generated accelerator loses a key benefit of parallel hardware, dynamic asynchrony, and the potential to hide long latency and cache misses. We have developed TAPAS, an HLS toolchain for generating parallel accelerators from programs with dynamic parallelism. TAPAS is built on top of Tapir [22], [39], which embeds fork-join parallelism into the compiler's intermediate-representation. TAPAS leverages the compiler IR to identify parallelism and synthesizes the hardware logic. TAPAS provides first-class architecture support for spawning, coordinating and synchronizing tasks during accelerator execution. We demonstrate TAPAS can generate accelerators for concurrent programs with heterogeneous, nested and recursive parallelism. Our evaluation on Intel-Altera DE1-SoC and Arria-10 boards demonstrates that TAPAS generated accelerators achieve 20× the power efficiency of an Intel Xeon, while maintaining comparable performance. We also show that TAPAS enables lightweight tasks that can be spawned in '10 cycles and enables accelerators to exploit available fine-grain parallelism. TAPAS is a complete HLS toolchain for synthesizing parallel programs to accelerators and is open-sourced.

---
### iDO: Compiler-Directed Failure Atomicity for Nonvolatile Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00029
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574546
* **Key Words**: Computer crashes, Nonvolatile memory, Programming, Registers, Random access memory, Hardware, Databases, C++ language, cache storage, concurrency control, program compilers, program diagnostics, system recovery, current idempotent region, current failure-atomic section, JUSTDO logging, nonvolatile caches, compiler-directed failure atomicity, nonvolatile memory, compiler-directed approach, instruments each store, persistent data, iDO compiler, idempotent instruction sequences, lock-based persistence mechanisms, persistent store, Non volatile memory, Memory consistency, Compiler optimization, 
* **Abstract**: This paper presents iDO, a compiler-directed approach to failure atomicity with nonvolatile memory. Unlike most prior work, which instruments each store of persistent data for redo or undo logging, the iDO compiler identifies idempotent instruction sequences, whose re-execution is guaranteed to be side-effect-free, thereby eliminating the need to log every persistent store. Using an extension of prior work on JUSTDO logging, the compiler then arranges, during recovery from failure, to back up each thread to the beginning of the current idempotent region and re-execute to the end of the current failure-atomic section. This extension transforms JUSTDO logging from a technique of value only on hypothetical future machines with nonvolatile caches into a technique that also significantly outperforms state-of-the art lock-based persistence mechanisms on current hardware during normal execution, while preserving very fast recovery times.

---
### Scalable Distributed Last-Level TLBs Using Low-Latency Interconnects.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00030
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574547
* **Key Words**: Random access memory, Delays, Prefetching, Runtime, Design methodology, Wires, Libraries, cache storage, microprocessor chips, multiprocessing systems, performance evaluation, memory translation requests, scalable distributed last-level TLBs, low-latency interconnects, multiple cores, memory translation performance, scalable shared L2 TLBs, monolithic shared TLBs, interconnect delays, lightweight single-cycle interconnect, Big data, stumbling block, TLB lookup latency, memory translation responses, NOCSTAR, Virtual memory, TLB, network-on-chip, caches, 
* **Abstract**: Recent studies have shown the potential of last-level TLBs shared by multiple cores in tackling memory translation performance challenges posed by "big data" workloads. A key stumbling block hindering their effectiveness, however, is their high access time. We present a design methodology to reduce these high access times so as to realize high-performance and scalable shared L2 TLBs. As a first step, we study the benefits of replacing monolithic shared TLBs with a distributed set of small TLB slices. While this approach does reduce TLB lookup latency, it increases interconnect delays in accessing remote slices. Therefore, as a second step, we devise a lightweight single-cycle interconnect among the TLB slices by tailoring wires and switches to the unique communication characteristics of memory translation requests and responses. Our approach, which we dub NOCSTAR (NOCs for scalable TLB architecture), combines the high hit rates of shared TLBs with low access times of private L2 TLBs, enabling significant system performance benefits.

---
### Duplicon Cache: Mitigating Off-Chip Memory Bank and Bank Group Conflicts Via Data Duplication.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00031
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574548
* **Key Words**: Random access memory, Delays, Memory management, Coherence, Probabilistic logic, Microarchitecture, cache storage, random-access storage, off-chip memory bank, memory controller, multiprogrammed memory intensive workloads, data duplication, bank group conflicts, Duplicon Cache, bank conflicts, bank group conflicts, duplication, set associative cache, sectored cache, demand activates filtering, usefulness tracking, probabilistic replacement, 
* **Abstract**: Bank and bank group conflicts are major performance bottlenecks for memory intensive workloads. Idealized experiments show removing bank and bank group conflicts collectively can improve performance by up to 37.5% and by 22.5% on average for our mix of multi-programmed memory intensive workloads. We propose the Duplicon Cache to mitigate bank and bank group conflict penalties by duplicating select lines of data to an alternative bank group, giving the memory controller the freedom to source the data from the bank group which avoids conflicts. The Duplicon Cache is entirely implemented in the memory controller and does not require changes to commodity memory. We identify and address the main challenges associated with duplication: 1) tracking duplicated data efficiently, 2) identifying which data to duplicate, and 3) replacing stale duplicated data while protecting useful ones. Our evaluations show the Duplicon Cache configured with 128MB of storage (out of 16GB of main memory) improves performance by 8.3% while reducing energy by 5.6%.

---
### Reducing DRAM Latency via Charge-Level-Aware Look-Ahead Partial Restoration.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00032
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574549
* **Key Words**: Arrays, Timing, Capacitors, DRAM chips, System performance, DRAM chips, charge-level-aware look-ahead partial restoration, CAL, memory controller, DRAM cells array activation, data access time, DRAM energy consumption, DRAM latency reduction, DRAM-latency, activation-latency, restoration-latency, partial-restoration, 
* **Abstract**: Long DRAM access latency is a major bottleneck for system performance. In order to access data in DRAM, a memory controller (1) activates (i.e., opens) a row of DRAM cells in a cell array, (2) restores the charge in the activated cells back to their full level, (3) performs read and write operations to the activated row, and (4) precharges the cell array to prepare for the next activation. The restoration operation is responsible for a large portion (up to 43.6%) of the total DRAM access latency. We find two frequent cases where the restoration operations performed by DRAM do not need to fully restore the charge level of the activated DRAM cells, which we can exploit to reduce the restoration latency. First, DRAM rows are periodically refreshed (i.e., brought back to full charge) to avoid data loss due to charge leakage from the cell. The charge level of a DRAM row that will be refreshed soon needs to be only partially restored, providing just enough charge so that the refresh can correctly detect the cells' data values. Second, the charge level of a DRAM row that will be activated again soon can be only partially restored, providing just enough charge for the activation to correctly detect the data value. However, partial restoration needs to be done carefully: for a row that will be activated again soon, restoring to only the minimum possible charge level can undermine the benefits of complementary mechanisms that reduce the activation time of highly-charged rows. To enable effective latency reduction for both activation and restoration, we propose charge-level-aware look-ahead partial restoration (CAL). CAL consists of two key components. First, CAL accurately predicts the next access time, which is the time between the current restoration operation and the next activation of the same row. Second, CAL uses the predicted next access time and the next refresh time to reduce the restoration time, ensuring that the amount of partial charge restoration is enough to ma...

---
### CABLE: A CAche-Based Link Encoder for Bandwidth-Starved Manycores.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00033
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574550
* **Key Words**: Dictionaries, Bandwidth, Communication cables, Coherence, Throughput, Instruction sets, cache storage, data compression, multiprocessing systems, pipeline processing, off-chip links, memory link interface, off-chip memory, cache-coherent link, multichip system, off-chip bandwidth, system throughput, bandwidth-starved manycores, scarce resource, massively-parallel computation, off-chip link compression, latency-driven compression schemes, latency-tolerant manycore systems, dictionary capacity, concurrent threads, point-to-point link compression, coherent caches, massive dictionary, scalable dictionary, data compression, CAche-Based Link Encoder, CABLE search pipeline hardware, cache memory, data compression, parallel processing, 
* **Abstract**: Off-chip bandwidth is a scarce resource in modern processors, and it is expected to become even more limited on a per-core basis as we move into the era of high-throughput and massively-parallel computation. One promising approach to overcome limited bandwidth is off-chip link compression. Unfortunately, previously proposed latency-driven compression schemes are not a good fit for latency-tolerant manycore systems, and they often do not have the dictionary capacity to accommodate more than a few concurrent threads. In this work, we present CABLE, a novel CAche-Based Link Encoder that enables point-to-point link compression between coherent caches, re-purposing the data already stored in the caches as a massive and scalable dictionary for data compression. We show the broad applicability of CABLE by applying it to two critical off-chip links: (1) the memory link interface to off-chip memory, and (2) the cache-coherent link between processors in a multi-chip system. We have implemented CABLE's search pipeline hardware in Verilog using the OpenPiton framework to show its feasibility. Evaluating with SPEC2006, we find that CABLE increases effective off-chip bandwidth by 7.2× and system throughput by 3.78× on average, 83% and 258% better than CPACK, respectively.

---
### Attaché: Towards Ideal Memory Compression by Mitigating Metadata Bandwidth Overheads.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00034
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574551
* **Key Words**: Metadata, Bandwidth, DRAM chips, Data compression, Engines, Memory modules, cache storage, data compression, energy consumption, meta data, SRAM chips, metadata bandwidth overheads, memory block, fine-grained line-level predictor, attache framework, blended metadata engine, metadata-cache, BLEM, compression predictor, COPR, global indicator, memory read request, energy consumption, baseline system, SRAM, ideal memory compression, data compression, memory system, coarse-grained page-level predictor, Data Compression, Metadata, Bandwidth, Sub Ranking, Memory Systems, 
* **Abstract**: Memory systems are becoming bandwidth constrained and data compression is seen as a simple technique to increase their effective bandwidth. However, data compression requires accessing Metadata which incurs additional bandwidth overheads. Even after using a Metadata-Cache, the bandwidth overheads of Metadata can reduce the benefits of compression. This paper proposes Attaché, a framework that reduces the overheads of Metadata accesses. The Attaché framework consists of two components. The first component, called the Blended Metadata Engine (BLEM), enables data and its Metadata to be accessed together. BLEM incurs additional Metadata accesses only 0.003% times and removes almost all Metadata bandwidth overheads. The second component, called theCompression Pre-dictor(COPR), predicts if the memory block is compressed. TheCOPR predictor uses a fine-grained line-level predictor, a coarse-grained page-level predictor, and a global indicator. This enables Attaché to predict the compressibility of the memory block before sending a memory read request. We implement Attaché on a memory system that uses Sub-Ranking. On average, Attaché achieves 15.3% speedup (ideal 17%) and saves 22% energy consumption (ideal 23%) when compared to a baseline system that does not employ data compression. Attaché is completely hardware-based and uses only 368KB of SRAM.

---
### Combining HW/SW Mechanisms to Improve NUMA Performance of Multi-GPU Systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00035
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574552
* **Key Words**: Graphics processing units, Bandwidth, Hardware, Coherence, Random access memory, Transistors, cache storage, graphics processing units, multiprocessing systems, parallel processing, shared memory systems, storage management, GPU performance scaling, system-level interconnects, inter-GPU interconnect bandwidth, multiGPU performance, frequent remote GPU memory accesses, page migration, local memory, recent proposals, software runtime system, shared pages, frequent remote memory accesses, remote shared data, GPU last-level-cache, remote data caching, shared-data working-set, combined performance analysis, NUMA performance, multiGPU systems, 4-node multiGPU system, page placement, page replication, caching remote data, ideal NUMA-GPU system, shared memory footprint, NUMA bandwidth bottleneck, reduced GPU memory capacity, video memory, single GPU, HW-SW mechanisms, CARVE, NUMA-aware software solutions, GPU LLC size, GPU, Multi-GPU, Memory, NUMA, HBM, DRAM-Cache, Coherence, Page-Migration, Page-Replication, 
* **Abstract**: Historically, improvement in GPU performance has been tightly coupled with transistor scaling. As Moore's Law slows down, performance of single GPUs may ultimately plateau. To continue GPU performance scaling, multiple GPUs can be connected using system-level interconnects. However, limited inter-GPU interconnect bandwidth (e.g., 64GB/s) can hurt multi-GPU performance when there are frequent remote GPU memory accesses. Traditional GPUs rely on page migration to service the memory accesses from local memory instead. Page migration fails when the page is simultaneously shared between multiple GPUs in the system. As such, recent proposals enhance the software runtime system to replicate read-only shared pages in local memory. Unfortunately, such practice fails when there are frequent remote memory accesses to read-write shared pages. To address this problem, recent proposals cache remote shared data in the GPU last-level-cache (LLC). Unfortunately, remote data caching also fails when the shared-data working-set exceeds the available GPU LLC size. This paper conducts a combined performance analysis of state-of-the-art software and hardware mechanisms to improve NUMA performance of multi-GPU systems. Our evaluations on a 4-node multi-GPU system reveal that the combination of work scheduling, page placement, page migration, page replication, and caching remote data still incurs a 47% slowdown relative to an ideal NUMA-GPU system. This is because the shared memory footprint tends to be significantly larger than the GPU LLC size and can not be replicated by software because the shared footprint has read-write property. Thus, we show that existing NUMA-aware software solutions require hardware support to address the NUMA bandwidth bottleneck. We propose Caching Remote Data in Video Memory (CARVE), a hardware mechanism that stores recently accessed remote shared data in a dedicated region of the GPU memory. CARVE outperforms state-of-the-art NUMA mechanisms and is within 6% t...

---
### Neighborhood-Aware Address Translation for Irregular GPU Applications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00036
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574553
* **Key Words**: Graphics processing units, Memory management, Legged locomotion, Instruction sets, cache storage, graphics processing units, memory architecture, paged storage, tree data structures, neighborhood-aware address translation, irregular GPU applications, virtual-to-physical address translations, concurrent memory accesses, address translation requests, address translation overheads, concurrent page walk requests, single GPU wavefront, virtual page addresses, address mappings, memory access, page table walker, cache line, single-instruction multiple-thread execution, concurrent page table walks, 
* **Abstract**: Recent studies on commercial hardware demonstrated that irregular GPU workloads could bottleneck on virtual-to-physical address translations. GPU's single-instruction multiple-thread (SIMT) execution can generate many concurrent memory accesses, all of which require address translation before accesses can complete. Unfortunately, many of these address translation requests often miss in the TLB, generating many concurrent page table walks. In this work, we investigate how to reduce address translation overheads for such applications. We observe that many of these concurrent page walk requests, while irregular from the perspective of a single GPU wavefront, still fall on neighboring virtual page addresses. The address mappings for these neighboring pages are typically stored in the same 64-byte cache line. Since cache lines are the smallest granularity of memory access, the page table walker implicitly reads address mappings (i.e., page table entries or PTEs) of many neighboring pages during the page walk of a single virtual address (VA). However, in the conventional hardware, mappings not associated with the original request are simply discarded. In this work, we propose mechanisms to coalesce the address translation needs of all pending page table walks in the same neighborhood that happen to have their address mappings fall on the same cache line. This is almost free; the page table walker (PTW) already reads a full cache line containing address mappings of all pages in the same neighborhood. We find this simple scheme can reduce the number of accesses to the inmemory page table by 37% on average. This speeds up a set of GPU workloads by an average of 1.7×.

---
### FineReg: Fine-Grained Register File Management for Augmenting GPU Throughput.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00037
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574554
* **Key Words**: Registers, Graphics processing units, Instruction sets, Throughput, Hardware, System-on-chip, Context, file organisation, graphics processing units, multi-threading, parallel architectures, fine-grained register file management, GPU throughput, graphics processing units, hardware resources, parallel thread executions, runtime, concurrent CTAs, monolithic register file, live registers, pending-CTA region, cooperative thread arrays, GPU architecture, FineReg, CTA registers, kilobytes, context switching, GPU, Register File, Thread-Level Parallelism, Performance, 
* **Abstract**: Graphics processing units (GPUs) include a large amount of hardware resources for parallel thread executions. However, the resources are not fully utilized during runtime, and observed throughput often falls far below the peak performance. A major cause is that GPUs cannot deploy enough number of warps at runtime. The limited size of register file constrains the number of cooperative thread arrays (CTAs) as one CTA takes up a few tens of kilobytes of registers. We observe that the actual working set size of a CTA is much smaller in general, and therefore there is room for additional CTAs to run. In this paper, we propose a novel GPU architecture called FineReg that improves overall throughput by increasing the number of concurrent CTAs. In particular, FineReg splits the monolithic register file into two regions, one for active CTAs and another for pending CTAs. Using FineReg, the GPU begins normal executions by allocating all registers required by active CTAs. If all warps of a CTA become stalled, FineReg moves the live registers (i.e., working set) of CTA to the pending-CTA region and launches an additional CTA by assigning registers to the newly activated CTA. If the registers of either active or pending-CTA region are used up, FineReg stops introducing additional CTAs and simply performs context switching between active and pending CTAs. Thus, FineReg increases the number of concurrent CTAs by reducing the effective size of per-CTA registers. Experiment results show that FineReg achieves 32.8% of performance improvement over a conventional GPU architecture.

---
### In-Register Parameter Caching for Dynamic Neural Nets with Virtual Persistent Processor Specialization.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00038
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574555
* **Key Words**: Kernel, Neural networks, Training, Registers, Graphics processing units, Computational modeling, Shape, cache storage, graphics processing units, learning (artificial intelligence), neural chips, operating system kernels, dynamic neural networks, static net training, natural language processing, computation graphs, in-register parameter caching, virtual persistent processor specialization, input-induced network structure, kernel CTAs, DyNet, function calls, Volta microarchitecture, dynamic net requirements, off-chip loads, CISC-like vector processors, forward-backward propagation kernel, VPPS, weight matrices, GPU SMs, GPU registers, GPU, Deep Learning, Neural Network, Dynamic Neural Network, Persistent, Specialization, Register, 
* **Abstract**: Dynamic neural networks enable higher representation flexibility compared to networks with a fixed architecture and are extensively deployed in problems dealing with varying input-induced network structure, such as those in Natural Language Processing. One of the standard optimizations used in static net training is persistency of recurrent weights on the chip. In dynamic nets, possibly-inhomogeneous computation graph for every input prevents caching recurrent weights in GPU registers. Therefore, existing solutions suffer from excessive recurring off-chip memory loads as well as compounded kernel launch overheads leading to underutilization of GPU SMs. In this paper, we present a software system that enables persistency of weight matrices during the training of dynamic neural networks on the GPU. Before the training begins, our approach named Virtual Persistent Processor Specialization (VPPS) specializes a forward-backward propagation kernel that contains in-register caching and operation routines. VPPS virtualizes persistent kernel CTAs as CISC-like vector processors that can be guided to execute supplied instructions. VPPS greatly reduces the overall amount of off-chip loads by caching weight matrices on the chip, while simultaneously, provides maximum portability as it does not make any assumptions about the shape of the given computation graphs hence fulfilling dynamic net requirements. We implemented our solution on DyNet and abstracted away its design complexities by providing simple function calls to the user. Our experiments on a Volta micro-architecture shows that, unlike the most competitive solutions, VPPS shows excellent performance even in small batch sizes and delivers up to 6x speedup on training dynamic nets.

---
### Voltage-Stacked GPUs: A Control Theory Driven Cross-Layer Solution for Practical Voltage Stacking in GPUs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00039
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574556
* **Key Words**: Graphics processing units, Voltage control, Stacking, Reliability, Power system reliability, Handheld computers, Regulators, graphics processing units, integrated circuit design, integrated circuit interconnections, integrated circuit reliability, low-power electronics, microprocessor chips, multiprocessing systems, power aware computing, voltage regulators, power delivery subsystems, power delivery path, circuit-level voltage regulation, distributed charge-recycling integrated voltage regulators, architecture-level voltage smoothing, voltage-stacked GPU, voltage stacking, PCB board, microprocessor chip, series-stacking multiple voltage domain, voltage conversion, power delivery efficiency, throughput-centric manycore architectures, Voltage Stack, Charge Recycle, Multi Story, GPU, Power Delivery System, Power Delivery Efficiency, Integrated Voltage Regulator, Architecture Support, Instruction Issue Width, Power Management, 
* **Abstract**: More than 20% of the available energy is lost in "the last centimeter" from the PCB board to the microprocessor chip due to inherent inefficiencies of power delivery subsystems (PDSs) in today's computing systems. By series-stacking multiple voltage domains to eliminate explicit voltage conversion and reduce loss along the power delivery path, voltage stacking (VS) is a novel configuration that can improve power delivery efficiency (PDE). However, VS suffers from aggravated levels of supply noise caused by current imbalance between the stacking layers, preventing its practical adoption in mainstream computing systems. Throughput-centric manycore architectures such as GPUs intrinsically exhibit more balanced workloads, yet suffer from lower PDE, making them ideal platforms to implement voltage stacking. In this paper, we present a cross-layer approach to practical voltage stacking implementation in GPUs. It combines circuit-level voltage regulation using distributed charge-recycling integrated voltage regulators (CR-IVRs) with architecture-level voltage smoothing guided by control theory. Our proposed voltage-stacked GPUs can eliminate 61.5% of total PDS energy loss and achieve 92.3% system-level power delivery efficiency, a 12.3% improvement over the conventional single-layer based PDS. Compared to the circuit-only solution, the cross-layer approach significantly reduces the implementation cost of voltage stacking (88% reduction in area overhead) without compromising supply reliability under worst-case scenarios and across a wide range of real-world benchmarks. In addition, we demonstrate that the cross-layer solution not only complements on-chip CR-IVRs to transparently manage current imbalance and restore stable layer voltages, but also serves as a seamless interface to accommodate higher-level power optimization techniques, traditionally thought to be incompatible with a VS configuration.

---
### Osiris: A Low-Cost Mechanism to Enable Restoration of Secure Non-Volatile Memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00040
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574557
* **Key Words**: Microarchitecture, cache storage, cryptography, DRAM chips, error correction codes, memory architecture, Osiris, low-cost mechanism, nonvolatile memories, mainstream computing market, NVM-equipped computing systems, processor vendors, security primitive, commodity processors, memory encryption, secure architectures, encryption counters, functional recovery, interrupted system, memory controller design, memory-intensive workloads, error-correction codes, restoration, Spectre attacks, Meltdown attacks, write endurance problem, cryptographic metadata, ECCs, US department of energy proxy applications, DoE, write-through counter-cache scheme, Secure Architecture, Non Volatile Memory, ECC, Computer Architecture, 
* **Abstract**: With Non-Volatile Memories (NVMs) beginning to enter the mainstream computing market, it is time to consider how to secure NVM-equipped computing systems. Recent Meltdown and Spectre attacks are evidence that security must be intrinsic to computing systems and not added as an afterthought. Processor vendors are taking the first steps and are beginning to build security primitives into commodity processors. One security primitive that is associated with the use of emerging NVMs is memory encryption. Memory encryption, while necessary, is very challenging when used with NVMs because it exacerbates the write endurance problem. Secure architectures use cryptographic metadata that must be persisted and restored to allow secure recovery of data in the event of power-loss. Specifically, encryption counters must be persistent to enable secure and functional recovery of an interrupted system. However, the cost of ensuring and maintaining persistence for these counters can be significant. In this paper, we propose a novel scheme to maintain encryption counters without the need for frequent updates. Our new memory controller design, Osiris, repurposes memory Error-Correction Codes (ECCs) to enable fast restoration and recovery of encryption counters. To evaluate our design, we use Gem5 to run eight memory-intensive workloads selected from SPEC2006 and U.S. Department of Energy (DoE) proxy applications. Compared to a write-through counter-cache scheme, on average, Osiris can reduce 48.7% of the memory writes (increase lifetime by 1.95x), and reduce the performance overhead from 51.5% (for write-through) to only 5.8%. Furthermore, without the need for backup battery or extra power-supply hold-up time, Osiris performs better than a battery-backed write-back (5.8% vs. 6.6% overhead) and has less write-traffic (2.6% vs. 5.9% overhead).

---
### Morphable Counters: Enabling Compact Integrity Trees For Low-Overhead Secure Memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00041
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574558
* **Key Words**: Encryption, Vegetation, Memory management, System-on-chip, Data integrity, cache storage, cryptography, microprocessor chips, storage management, energy-delay product, morphable counters, encryption, cacheable on-chip, performance overheads, integrity tree design, replay attacks, man-in-the-middle attacks, off-chip main memory, counter-overflows, split counters, Memory Security, Replay Attack, Merkle Tree, MAC, Intel SGX, Split Counters, Encryption, Compression, 
* **Abstract**: Securing off-chip main memory is essential for protection from adversaries with physical access to systems. However, current secure-memory designs incur considerable performance overheads - a major cause being the multiple memory accesses required for traversing an integrity-tree, that provides protection against man-in-the-middle attacks or replay attacks. In this paper, we provide a scalable solution to this problem by proposing a compact integrity tree design that requires fewer memory accesses for its traversal. We enable this by proposing new storage-efficient representations for the counters used for encryption and integrity-tree in secure memories. Our Morphable Counters are more cacheable on-chip, as they provide more counters per cacheline than existing split counters. Additionally, they incur lower overheads due to counter-overflows, by dynamically switching between counter representations based on usage pattern. We show that using Morphable Counters enables a 128-ary integrity-tree, that can improve performance by 6.3% on average (up to 28.3%) and reduce system energy-delay product by 8.8% on average, compared to an aggressive baseline using split counters with a 64-ary integrity-tree. These benefits come without any additional storage or reduction in security and are derived from our compact counter representation, that reduces the integrity-tree size for a 16GB memory from 4MB in the baseline to 1MB. Compared to recently proposed VAULT, our design provides a speedup of 13.5% on average (up to 47.4%).

---
### InvisiSpec: Making Speculative Execution Invisible in the Cache Hierarchy.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00042
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574559
* **Key Words**: Hardware, Load modeling, Receivers, Coherence, Security, Monitoring, Transient analysis, cache storage, cryptography, microprocessor chips, multiprocessing systems, parallel architectures, program compilers, speculative execution invisible, side channel attacks, speculative execution attacks, microarchitectural state, hardware speculation attacks, InvisiSpec blocks microarchitectural, multiprocessor data cache hierarchy, unsafe speculative loads, speculative buffer, futuristic attacks, Spectre attacks, execution slowdown, memory consistency, hardware security, speculation, side channel, memory hierarchy, 
* **Abstract**: Hardware speculation offers a major surface for micro-architectural covert and side channel attacks. Unfortunately, defending against speculative execution attacks is challenging. The reason is that speculations destined to be squashed execute incorrect instructions, outside the scope of what programmers and compilers reason about. Further, any change to micro-architectural state made by speculative execution can leak information. In this paper, we propose InvisiSpec, a novel strategy to defend against hardware speculation attacks in multiprocessors by making speculation invisible in the data cache hierarchy. InvisiSpec blocks micro-architectural covert and side channels through the multiprocessor data cache hierarchy due to speculative loads. In InvisiSpec, unsafe speculative loads read data into a speculative buffer, without modifying the cache hierarchy. When the loads become safe, InvisiSpec makes them visible to the rest of the system. InvisiSpec identifies loads that might have violated memory consistency and, at this time, forces them to perform a validation step. We propose two InvisiSpec designs: one to defend against Spectre-like attacks and another to defend against futuristic attacks, where any speculative load may pose a threat. Our simulations with 23 SPEC and 10 PARSEC workloads show that InvisiSpec is effective. Under TSO, using fences to defend against Spectre attacks slows down execution by 74% relative to a conventional, insecure processor; InvisiSpec reduces the execution slowdown to only 21%. Using fences to defend against futuristic attacks slows down execution by 208%; InvisiSpec reduces the slowdown to 72%.

---
### Improving the Performance and Endurance of Encrypted Non-Volatile Main Memory through Deduplicating Writes.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00043
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574560
* **Key Words**: Nonvolatile memory, Encryption, Metadata, System performance, Computer security, cache storage, cryptography, DRAM chips, storage management, DeWrite, encrypted NVMs, memory writes, memory encryption, NVM wear-out, in-line deduplication technique, nonvolatile main memory, deduplication-aware scheme, IPC, cache-line-level duplications, NVMain, SPEC CPU2006, PARSEC, deduplication encryption, deduplicating writes, Non volatile memory, Memory encryption, Deduplication, 
* **Abstract**: Non-volatile memory (NVM) technologies are considered as promising candidates of the next-generation main memory. However, the non-volatility of NVMs leads to new security vulnerabilities. For example, it is not difficult to access sensitive data stored on stolen NVMs. Memory encryption can be employed to mitigate the security vulnerabilities, but it increases the number of bits written to NVMs due to the diffusion property and thereby aggravates the NVM wear-out induced by writes. To address these security and endurance challenges, this paper proposes DeWrite, a secure and deduplication-aware scheme to enhance the performance and endurance of encrypted NVMs based on a new in-line deduplication technique and the synergistic integrations of deduplication and memory encryption. Specifically, it performs low-latency in-line deduplication to exploit the abundant cache-line-level duplications leveraging the intrinsic read/write asymmetry of NVMs and light-weight hashing. It also opportunistically parallelizes the operations of deduplication and encryption and allows them to co-locate the metadata for high time and space efficiency. DeWrite was implemented on the gem5 with NVMain and evaluated using 20 applications from SPEC CPU2006 and PARSEC. Extensive experimental results demonstrate that DeWrite reduces on average 54% writes to encrypted NVMs, and speeds up memory writes and reads of encrypted NVMs by 4.2 × and 3.1 ×, respectively. Meanwhile, DeWrite improves the system IPC by 82% and reduces 40% of energy consumption on average.

---
### SSDcheck: Timely and Accurate Prediction of Irregular Behaviors in Black-Box SSDs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00044
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574561
* **Key Words**: Performance evaluation, Throughput, Resource management, Predictive models, Quality of service, Feature extraction, Adaptation models, cache storage, flash memories, input-output programs, NAND circuits, performance evaluation, random-access storage, storage management, SSDcheck, static feature parameters, target SSD, on-average prediction accuracy, high-latency predictions, tail latency reduction, timely prediction, irregular behaviors, modern servers, fast storage device, complex devices, device-specific goals, unpredictable irregular access latency, server architects, quality-of-service requirements, performance potential, key performance-critical features, general SSD performance model, solid-state drives, real-world SSD, commodity black-box SSD, irregular inter-intra-SSD access latencies, unpredictable inter-intra-SSD access latencies, Performance Modeling, SSD, Storage System, 
* **Abstract**: Modern servers are actively deploying Solid-State Drives (SSDs). However, rather than just a fast storage device, SSDs are complex devices designed for device-specific goals (e.g., latency, throughput, endurance, cost) with their internal mechanisms undisclosed to users as the proprietary asset, which leads to unpredictable, irregular inter/intra-SSD access latencies. This unpredictable irregular access latency has been a fundamental challenge to server architects aiming to satisfy critical quality-of-service requirements and/or achieve the full performance potential of commodity SSDs. In this paper, we propose SSDcheck, a novel SSD performance model to accurately predict the latency of next access to commodity black-box SSDs. First, after analyzing a wide spectrum of real-world SSDs, we identify key performance-critical features (e.g., garbage collection, write buffering) required to construct a general SSD performance model. Next, SSDcheck runs diagnosis code snippets to extract static feature parameters (e.g., size, threshold) from the target SSD, and constructs its performance model. Finally, during runtime, SSDcheck dynamically manages the performance model to predict the latency of the next access. Our evaluations show that SSDcheck achieves up to 98.96% and 79.96% on-average prediction accuracy for normal-latency and high-latency predictions, respectively. Next, we show the effectiveness of SSDcheck by implementing a new volume manager improving the throughput by up to 4.29x with the tail latency reduction down to 6.53%, and a new I/O request handler improving the throughput by up to 44.0% with the tail latency reduction down to 26.9%. We then show how to further improve the results of scheduling with the help of an emerging Non-Volatile Memory (e.g., PCM). SSDcheck does not require any hardware modifications, which can be harmlessly disabled for any SSDs uncovered by the performance model.

---
### Amber*: Enabling Precise Full-System Simulation with Detailed Modeling of All SSD Resources.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00045
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574562
* **Key Words**: Data models, Nonvolatile memory, Computational modeling, Hardware, Computer architecture, Software, Data transfer, cache storage, computer architecture, embedded systems, firmware, flash memories, memory architecture, multiprocessing systems, operating systems (computers), power aware computing, system-on-chip, Amber, enabling precise full-system simulation, SSD resources, storage component, modern memory hierarchies, SSD research, future simulation-based studies, SSD subsystems, full-system environment, full-system simulations, necessary hardware, hardware components, multiple device controllers, internal caches, software modules, storage interfaces, storage subsystems, host system, SSD simulation framework, system simulation environment, DRAM cache logic, flash firmware, diverse standard protocols, system buses, popular full system simulator, functional timing CPU models, OS systems, hardware platforms, system-level challenges, passive storage, active storage architectures, Full-System Simulator, Solid State Drive, Non Volatile Memory, Memory System, Storage, Flash Memory, 
* **Abstract**: SSDs become a major storage component in modern memory hierarchies, and SSD research demands exploring future simulation-based studies by integrating SSD subsystems into a full-system environment. However, several challenges exist to model SSDs under a full-system simulations; SSDs are composed upon their own complete system and architecture, which employ all necessary hardware, such as CPUs, DRAM and interconnect network. Employing the hardware components, SSDs also require to have multiple device controllers, internal caches and software modules that respect a wide spectrum of storage interfaces and protocols. These SSD hardware and software are all necessary to incarnate storage subsystems under full-system environment, which can operate in parallel with the host system. In this work, we introduce a new SSD simulation framework, SimpleSSD 2.0, namely Amber, that models embedded CPU cores, DRAMs, and various flash technologies (within an SSD), and operate under the full system simulation environment by enabling a data transfer emulation. Amber also includes full firmware stack, including DRAM cache logic, flash firmware, such as FTL and HIL, and obey diverse standard protocols by revising the host DMA engines and system buses of a popular full system simulator's all functional and timing CPU models (gem5). The proposed simulator can capture the details of dynamic performance and power of embedded cores, DRAMs, firmware and flash under the executions of various OS systems and hardware platforms. Using Amber, we characterize several system-level challenges by simulating different types of full-systems, such as mobile devices and general-purpose computers, and offer comprehensive analyses by comparing passive storage and active storage architectures.

---
### Invalid Data-Aware Coding to Enhance the Read Performance of High-Density Flash Memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00046
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574563
* **Key Words**: Encoding, Microprocessors, Threshold voltage, Error correction codes, Reliability, Arrays, flash memories, solid state drives, bit invalidation, read-intensive workloads, read response times, read performance, high-density flash memories, high bit-density flash memories, MultiLevel Cell, cell bit-density, TLC cell, long read latencies, Invalid Data-Aware coding, read variation problem, triple-level cell, IDA coding, TLC-based SSD, middle bit read latencies, Flash Coding, High Density Flash, Read Latency, Data Refresh, 
* **Abstract**: High bit-density flash memories such as Multi-Level Cell (MLC) and Triple-Level Cell (TLC) flash have become a norm, since doubling the cell bit-density can increase the storage capacity by 2× using the same number of cells. However, these high bit-density flash memories suffer from the read variation problem - e.g., the three kinds of bits (i.e., lower, middle, and upper bits) in a TLC cell have different read latencies; reading an upper bit takes a longer time than reading a middle bit, and reading a lower bit takes the minimum time. In this paper, we note that, in the conventional coding, the long read latencies of the middle and upper bits are not reduced even after the lower bit value is invalidated (i.e., no longer used). Motivated by this problem with the traditional coding, we propose a new coding technique, called Invalid Data-Aware (IDA) coding, which reduces the upper and middle bit read latencies close to the lower bit read latency when the lower bit becomes invalid. The main strategy the IDA coding employs is to merge the duplicated voltage states coming from the bit invalidation and reduce the number of (read) trials to identify the voltage state of a cell. To hide the performance and reliability degradation caused by the application of the IDA coding, we also propose to implement it as a part of the data refresh function, which is a fundamental operation in modern SSDs to keep its data safer and longer. With an extensive analysis of a TLC-based SSD using a variety of read-intensive workloads, we report that our IDA coding improves the read response times by 28%, on average; it is also quite effective in devices with different bit densities and timing parameters.

---
### Persistence Parallelism Optimization: A Holistic Approach from Memory Bus to RDMA Network.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00047
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574564
* **Key Words**: Instruction sets, Memory management, Nonvolatile memory, Bars, Microarchitecture, Micromechanical devices, Scheduling, cache storage, multi-threading, optimisation, phase change memories, data persistence, persistent memory system, memory controller, data path, RDMA network, inter-thread persistence parallelism, bank-level parallelism, persistence parallelism optimization, data services, nonvolatile memories, remote direct memory access network, NVM, phase change memory, PCM, resistive RAM, ReRAM, byte-addressability, cache hierarchy, remote node, barrier epoch management, BLP, remote memory systems, persistent memory systems, memory persistency, RDMA, 
* **Abstract**: Emerging non-volatile memories (NVM), such as phase change memory (PCM) and Resistive RAM (ReRAM), incorporate the features of fast byte-addressability and data persistence, which are beneficial for data services such as file systems and databases. To support data persistence, a persistent memory system requires ordering for write requests. The datapath of a persistent request consists of three segments: through the cache hierarchy to the memory controller, through the bus from the memory controller to memory devices, and through the network from a remote node to a local node. Previous work contributes significantly to improve the persistence parallelism in the first segment of the data path. However, we observe that the memory bus and the Remote Direct Memory Access (RDMA) network remain severely under-utilized because the persistence parallelism in these two segments is not fully leveraged during ordering. In this paper, we propose a novel architecture to further improve the persistence parallelism in the memory bus and the RDMA network. First, we utilize inter-thread persistence parallelism for barrier epoch management with better bank-level parallelism (BLP). Second, we enable intra-thread persistence parallelism for remote requests through RDMA network with buffered strict persistence. With these features, the architecture efficiently supports persistence through all three segments of the write datapath. Experimental results show that for local applications, the proposed mechanism can achieve 1.3× performance improvement, compared to the original buffered persistence work. In addition, it can achieve 1.93× performance improvement for remote applications serviced through the RDMA network.

---
### PiCL: A Software-Transparent, Persistent Cache Log for Nonvolatile Main Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00048
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574565
* **Key Words**: Nonvolatile memory, Computer crashes, Checkpointing, System-on-chip, Random access memory, Software, Field programmable gate arrays, field programmable gate arrays, microprocessor chips, random-access storage, OpenPiton framework, Verilog, FPGA prototype, big on-chip caches, hardware write-ahead logging schemes, software-transparent crash consistency, cache log, asynchronous cache-scan, cache-driven logging, multiundo logging, WAL checkpointing mechanism, PiCL, random-access performance, multicore systems, error-prone transactional semantics, nonvolatile main memory, cache memory, nonvolatile memory, parallel processing, computer crashes, checkpointing, 
* **Abstract**: Software-transparent crash consistency is a promising direction to immediately reap the benefits of nonvolatile main memory (NVMM) without encumbering programmers with error-prone transactional semantics. Unfortunately, proposed hardware write-ahead logging (WAL) schemes have high performance overhead, particularly for multi-core systems with many threads and big on-chip caches and NVMs with low random-access performance. This paper proposes PiCL, a new WAL checkpointing mechanism that provides a low overhead, software-transparent crash consistency solution for NVMM. PiCL introduces multi-undo logging, cache-driven logging, and asynchronous cache-scan to reduce random accesses and enable good row locality at the NVM. The key idea is that: by relaxing the durability timing of checkpoints, crash consistency can be provided with less than 1% performance overhead where 1.5x to 5.0x slowdown was typical with prior work. To demonstrate the feasibility of software-transparent crash consistency, we fully implemented PiCL as an FPGA prototype in Verilog using the OpenPiton framework.

---
### Efficient Hardware-Assisted Logging with Asynchronous and Direct-Update for Persistent Memory.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00049
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574566
* **Key Words**: Nonvolatile memory, Random access memory, Writing, Optimization, Degradation, Space exploration, Software, cache storage, database management systems, DRAM chips, system monitoring, transaction processing, performance degradation, undo+redo approach, HW-assisted logging techniques, write-ahead log technique, hardware-assisted logging, logging mechanism, prior redo approach, in-place data update, synchronous inplace data updates, data writes, transaction execution, log writing, HW log manager, undo+redo principles, nonvolatile memory, atomic durability support, data consistency, persistent memory, direct-update, Atomicity, Durability, Write ahead Logging, Persistent Memory, 
* **Abstract**: Supporting atomic durability in emerging persistent memory requires data consistency across potential system failures. For atomic durability support in the non-volatile memory, the traditional write-ahead log (WAL) technique has been employed to guarantee the persistency of logs before actual data updates. Based on the WAL mechanism, recent studies proposed HWassisted logging techniques with undo, redo, or undo+redo principles. The HW log manager allows the overlapping of log writing and transaction execution, as long as the atomicity invariant can be satisfied. Although the efficiency of both log and data writes must be optimized, the prior work exhibit trade-offs in performance under various access patterns. The undo approach experiences performance degradation due to synchronous inplace data updates since the log contains only the old values. On the other hand, the undo+redo approach stores both old and new values, and does not require synchronous in-place data updates. However, the larger log size increases the amount of log writes. The prior redo approach demands extra NVM read bandwidth for indirectly updating in-place data from the new values in logs. To overcome the limitations of the previous approaches, this paper proposes a novel redo-based logging (ReDU), which performs direct and asynchronous in-place data update to NVM. ReDU exploits a small region of DRAM as a write-cache to remove NVM writes from the critical path. The experimental results show that the proposed logging mechanism provides the best performance under a variety of write patterns, showing 8.6%, 14.2%, and 23.6% better performance compared to the previous undo, redo, and undo+redo approaches, respectively.

---
### CHAMELEON: A Dynamically Reconfigurable Heterogeneous Memory System.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00050
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574567
* **Key Words**: Random access memory, Bandwidth, Sockets, Linux, Hardware, Computer architecture, Task analysis, cache storage, DRAM chips, hardware-software codesign, operating systems (computers), computing systems, physical memory capacity, OS-visible memory capacity, bandwidth, OS-managed part of memory, hybrid architecture, hardware-software co-designed system, switching memory regions, multi-programmed workloads, latency-optimized cache, hardware-managed cache, Chameleon, PoM architectures, DRAM cache, die-stacked memories, dynamically reconfigurable heterogeneous memory system, Stacked DRAM, Heterogeneous memory system, HBM, Cache, Hardware-software co-design, 
* **Abstract**: Modern computing systems and applications have growing demand for memories with higher bandwidth. This demand can be alleviated using fast, large on-die or die-stacked memories. They are typically used with traditional DRAM as part of a heterogeneous memory system and used either as a DRAM cache or as a hardware-or OS-managed part of memory (PoM). Caches adapt rapidly to application needs and typically provide higher performance but reduce the total OS-visible memory capacity. PoM architectures increase the total OS-visible memory capacity but exhibit additional overheads due to swapping large blocks of data between fast and slow memory. In this paper, we propose Chameleon, a hybrid architecture that bridges the gap between cache and PoM architectures. When applications need a large memory, Chameleon uses both fast and slow memories as PoM, maximizing the available space for the application. When the application's footprint is smaller than the total physical memory capacity, Chameleon opportunistically uses free space in the system as a hardware-managed cache. Chameleon is a hardware-software co-designed system where the OS notifies the hardware of pages that are allocated or freed, and hardware decides on switching memory regions between PoM-and cache-modes dynamically. Based on our evaluation of multi-programmed workloads on a system with 4GB fast memory and 20GB slow memory, Chameleon improves the average performance by 11.6% over PoM and 24.2% over a latency-optimized cache.

---
### Compresso: Pragmatic Main Memory Compression.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00051
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574568
* **Key Words**: Metadata, Memory management, Hardware, Bandwidth, Optimization, Resource management, Aerospace electronics, computer centres, data compression, memory architecture, multiprocessing systems, operating systems (computers), optimisation, single-core systems, multicore systems, competitive compressed systems, Compresso, pragmatic main memory compression, energy efficiency, datacenter applications, operating system, hardware memory compression architecture, data-movement trade-offs, client applications, holistic evaluation, memory movement, Memory, Main memory, compression, main memory compression, compression architecture, hardware compression, memory capacity, 
* **Abstract**: Today, larger memory capacity and higher memory bandwidth are required for better performance and energy efficiency for many important client and datacenter applications. Hardware memory compression provides a promising direction to achieve this without increasing system cost. Unfortunately, current memory compression solutions face two significant challenges. First, keeping memory compressed requires additional memory accesses, sometimes on the critical path, which can cause performance overheads. Second, they require changing the operating system to take advantage of the increased capacity, and to handle incompressible data, which delays deployment. We propose Compresso, a hardware memory compression architecture that minimizes memory overheads due to compression, with no changes to the OS. We identify new data-movement trade-offs and propose optimizations that reduce additional memory movement to improve system efficiency. We propose a holistic evaluation for compressed systems. Our results show that Compresso achieves a 1.85x compression for main memory on average, with a 24% speedup over a competitive hardware compressed system for single-core systems and 27% for multi-core systems. As compared to competitive compressed systems, Compresso not only reduces performance overhead of compression, but also increases performance gain from higher memory capacity.

---
### Farewell My Shared LLC! A Case for Private Die-Stacked DRAM Caches for Servers.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00052
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574569
* **Key Words**: Random access memory, Servers, Program processors, System-on-chip, Sensitivity, Delays, Web search, cache storage, DRAM chips, multiprocessing systems, farewell my shared LLC, conventional CPU architectures, higher performance, last-level caches, server processors, on-chip area constraints, storage capacity, long planar interconnect, shared cache capacity, workload colocation, Die-Stacked Private LLC Organization, SILO, planar wire spans, private caches, inter-core cache contention, traditional workloads, strong performance isolation, shared LLC, conventional cache architectures, low access delays, die-stacked DRAM caches, last-level cache, private cache, DRAM, die stacking, server workloads, 
* **Abstract**: The slowdown in technology scaling mandates rethinking of conventional CPU architectures in a quest for higher performance and new capabilities. This work takes a step in this direction by questioning the value of on-chip shared last-level caches (LLCs) in server processors and argues for a better alternative. Shared LLCs have a number of limitations, including on-chip area constraints that limit storage capacity, long planar interconnect spans that increase access latency, and contention for the shared cache capacity that hurts performance under workload colocation. To overcome these limitations, we propose a Die-Stacked Private LLC Organization (SILO), which combines conventional on-chip private L1 (and optionally, L2) caches with a per-core private LLC in die-stacked DRAM. By stacking LLC slices directly above each core, SILO avoids long planar wire spans. The use of private caches inherently avoids inter-core cache contention. Last but not the least, engineering the DRAM for latency affords low access delays while still providing over 100MB of capacity per core in today's technology. Evaluation results show that SILO outperforms state-of-the-art conventional cache architectures on a range of scale-out and traditional workloads while delivering strong performance isolation under colocation.

---
### Leveraging CPU Electromagnetic Emanations for Voltage Noise Characterization.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00053
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574570
* **Key Words**: Voltage measurement, Resonant frequency, System-on-chip, Frequency measurement, Genetic algorithms, Stress, Oscillators, genetic algorithms, integrated circuit design, integrated circuit noise, integrated circuit testing, microprocessor chips, first-order resonance-frequency, genetic-algorithm, voltage noise stress tests, spectrum analyzer, nonintrusive zero-overhead approach, power-delivery LC tank network, post-silicon dI-dt voltage noise generation, CPU electromagnetic emanation sensing, zero-overhead approach, run-time overheads, expensive pad resources, on-chip dedicated circuitry, on-package measurement points, direct voltage measurements, voltage noise characterization, direct voltage noise measurements, ×86-64 AMD desktop CPU, ARM multicore mobile CPU clusters, electromagnetic signal amplitude, voltage noise stress, high voltage noise, high amplitude electromagnetic emanations, voltage-noise, dI/dt, stress-tests, Electromagnetic-emanations, resonance-frequency, power-delivery-networks, genetic algorithms, monitoring, methodology, characterization, 
* **Abstract**: Worst-case dI/dt voltage noise is typically characterized post-silicon using direct voltage measurements through either on-package measurement points or on-chip dedicated circuitry. These approaches consume expensive pad resources or suffer from design-time and run-time overheads. This work proposes an alternative non-intrusive, zero-overhead approach for post-silicon dI/dt voltage noise generation based on sensing CPU electromagnetic emanations using an antenna and a spectrum analyzer. The approach is based on the observation that high amplitude electromagnetic emanations are correlated with high voltage noise. We leverage this observation to automatically generate voltage noise (dI/dt) stress tests with a genetic-algorithm that is driven by electromagnetic signal amplitude and to obtain the first-order resonance-frequency of the Power-Delivery LC-tank network. The generality of the approach is established by successfully applying it to three different CPUs: two ARM multi-core mobile CPU clusters hosted on a big.LITTLE configuration and an ×86-64 AMD desktop CPU. The efficacy of the proposed methodology is validated through VMIN and direct voltage noise measurements.

---
### RpStacks-MT: A High-Throughput Design Evaluation Methodology for Multi-Core Processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00054
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574571
* **Key Words**: Analytical models, Multicore processing, Space exploration, Throughput, Load modeling, Predictive models, Microarchitecture, graph theory, logic design, microprocessor chips, multiprocessing systems, performance evaluation, art design exploration, processor design, graph model, reuse distance-based memory system model, graph-based multicore performance model, multicore processor designs, computer architects, multicore processors, RpStacks-MT, Design space exploration, Performance analysis, Simulation, 
* **Abstract**: Computer architects put significant efforts on the design space exploration of a new processor, as it determines the overall characteristics (e.g., performance, power, cost) of the final product. To thoroughly explore the space and achieve the best results, they need high design evaluation throughput - the ability to quickly assess a large number of designs with minimal costs. Unfortunately, the existing simulators and performance models are either too slow or too inaccurate to meet this demand. As a result, architects often sacrifice the design space coverage to end up with a sub-optimal product. To address this challenge, we propose RpStacks-MT, a methodology to evaluate multi-core processor designs with high throughput. First, we propose a graph-based multi-core performance model, which overcomes the limitations of the existing models to accurately describe a multi-core processor's key performance behaviors. Second, we propose a reuse distance-based memory system model and a dynamic scheduling reconstruction method, which help our graph model to quickly track the performance changes from processor design changes. Lastly, we combine these models with a state of the art design exploration idea to evaluate multiple processor designs in an efficient way. Our evaluations show that RpStacks-MT achieves extremely high design evaluation throughput - 88× higher versus a conventional cycle-level simulator and 18× higher versus an accelerated simulator (on average, for evaluating 10,000 designs) - while maintaining simulator-level accuracy.

---
### The EH Model: Early Design Space Exploration of Intermittent Processor Architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00055
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574572
* **Key Words**: Nonvolatile memory, Computer architecture, Mathematical model, Space exploration, Power system reliability, Computational modeling, Task analysis, embedded systems, energy conservation, energy harvesting, logic design, low-power electronics, microprocessor chips, optimisation, space exploration, power outages, energy-harvesting model, energy-harvesting devices, intermittent processor architectures, intermittent processors, energy cost, intermittent system, intermittent computing, Energy harvesting, Intermittent computing, Analytical modelling, 
* **Abstract**: Energy-harvesting devices-which operate solely on energy collected from their environment-have brought forth a new paradigm of intermittent computing. These devices succumb to frequent power outages that would cause conventional systems to be stuck in a perpetual loop of restarting computation and never making progress. Ensuring forward progress in an intermittent execution model requires saving state in nonvolatile memory (backup) and potentially re-executing from the last saved state upon a power loss (restore). The interplay between spending energy on useful processing and spending energy on these necessary overheads yield unexpected trade-offs. To facilitate early design space exploration, the field of intermittent computing requires better models for 1) generalizing and reasoning about these trade-offs and 2) helping architects and programmers in making early-stage design decisions. We propose the EH model, which characterizes an intermittent system's ability to maximize how much of its available energy is spent on useful processor execution. The model parametrizes the energy costs associated with intermittent execution to allow an intuitive understanding of how forward progress can change. We use the EH model to explore how forward progress is impacted with the frequency of backups and the energy cost of backups and restores. We validate the EH model with hardware measurements on an MSP430 and characterize its parameters via simulation. We also demonstrate how architects and programmers can use the model to explore the design space of intermittent processors, derive insights, and model new optimizations that are unique to intermittent processor architectures.

---
### CounterMiner: Mining Big Performance Data from Hardware Counters.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00056
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574573
* **Key Words**: Hardware, Time series analysis, Program processors, Cloud computing, Microarchitecture, Data mining, Benchmark testing, benchmark testing, Big Data, cache storage, cloud computing, data analysis, data mining, learning (artificial intelligence), power aware computing, storage management, hardware performance counters, performance bottlenecks, CounterMiner, data mining, machine learning techniques, Big performance Data mining, Performance, big data, computer architecture, performance counters, data mining, 
* **Abstract**: Modern processors typically provide a small number of hardware performance counters to capture a large number of microarchitecture events. These counters can easily generate a huge amount (e.g., GB or TB per day) of data, which we call big performance data in cloud computing platforms with more than thousands of servers and millions of complex workloads running in a "24/7/365" manner. The big performance data provides a precious foundation for root cause analysis of performance bottlenecks, architecture and compiler optimization, and many more. However, it is challenging to extract value from the big performance data due to: 1) the many unperceivable errors (e.g., outliers and missing values); and 2) the difficulty of obtaining insights, e.g., relating events to performance. In this paper, we propose CounterMiner, a rigorous methodology that enables the measurement and understanding of big performance data by using data mining and machine learning techniques. It includes three novel components: 1) using data cleaning to improve data quality by replacing outliers and filling in missing values; 2) iteratively quantifying, ranking, and pruning events based on their importance with respect to performance; 3) quantifying interaction intensity between two events by residual variance. We use sixteen benchmarks (eight from CloudSuite and eight from the Spark version of HiBench) to evaluate CounterMiner. The experimental results show that CounterMiner reduces the average error from 28.3% to 7.7% when multiplexing 10 events on 4 hardware counters. We also conduct a real-world case study, showing that identifying important configuration parameters of Spark programs by event importance is much faster than directly ranking the importance of these parameters.

---
### Taming the Killer Microsecond.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00057
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574574
* **Key Words**: Hardware, Performance evaluation, Kernel, Prefetching, Servers, Random access memory, cache storage, DRAM chips, field programmable gate arrays, memory architecture, performance evaluation, software architecture, storage management, memory technologies, microsecond range, microsecond-level latencies, FPGA-based microsecond-latency device emulator, open-source data-intensive applications, on-demand memory accesses, access-level parallelism, track in-flight accesses, microsecond-level devices, software architectures, killer microsecond problem, software latency-hiding techniques, DRAM-based implementations, hardware architectures, Killer microseconds, Emerging storage, Data intensive applications, FPGA, 
* **Abstract**: Modern applications require access to vast datasets at low latencies. Emerging memory technologies can enable faster access to significantly larger volumes of data than what is possible today. However, these memory technologies have a significant caveat: their random access latency falls in a range that cannot be effectively hidden using current hardware and software latency-hiding techniques-namely, the microsecond range. Finding the root cause of this "Killer Microsecond" problem, is the subject of this work. Our goal is to answer the critical question of why existing hardware and software cannot hide microsecond-level latencies, and whether drastic changes to existing platforms are necessary to utilize microsecond-latency devices effectively. We use an FPGA-based microsecond-latency device emulator, a carefully-crafted microbenchmark, and three open-source data-intensive applications to show that existing systems are indeed incapable of effectively hiding such latencies. However, after uncovering the root causes of the problem, we show that simple changes to existing systems are sufficient to support microsecond-latency devices. In particular, we show that by replacing on-demand memory accesses with prefetch requests followed by fast user-mode context switches (to increase access-level parallelism) and enlarging hardware queues that track in-flight accesses (to accommodate many parallel accesses), conventional architectures can effectively hide microsecond-level latencies, and approach the performance of DRAM-based implementations of the same applications. In other words, we show that successful usage of microsecond-level devices is not predicated on drastically new hardware and software architectures.

---
### Adaptive Scheduling for Systems with Asymmetric Memory Hierarchies.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00058
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574575
* **Key Words**: Instruction sets, Multicore processing, Schedules, Bandwidth, Coherence, Random access memory, cache storage, multiprocessing systems, scheduling, high-quality thread-to-hierarchy mappings, AMS monitors threads, core types, cache partitioning, high-quality schedules, exhaustive-search scheduler, asymmetry-oblivious schedulers, adaptive scheduling, asymmetric memory hierarchies, conventional multicores, deep cache hierarchies, data movement, near-data processing systems, NDP cores, cheaper memory accesses, shallow cache hierarchies, shallow nor deep hierarchies, scheduling computation, adaptive scheduler, cache hierarchies, near-data processing, asymmetric systems, scheduling, analytical performance modeling, 
* **Abstract**: Conventional multicores rely on deep cache hierarchies to reduce data movement. Recent advances in die stacking have enabled near-data processing (NDP) systems that reduce data movement by placing cores close to memory. NDP cores enjoy cheaper memory accesses and are more area-constrained, so they use shallow cache hierarchies instead. Since neither shallow nor deep hierarchies work well for all applications, prior work has proposed systems that incorporate both. These asymmetric memory hierarchies can be highly beneficial, but they require scheduling computation to the right hierarchy. We present AMS, an adaptive scheduler that automatically finds high-quality thread-to-hierarchy mappings. AMS monitors threads, accurately models their performance under different hierarchies and core types, and adapts algorithms first proposed for cache partitioning to produce high-quality schedules. AMS is cheap enough to use online, so it adapts to program phases, and performs within 1% of an exhaustive-search scheduler. As a result, AMS outperforms asymmetry-oblivious schedulers by up to 37% and by 18% on average.

---
### Processing-in-Memory for Energy-Efficient Neural Network Training: A Heterogeneous Approach.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00059
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574576
* **Key Words**: Training, Hardware, Programming, Computational modeling, Artificial neural networks, Computer architecture, Runtime, learning (artificial intelligence), microprocessor chips, multiprocessing systems, neural nets, parallel processing, power aware computing, scheduling, software maintenance, energy-efficient neural network training, NNs, image classification, speech recognition, object detection, computer vision, deep neural networks, processor, memory access characteristics, processing-in-memory system, fix-function arithmetic units, 3D die-stacked memory, heterogeneous PIM architecture, CPU, OpenCL programming model, hardware heterogeneity-aware runtime system, heterogeneous hardware, system energy efficiency, fine-grained operations, software-hardware co-design, program portability, program maintenance, DNNs, ARM-based programmable cores, Processing in memory, Deep neural networks, Energy efficient, Heterogeneous architecture, Programming model, 
* **Abstract**: Neural networks (NNs) have been adopted in a wide range of application domains, such as image classification, speech recognition, object detection, and computer vision. However, training NNs - especially deep neural networks (DNNs) - can be energy and time consuming, because of frequent data movement between processor and memory. Furthermore, training involves massive fine-grained operations with various computation and memory access characteristics. Exploiting high parallelism with such diverse operations is challenging. To address these challenges, we propose a software/hardware co-design of heterogeneous processing-in-memory (PIM) system. Our hardware design incorporates hundreds of fix-function arithmetic units and ARM-based programmable cores on the logic layer of a 3D die-stacked memory to form a heterogeneous PIM architecture attached to CPU. Our software design offers a programming model and a runtime system that program, offload, and schedule various NN training operations across compute resources provided by CPU and heterogeneous PIM. By extending the OpenCL programming model and employing a hardware heterogeneity-aware runtime system, we enable high program portability and easy program maintenance across various heterogeneous hardware, optimize system energy efficiency, and improve hardware utilization.

---
### LerGAN: A Zero-Free, Low Data Movement and PIM-Based GAN Architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00060
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574577
* **Key Words**: Gallium nitride, Training, Generators, Generative adversarial networks, Mathematical model, Artificial neural networks, Acceleration, field programmable gate arrays, multiprocessing systems, neural nets, unsupervised learning, LerGAN, zero-free data movement, low data movement, PIM-based GAN architecture, powerful unsupervised learning method, breakthrough technologies, MIT Technology Review, complex train phases, inter-mediate data, PIM-based GAN accelerator, training GAN, ReRAM-based PIM, zero-related computations, FPGA-based GAN accelerator, ReRAM-based neural network accelerator, generative adversarial network, PIM, GAN, Zero free, 3D Connection, 
* **Abstract**: As a powerful unsupervised learning method, Generative Adversarial Network (GAN) plays an important role in many domains such as video prediction and autonomous driving. It is one of the ten breakthrough technologies in 2018 reported in MIT Technology Review. However, training a GAN imposes three more challenges: (1) intensive communication caused by complex train phases of GAN, (2) much more ineffectual computations caused by special convolutions, and (3) more frequent off-chip memory accesses for exchanging inter-mediate data between the generator and the discriminator. In this paper, we propose LerGAN, a PIM-based GAN accelerator to address the challenges of training GAN. We first propose a zero-free data reshaping scheme for ReRAM-based PIM, which removes the zero-related computations. We then propose a 3D-connected PIM, which can reconfigure connections inside PIM dynamically according to dataflows of propagation and updating. Our proposed techniques reduce data movement to a great extent, avoiding I/O to become a bottleneck of training GANs. Finally, we propose LerGAN based on these two techniques, providing different levels of accelerating GAN for programmers. Experiments shows that LerGAN achieves 47.2×, 21.42× and 7.46× speedup over FPGA-based GAN accelerator, GPU platform, and ReRAM-based neural network accelerator respectively. Moreover, LerGAN achieves 9.75×, 7.68× energy saving on average over GPU platform, ReRAM-based neural network accelerator respectively, and has 1.04× energy consuming over FPGA-based GAN accelerator.

---
### Multi-dimensional Parallel Training of Winograd Layer on Memory-Centric Architecture.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00061
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574578
* **Key Words**: Convolution, Parallel processing, Training, Transforms, Computer architecture, Topology, Neural networks, graphics processing units, neural nets, parallel processing, multidimensional parallel training, Winograd layer, memory-centric architecture, neural network training, convolution layer, intratile parallelism, dimension exploiting intra-tile parallelism, data parallelism dimension, data accesses, MDP architecture, tile transfer, Winograd domain feature maps, near-data processing architecture, memory-centric network organization, tile gathering communication, reconfigurable memory-centric network architecture, network channel connectivity, NDP architecture, convolutional neural networks, Convolutional Neural Network, Near data Processing, Memory centric Network, 
* **Abstract**: Accelerating neural network training is critical in exploring design space of neural networks. Data parallelism is commonly used to accelerate training for Convolutional Neural Networks (CNN) where input batch is distributed across the multiple workers; however, the increase in communication of weight gradients across the workers limits scalability. In this work, we propose multi-dimensional parallel (MDP) training of convolution layer by exploiting both data parallelism and intratile parallelism available in Winograd transformed convolution. Workers are organized across two dimensions - one dimension exploiting intra-tile parallelism while the other dimension exploits data parallelism. MDP reduces the amount of communication necessary for weight gradients since weight gradients are only communicated across the data parallelism dimension. However, Winograd transform fundamentally requires more data accesses and the proposed MDP architecture also introduces a new type of communication which we refer to as tile transfer - gather/scatter of Winograd domain feature maps (tiles). We propose a scalable near-data processing (NDP) architecture to minimize the cost of data accesses through 3D stacked memory while leveraging a memory-centric network organization to provide high-connectivity between the workers with intra-tile parallelism to accelerate tile transfer. To minimize tile gathering communication overhead, we exploit prediction of activation of spatial domain neurons in order to remove the communication of tiles that are transformed to non-activated neurons. In order to balance the communication required for weight gradients and tile transfer, we also propose a reconfigurable memory-centric network architecture that reconfigures network channel connectivity between the workers for each convolution layer. Our evaluations show that the proposed MDP with NDP architecture accelerates training by 2.7×, 9.5-21× compared to the data parallel training with the NDP architect...

---
### SCOPE: A Stochastic Computing Engine for DRAM-Based In-Situ Accelerator.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00062
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574579
* **Key Words**: Decoding, Registers, Arrays, Logic arrays, Microarchitecture, Generators, Table lookup, digital arithmetic, DRAM chips, integrated circuit design, integrated memory circuits, learning (artificial intelligence), logic design, logic gates, memory architecture, neural nets, stochastic processes, DRAM-based in-situ accelerator, stochastic computing engine, memory-centric architecture, computing logic, memory resources, deep neural network inference, hybrid deterministic stochastic computing arithmetic, hierarchical deterministic stochastic computing arithmetic, bitwise AND operations, integer multiplications, stochastic bitstreams, error-tolerant applications, tightly couple processing units, SCOPE, memory bitline, DRAM, Accelerator, Stochastic Computing, 
* **Abstract**: Memory-centric architecture, which bridges the gap between compute and memory, is considered as a promising solution to tackle the memory wall and the power wall. Such architecture integrates the computing logic and the memory resources close to each other, in order to embrace large internal memory bandwidth and reduce the data movement overhead. The closer the compute and memory resources are located, the greater these benefits become. DRAM-based in-situ accelerators [1] tightly couple processing units to every memory bitline, achieving the maximum benefits among various memory-centric architectures. However, the processing units in such architectures are typically limited to simple functions like AND/OR due to strict area and power overhead constraints in DRAMs, making it difficult to accomplish complex tasks while providing high performance. In this paper, we address the challenge by applying stochastic computing arithmetic to the DRAM-based in-situ accelerator, targeting at the acceleration of error-tolerant applications such as deep learning. In stochastic computing, binary numbers are converted into stochastic bitstreams, which turns integer multiplications into simple bitwise AND operations, but at the expense of larger memory capacity/bandwidth demands. Stochastic computing is a perfect match for the DRAM-based in-situ accelerators because it addresses the in-situ accelerator's low performance problem by simplifying the operations, while leveraging the in-situ accelerator's advantage of large memory capacity/bandwidth. To further boost the performance and compensate for the numerical precision loss, we propose a novel Hierarchical and Hybrid Deterministic (H 2 D) stochastic computing arithmetic. Finally, we consider quantized deep neural network inference and training applications as a case study. The proposed architecture provides 2.3× improvement in performance per unit area compared with the binary arithmetic baseline, and 3.8× improvement over GPU. The p...

---
### Exploring and Optimizing Chipkill-Correct for Persistent Memory Based on High-Density NVRAMs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00063
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574580
* **Key Words**: Random access memory, Nonvolatile memory, Error correction codes, Servers, Reliability, Runtime, Error correction, DRAM chips, error correction, error statistics, integrated circuit reliability, storage management, server main memory, system crash, storage systems, boot time, bit error rates, memory block, bit error correction scheme, nonvolatile random access memories, chipkill-correct scheme, NVRAMs, DRAM subsystems, dynamic random access memory, very long ECC words, VLEWs, chip failure protection bits, ECC, Microarchitecture, Persistent Memory Systems, Reliability, 
* **Abstract**: Emerging high-density non-volatile random access memories (NVRAMs) can significantly enhance server main memory by providing both higher memory density and fast persistent memory. An unique design requirement for server main memory is strong reliability because uncorrectable errors can cause a system crash or permanent data loss. Traditional dynamic random access memory (DRAM) subsystems have used chipkill-correct to provide this reliability, while storage systems provide similar protection using very long ECC words (VLEWs). This paper presents an efficient chipkill-correct scheme for persistent memory based on high-density NVRAMs. For efficiency, the scheme decouples error correction at boot time from error correction at runtime. At boot time, when bit error rates are higher, the scheme uses VLEWs to efficiently ensure reliable data survival for a week to a year without refresh by correcting a large number of bit errors at low storage cost. At runtime, when bit error rates are lower, it reuses each memory block's chip failure protection bits to opportunistically correct bit errors at high performance. The proposal incurs a total storage cost of 27%. Compared to a bit error correction scheme, the proposal adds chip failure protection at no additional storage cost and at 2% average performance overhead.

---
### Comprehensive Evaluation of Supply Voltage Underscaling in FPGA on-Chip Memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00064
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574581
* **Key Words**: Field programmable gate arrays, Reliability, Artificial neural networks, Voltage control, Circuit faults, System-on-chip, Power demand, field programmable gate arrays, power aware computing, random-access storage, FPGA on-chip memories, voltage scaling, nominal level, energy consumption, voltage guardband, chip vendors, worst-case process, environmental scenarios, FPGA architectures, magnitude power savings, low-voltage BRAM operations, substantial NN energy savings, NN accuracy loss, undervolting faults, comprehensive evaluation, supply voltage underscaling, field programmable gate arrays, typical FPGA-based neural network accelerator, FPGA, Voltage Scaling, Energy, Reliability, Neural Network, 
* **Abstract**: In this work, we evaluate aggressive undervolting, i.e., voltage scaling below the nominal level to reduce the energy consumption of Field Programmable Gate Arrays (FPGAs). Usually, voltage guardbands are added by chip vendors to ensure the worst-case process and environmental scenarios. Through experimenting on several FPGA architectures, we measure this voltage guardband to be on average 39% of the nominal level, which in turn, delivers more than an order of magnitude power savings. However, further undervolting below the voltage guardband may cause reliability issues as the result of the circuit delay increase, i.e., start to appear faults. We extensively characterize the behavior of these faults in terms of the rate, location, type, as well as sensitivity to environmental temperature, with a concentration of on-chip memories, or Block RAMs (BRAMs). Finally, we evaluate a typical FPGA-based Neural Network (NN) accelerator under low-voltage BRAM operations. In consequence, the substantial NN energy savings come with the cost of NN accuracy loss. To attain power savings without NN accuracy loss, we propose a novel technique that relies on the deterministic behavior of undervolting faults and can limit the accuracy loss to 0.1% without any timing-slack overhead.

---
### Error Correlation Prediction in Lockstep Processors for Safety-Critical Systems.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00065
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574582
* **Key Words**: Control systems, Program processors, Hardware, Correlation, Built-in self-test, Hazards, fault tolerant computing, microprocessor chips, program diagnostics, safety-critical software, lockstep processor, safety-critical system, lockstep error checker, error correlation prediction, online diagnostics, CPU, Lockstepping, Redundant execution, Fault tolerance, Functional safety, 
* **Abstract**: This paper presents a new phenomenon called error correlation prediction for lockstep processors. Lockstep processors run the same copy of a program, and their outputs are compared at every cycle to detect divergence, and have been popular in safety-critical systems. When the lockstep error checker detects an error, it alerts the safety-critical system by putting the lockstep processor in a safe state in order to prevent hazards. This is done by running the online diagnostics to identify the cause of the error because the lockstep processor has no knowledge of whether the error is caused by a transient or permanent fault. The online diagnostics can be avoided if the error is caused by a transient fault, and the lockstep processor can recover from it. If, however, it is caused by a permanent fault, having prior knowledge about error's likely location(s) within the CPU speeds up the diagnostics process. We discover that the error's type and likely location(s) inside CPUs from which the fault may have originated can be predicted by analyzing the output signals of the CPU(s) when the error is detected. We design a simple static predictor exploiting this phenomenon and show that system availability can be increased by 42-64% with an overhead of less than 2% in silicon area and power.

---
### Fault Site Pruning for Practical Reliability Analysis of GPGPU Applications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00066
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574583
* **Key Words**: Kernel, Resilience, Graphics processing units, Registers, Reliability, Message systems, Instruction sets, graphics processing units, parallel processing, power aware computing, reliability, GPGPU applications, fault injections, fault site pruning, energy-efficient data-parallel computing, application output quality, general purpose GPU applications, CPU applications, fault site reduction, power budget, register bit position, GPGPU Reliability, Fault injection, GPGPU Applications Error Resilience, Fault Site Pruning, 
* **Abstract**: Graphics Processing Units (GPUs) have rapidly evolved to enable energy-efficient data-parallel computing for a broad range of scientific areas. While GPUs achieve exascale performance at a stringent power budget, they are also susceptible to soft errors, often caused by high-energy particle strikes, that can significantly affect the application output quality. Understanding the resilience of general purpose GPU applications is the purpose of this study. To this end, it is imperative to explore the range of application output by injecting faults at all the potential fault sites. This problem is especially challenging because unlike CPU applications, which are mostly single-threaded, GPGPU applications can contain hundreds to thousands of threads, resulting in a tremendously large fault site space - in the order of billions even for some simple applications. In this paper, we present a systematic way to progressively prune the fault site space aiming to dramatically reduce the number of fault injections such that assessment for GPGPU application error resilience can be practical. The key insight behind our proposed methodology stems from the fact that GPGPU applications spawn a lot of threads, however, many of them execute the same set of instructions. Therefore, several fault sites are redundant and can be pruned by a careful analysis of faults across threads and instructions. We identify important features across a set of 10 applications (16 kernels) from Rodinia and Polybench suites and conclude that threads can be first classified based on the number of the dynamic instructions they execute. We achieve significant fault site reduction by analyzing only a small subset of threads that are representative of the dynamic instruction behavior (and therefore error resilience behavior) of the GPGPU applications. Further pruning is achieved by identifying and analyzing: a) the dynamic instruction commonalities (and differences) across code blocks within this representative...

---
### SwapCodes: Error Codes for Hardware-Software Cooperative GPU Pipeline Error Detection.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00067
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574584
* **Key Words**: Pipelines, Error correction codes, Registers, Graphics processing units, Instruction sets, Hardware, error correction codes, error detection, graphics processing units, multi-threading, pipeline processing, hardware-software cooperative GPU pipeline error detection, intra-thread instruction duplication, software-enforced instruction duplication, explicit checking instructions, software-hardware cooperative mechanisms, intra-thread duplication, register file ECC hardware, storage errors, instruction checking, hardware error checkers, SwapCodes, program register usage, arithmetic operation count per thread, error checking and correcting codes, Error Correcting Codes, Error Detecting Codes, Instruction Duplication, GPU, Computer Architecture, 
* **Abstract**: Intra-thread instruction duplication offers straightforward and effective pipeline error detection for data-intensive processors. However, software-enforced instruction duplication uses explicit checking instructions, roughly doubles program register usage, and doubles the arithmetic operation count per thread, potentially leading to severe slowdowns. This paper investigates SwapCodes, a family of software-hardware cooperative mechanisms to accelerate intra-thread duplication in GPUs. SwapCodes leverages the register file ECC hardware to detect pipeline errors without sacrificing the ability of ECC to detect and correct storage errors. By implicitly checking for pipeline errors on each register read, SwapCodes avoids the overheads of instruction checking without adding new hardware error checkers or buffers. We describe a family of SwapCodes implementations that successively eliminate the sources of inefficiency in intra-thread duplication with different complexities and error detection and correction trade-offs. We apply SwapCodes to protect a GPU-based processor against pipeline errors, and demonstrate that it is able to detect more than 99.3% of pipeline errors while improving performance and system efficiency relative to software-enforced duplication-the most performant SwapCodes organization incurs just 15% average slowdown over the un-duplicated program.

---
### CEASER: Mitigating Conflict-Based Cache Attacks via Encrypted-Address and Remapping.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00068
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574585
* **Key Words**: Programmable logic arrays, Encryption, Program processors, Timing, Indexes, cache storage, cryptography, operating systems (computers), dynamic-remapping, mitigate conflict-based cache attacks, low-latency block-cipher, LLBC, decryption, encryption key, encrypted line-address, physical line-address, OS support, storage-intensive tables, randomized mapping, cache conflicts, cache space, CEASER, 
* **Abstract**: Modern processors share the last-level cache between all the cores to efficiently utilize the cache space. Unfortunately, such sharing makes the cache vulnerable to attacks whereby an adversary can infer the access pattern of a co-running application by carefully orchestrating evictions using cache conflicts. Conflict-based attacks can be mitigated by randomizing the location of the lines in the cache. Unfortunately, prior proposals for randomized mapping require storage-intensive tables and are effective only if the OS can classify the applications into protected and unprotected groups. The goal of this paper is to mitigate conflict-based attacks while incurring negligible storage and performance overheads, and without relying on OS support. This paper provides the key insight that randomized mapping can be accomplished efficiently by accessing the cache with an encrypted address, as encryption would cause the lines that map to the same set of a conventional cache to get scattered to different sets. This paper proposes CEASE, a design that uses Low-Latency Block-Cipher (LLBC) to translate the physical line-address into an encrypted line-address, and accesses the cache with this encrypted line-address. We analyze efficient designs for LLBC that can perform encryption and decryption within two cycles. We also propose CEASER, a design that periodically changes the encryption key and performs dynamic-remapping to improve robustness. CEASER provides strong security (tolerates 100+ years of attack), has low performance overhead (1% slowdown), requires a storage overhead of less than 24 bytes for the newly added structures, and does not need any OS support.

---
### PipeProof: Automated Memory Consistency Proofs for Microarchitectural Specifications.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00069
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574586
* **Key Words**: Microarchitecture, Hardware, Computer bugs, Radio frequency, Pipelines, Load modeling, Tools, data structures, formal specification, microprocessor chips, parallel programming, program verification, storage management, automated memory consistency proofs, microarchitectural specifications, load instructions, parallel programs, axiomatic microarchitectural ordering specification, axiomatic ISA-level MCM specification, microarchitectural executions, microarchitectural correctness, automatic abstraction refinement approach, microarchitectural MCM correctness proofs, PipeProof, hardware MCM verification, transitive chain abstraction, SC MCM, TSO MCM, Memory consistency models, automated verification, formal verification, happens-before graphs, abstraction refinement, 
* **Abstract**: Memory consistency models (MCMs) specify rules which constrain the values that can be returned by load instructions in parallel programs. To ensure that parallel programs run correctly, verification of hardware MCM implementations would ideally be complete; i.e. verified as being correct across all possible executions of all possible programs. However, no existing automated approach is capable of such complete verification. To help fill this verification gap, we present PipeProof, a methodology and tool for complete MCM verification of an axiomatic microarchitectural (hardware-level) ordering specification against an axiomatic ISA-level MCM specification. PipeProof can automatically prove a microarchitecture correct in all cases, or return an indication (often a counterexample) that the microarchitecture could not be verified. To accomplish unbounded verification, PipeProof introduces the novel Transitive Chain Abstraction to represent microarchitectural executions of an arbitrary number of instructions using only a small, finite number of instructions. With the help of this abstraction, PipeProof proves microarchitectural correctness using an automatic abstraction refinement approach. PipeProof's implementation also includes algorithmic optimizations which improve runtime by greatly reducing the number of cases considered. As a proof-of-concept study, we present results for modelling and proving correct simple microarchitectures implementing the SC and TSO MCMs. PipeProof verifies both case studies in under an hour, showing that it is indeed possible to automate microarchitectural MCM correctness proofs.

---
### Application-Transparent Near-Memory Processing Architecture with Memory Channel Network.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00070
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574587
* **Key Words**: Servers, Bandwidth, Random access memory, Hardware, Computer architecture, Ethernet, device drivers, DRAM chips, local area networks, message passing, low-latency communications, high-bandwidthcommunication, DRAM bandwidth utilization, MPI, ethernet links, memory channel network architecture, application-transparent near-memory processing architecture, servers capacity, software application, distributed computing frameworks, host processor hardware, MCN DIMM, nonvolatile memory technologies, data-intensive applications, Near Memory Processing, Processing In Memory, Ethernet, TCP IP, Distributed Systems, Mobile Processors, Application Transparent, Memory Channel, DRAM, Buffer Device, 
* **Abstract**: The physical memory capacity of servers is expected to increase drastically with deployment of the forthcoming non-volatile memory technologies. This is a welcomed improvement for emerging data-intensive applications. For such servers to be cost-effective, nonetheless, we must cost-effectively increase compute throughput and memory bandwidth commensurate with the increase in memory capacity without compromising application readiness. Tackling this challenge, we present Memory Channel Network (MCN) architecture in this paper. Specifically, first, we propose an MCN DIMM, an extension of a buffered DIMM where a small but capable processor called MCN processor is integrated with a buffer device on the DIMM for near-memory processing. Second, we implement device drivers to give the host and MCN processors in a server an illusion that they are independent heterogeneous nodes connected through an Ethernet link. These allow the host and MCN processors in a server to run a given data-intensive application together based on popular distributed computing frameworks such as MPI and Spark without any change in the host processor hardware and its application software, while offering the benefits of high-bandwidth and low-latency communications between the host and the MCN processors over memory channels. As such, MCN can serve as an application-transparent framework which can seamlessly unify near-memory processing within a server and distributed computing across such servers for data-intensive applications. Our simulation running the full software stack shows that a server with 8 MCN DIMMs offers 4.56X higher throughput and consume 47.5% less energy than a cluster with 9 conventional nodes connected through Ethernet links, as it facilitates up to 8.17X higher aggregate DRAM bandwidth utilization. Lastly, we demonstrate the feasibility of MCN with an IBM POWER8 system and an experimental buffered DIMM.

---
### End-to-End Automated Exploit Generation for Validating the Security of Processor Designs.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00071
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574588
* **Key Words**: Security, Hardware, Computer bugs, C++ languages, Hardware design languages, Clocks, Engines, field programmable gate arrays, microprocessor chips, optimisation, program debugging, program testing, program verification, security of data, end-to-end automated exploit generation, FPGA board, academic model checking tools, commercial model checking tools, CPUs, program stubs, validation technique, cycle stitching method, hardware-oriented backward symbolic execution engine, hardware vulnerabilities, security-critical invariants, processor design, Symbolic Execution, Exploit Generation, Processor Security, 
* **Abstract**: This paper presents Coppelia, an end-to-end tool that, given a processor design and a set of security-critical invariants, automatically generates complete, replayable exploit programs to help designers find, contextualize, and assess the security threat of hardware vulnerabilities. In Coppelia, we develop a hardware-oriented backward symbolic execution engine with a new cycle stitching method and fast validation technique, along with several optimizations for exploit generation. We then add program stubs to complete the exploit. We evaluate Coppelia on three CPUs of different architectures. Coppelia is able to find and generate exploits for 29 of 31 known vulnerabilities in these CPUs, including 11 vulnerabilities that commercial and academic model checking tools can not find. All of the generated exploits are successfully replayable on an FPGA board. Moreover, Coppelia finds 4 new vulnerabilities along with exploits in these CPUs. We also use Coppelia to verify whether a security patch indeed fixed a vulnerability, and to refine a set of assertions.

---
### Magic-State Functional Units: Mapping and Scheduling Multi-Level Distillation Circuits for Fault-Tolerant Quantum Architectures.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00072
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574589
* **Key Words**: Logic gates, Protocols, Production facilities, Error correction codes, Error analysis, error correction codes, fault tolerant computing, graph theory, logic circuits, quantum gates, scheduling, fault-tolerant quantum computation, reliable error-corrected computation, hardware functional units, space-time optimized magic-state factories, surface code error-corrected machines, distant qubits, surface code braids, quantum circuits, scheduling techniques, gate reordering, qubit renaming, mapping techniques, multilevel distillation circuit scheduling, multilevel distillation circuit mapping, high-fidelity encoded qubit production, braid repulsion, graph partitioning, community detection algorithms, stitching algorithm, mapping subgraphs, physical machine, space-time volume, long-term path, quantum computers, fault-tolerant quantum architectures, magic-state functional units, dipole moment braid rotation, Quantum Computing, Quantum Error Correction, Surface Code, Magic State Distillation, 
* **Abstract**: Quantum computers have recently made great strides and are on a long-term path towards useful fault-tolerant computation. A dominant overhead in fault-tolerant quantum computation is the production of high-fidelity encoded qubits, called magic states, which enable reliable error-corrected computation. We present the first detailed designs of hardware functional units that implement space-time optimized magic-state factories for surface code error-corrected machines. Interactions among distant qubits require surface code braids (physical pathways on chip) which must be routed. Magic-state factories are circuits comprised of a complex set of braids that is more difficult to route than quantum circuits considered in previous work [1]. This paper explores the impact of scheduling techniques, such as gate reordering and qubit renaming, and we propose two novel mapping techniques: braid repulsion and dipole moment braid rotation. We combine these techniques with graph partitioning and community detection algorithms, and further introduce a stitching algorithm for mapping subgraphs onto a physical machine. Our results show a factor of 5.64 reduction in space-time volume compared to the best-known previous designs for magic-state factories.

---
### MDACache: Caching for Multi-Dimensional-Access Memories.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00073
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574590
* **Key Words**: Random access memory, Organizations, Nonvolatile memory, Software, Arrays, Layout, System-on-chip, cache storage, DRAM chips, integrated circuit design, SRAM chips, MDA cache hierarchy, MDA memory systems, software memory layout, multidimensional access patterns, MDACache, multidimensional-access memories, DRAM implementation, SRAM implementation, vectorization support, compiler level, 2-D caching, 1D SRAM structure, Cache design, Symmetric memories, Crosspoint memory, Row-Column vectorization, 2D memory access, 
* **Abstract**: For several emerging memory technologies, a natural formulation of memory arrays (cross-point) provides nearly symmetric access costs along multiple (e.g., both row and column) dimensions in contrast to the row-oriented nature of most DRAM and SRAM implementations, producing a Multi-Dimensional-Access (MDA) memory. While MDA memories can directly support applications with both row and column preferences, most modern processors do not directly access either the rows or columns of memories: memory accesses proceed through a cache hierarchy that abstracts many of the physical features that supply the aforementioned symmetry. To reap the full benefits of MDA memories, a co-design approach must occur across software memory layout, the mapping between the physical and logical organization of the memory arrays, and the cache hierarchy itself in order to efficiently express, convey, and exploit multidimensional access patterns. In this paper, we describe a taxonomy for different ways of connecting row and column preferences at the application level to an MDA memory through an MDA cache hierarchy and explore specific implementations for the most plausible design points. We extend vectorization support at the compiler level to provide the necessary information to extract preferences and provide compatible memory layouts, and evaluate the tradeoffs among multiple cache designs for the MDA memory systems. Our results indicate that both logically 2-D caching using physically 1-D SRAM structures and on-chip physically 2-D caches can both provide significant improvements in performance over a traditional cache system interfacing with an MDA memory, reducing execution time by 72% and 65%, respectively. We then explore the sensitivity of these benefits as a function of the working-set to cache capacity ratio as well as to MDA technology assumptions.

---
### GeneSys: Enabling Continuous Learning through Neural Network Evolution in Hardware.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00074
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574591
* **Key Words**: Artificial neural networks, Topology, Genomics, Bioinformatics, Sociology, Statistics, Engines, backpropagation, evolutionary computation, hardware-software codesign, inference mechanisms, neural nets, parallel processing, topology, ubiquitous autonomous agents, heavy-weight processing, HW-SW prototype, closed loop learning engine, EvE, inference engine, ADAM, hand-optimization, backpropagation training, GPU systems, GeneSys, neural network evolution, modern deep learning systems, hand-tuned neural network topology, large-scale compute resources, adaptive general purpose intelligent systems, evolutionary algorithm based methods, higher energy-efficiency, image classification, hardware, continuous learning, speech recognition, reinforcement learning based methods, EA-based learning system, gene-level parallelism, population-level parallelism, OpenAI gym, CPU systems, accelerator, hardware, microarchitecture, artificial intelligence, general purpose, continuous learning, hardware software codesign, adam, eve, SoC, 
* **Abstract**: Modern deep learning systems rely on (a) a hand-tuned neural network topology, (b) massive amounts of labeled training data, and (c) extensive training over large-scale compute resources to build a system that can perform efficient image classification or speech recognition. Unfortunately, we are still far away from implementing adaptive general purpose intelligent systems which would need to learn autonomously in unknown environments and may not have access to some or any of these three components. Reinforcement learning and evolutionary algorithm (EA) based methods circumvent this problem by continuously interacting with the environment and updating the models based on obtained rewards. However, deploying these algorithms on ubiquitous autonomous agents at the edge (robots/drones) demands extremely high energy-efficiency due to (i) tight power and energy budgets, (ii) continuous/lifelong interaction with the environment, (iii) intermittent or no connectivity to the cloud to run heavy-weight processing. To address this need, we present GENESYS, an HW-SW prototype of an EA-based learning system, that comprises a closed loop learning engine called EvE and an inference engine called ADAM. EvE can evolve the topology and weights of neural networks completely in hardware for the task at hand, without requiring hand-optimization or backpropagation training. ADAM continuously interacts with the environment and is optimized for efficiently running the irregular neural networks generated by EvE. GENESYS identifies and leverages multiple unique avenues of parallelism unique to EAs that we term "gene"- level parallelism, and "population"-level parallelism. We ran GENESYS with a suite of environments from OpenAI gym and observed 2-5 orders of magnitude higher energy-efficiency over state-of-the-art embedded and desktop CPU and GPU systems.

---
### CritICs Critiquing Criticality in Mobile Apps.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00075
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574592
* **Key Words**: Optimization, Hardware, Prefetching, Pipelines, Integrated circuits, Aggregates, microprocessor chips, mobile computing, optimisation, pipeline processing, program compilers, CPU optimizations, ISA format, compiler, software mechanism, datapath pipeline, criticality optimizations, critical instruction chains, mobile app characteristics, systematic analysis, CritICs critiquing criticality, aggregate level optimization, dependence chain, Criticality, CPU, Mobile, Energy, 
* **Abstract**: In this paper, we conduct a systematic analysis to show that existing CPU optimizations targeting scientific/server workloads are not always well suited for mobile apps. In particular, we observe that the well-known and very important concept of identifying and accelerating individual critical instructions in workloads such as SPEC, are not as effective for mobile apps. Several differences in mobile app characteristics including (i) dependencies between critical instructions interspersed with non-critical instructions in the dependence chain, (ii) temporal proximity of the critical instructions in the dynamic stream, and (iii) the bottleneck shifting to the front from the rear of the datapath pipeline, are key contributors to the ineffectiveness of traditional criticality based optimizations. Instead, we propose the concept of Critical Instruction Chains (CritICs) - which are short, critical and self contained sequences of instructions, for aggregate level optimization. With motivating results, we show that an offline profiler/analysis framework can easily identify these CritICs, and we propose a very simple software mechanism in the compiler that exploits ARM's 16-bit ISA format to nearly double the fetch bandwidth of these instructions. We have implemented this entire framework - both profiler and compiler passes, and evaluated its effectiveness for 10 popular apps from the Play Store. Experimental evaluations show that our approach is much more effective than two previously studied criticality optimizations, yielding a speedup of 12.65%, and energy savings of 15% in the CPU (translating to a system wide energy savings of 4.6%), requiring very little additional hardware support.

---
### EMPROF: Memory Profiling Via EM-Emanation in IoT and Hand-Held Devices.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00076
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574593
* **Key Words**: Hardware, Microarchitecture, Instruments, Real-time systems, Power demand, Time-domain analysis, Performance evaluation, cache storage, Internet of Things, storage management, hard-to-profile software components, LLC misses, processor stalls, memory profiling, profiled system, EMPROF analyzes, hand-held devices, memory subsystem, EM-emanation, IoT devices, software instrumentation, last-level cache, observer effect, target machine, embedded devices, boot code, application code, SPEC, cycle-accurate simulator, EM signal bandwidth, hardware support instrumentation, EM side-channel, last-level cache, main memory, memory profiling, EM emanation, system profiling, cycle accurate simulator, code attribution, 
* **Abstract**: This paper presents EMPROF, a new method for profiling the performance impact of the memory subsystem without any support on, or interference with, the profiled system. Rather than rely on hardware support and/or software instrumentation on the profiled system, EMPROF analyzes the system's EM emanations to identify processor stalls that are associated with last-level cache (LLC) misses. This enables EMPROF to accurately pinpoint LLC misses in the execution timeline and to measure the cost (stall time) of each miss. Since EMPROF has zero "observer effect", so it can be used to profile applications that adjust their activity to their performance. It has no overhead on target machine, so it can be used for profiling embedded, hand-held, and IoT devices which usually have limited support for collecting, and limited resources for storing, the profiling data. Finally, since EMPROF can profile the system as-is, its profiling of boot code and other hard-to-profile software components is as accurate as its profiling of application code. To illustrate the effectiveness of EMPROF, we first validate its results using microbenchmarks with known memory behavior, and also on SPEC benchmarks running a cycle-accurate simulator that can provide detailed ground-truth data about LLC misses and processor stalls. We then demonstrate the effectiveness of EMPROF on real systems, including profiling of boot activity, show how its results can be attributed to the specific parts of the application code when that code is available, and provide additional insight on the statistics reported by EMPROF and how they are affected by the EM signal bandwidth provided to EMPROF.

---
### MAVBench: Micro Aerial Vehicle Benchmarking.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00077
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574594
* **Key Words**: Drones, Batteries, Benchmark testing, FAA, Rotors, Hardware, Kernel, autonomous aerial vehicles, battery powered vehicles, closed loop systems, mobile robots, autonomous MAVs, MAVBench framework, closed-loop simulation platform, total system energy relationship, microaerial vehicle benchmarking, battery consumption, unmanned aerial vehicles, aerial computing systems, Benchmark, robotics, Aerial computing, simuator, micro aerial vehicles, 
* **Abstract**: Unmanned Aerial Vehicles (UAVs) are getting closer to becoming ubiquitous in everyday life. Among them, Micro Aerial Vehicles (MAVs) have seen an outburst of attention recently, specifically in the area with a demand for autonomy. A key challenge standing in the way of making MAVs autonomous is that researchers lack the comprehensive understanding of how performance, power, and computational bottlenecks affect MAV applications. MAVs must operate under a stringent power budget, which severely limits their flight endurance time. As such, there is a need for new tools, benchmarks, and methodologies to foster the systematic development of autonomous MAVs. In this paper, we introduce the MAVBench' framework which consists of a closed-loop simulator and an end-to-end application benchmark suite. A closed-loop simulation platform is needed to probe and understand the intra-system (application data flow) and inter-system (system and environment) interactions in MAV applications to pinpoint bottlenecks and identify opportunities for hardware and software co-design and optimization. In addition to the simulator, MAVBench provides a benchmark suite, the first of its kind, consisting of a variety of MAV applications designed to enable computer architects to perform characterization and develop future aerial computing systems. Using our open source, end-to-end experimental platform, we uncover a hidden, and thus far unexpected compute to total system energy relationship in MAVs. Furthermore, we explore the role of compute by presenting three case studies targeting performance, energy and reliability. These studies confirm that an efficient system design can improve MAV's battery consumption by up to 1.8X.

---
### Architectural Support for Efficient Large-Scale Automata Processing.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00078
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574595
* **Key Words**: Routing, Learning automata, Parallel processing, Acceleration, Automata, Machine learning, Random access memory, finite automata, learning (artificial intelligence), baseline AP execution, machine learning, spatial architecture, non-deterministic finite automata, SparseAP, NFA states, architectural support, automata programs, automata processor, profiling-based mechanisms, AP chips, Accelerators, Automata, Performance, 
* **Abstract**: The Automata Processor (AP) accelerates applications from domains ranging from machine learning to genomics. However, as a spatial architecture, it is unable to handle larger automata programs without repeated reconfiguration and re-execution. To achieve high throughput, this paper proposes for the first time architectural support for AP to efficiently execute large-scale applications. We find that a large number of existing and new Non-deterministic Finite Automata (NFA) based applications have states that are never enabled but are still configured on the AP chips leading to their underutilization. With the help of careful characterization and profiling-based mechanisms, we predict which states are never enabled and hence need not be configured on AP. Furthermore, we develop SparseAP, a new execution mode for AP to efficiently handle the mis-predicted NFA states. Our detailed simulations across 26 applications from various domains show that our newly proposed execution model for AP can obtain 2.1x geometric mean speedup (up to 47x) over the baseline AP execution.

---
### ASPEN: A Scalable In-SRAM Architecture for Pushdown Automata.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00079
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574596
* **Key Words**: Automata, XML, Computer architecture, Task analysis, Data mining, Computational modeling, automata theory, data mining, grammars, object-oriented programming, parallel architectures, program compilers, pushdown automata, tree data structures, XML, irregular memory access patterns, data processing applications, memory-centric automata processing engines, XML parsing, ASPEN, reconfigurable memory-centric architecture, previous automata processing architectures, richer deterministic pushdown automata computational model, transition routing operations, memory arrays, automata executable, application scenarios, frequent subtree mining, In-SRAM architecture, tree-structured, data mining tasks, typical CPU processing solutions, branch misprediction penalties, nested structures, XML parser, JSON Web content, pushdown automata, emerging technologies (memory and computing), accelerators, 
* **Abstract**: Many applications process some form of tree-structured or recursively-nested data, such as parsing XML or JSON web content as well as various data mining tasks. Typical CPU processing solutions are hindered by branch misprediction penalties while attempting to reconstruct nested structures and also by irregular memory access patterns. Recent work has demonstrated improved performance for many data processing applications through memory-centric automata processing engines. Unfortunately, these architectures do not support a computational model rich enough for tasks such as XML parsing. In this paper, we present ASPEN, a general-purpose, scalable, and reconfigurable memory-centric architecture for processing of tree-like data. We take inspiration from previous automata processing architectures, but support the richer deterministic pushdown automata computational model. We propose a custom datapath capable of performing the state matching, stack manipulation, and transition routing operations of pushdown automata, all efficiently stored and computed in memory arrays. Further, we present compilation algorithms for transforming large classes of existing grammars to pushdown automata executable on ASPEN, and demonstrate their effectiveness on four different languages: Cool (object oriented programming), DOT (graph visualization), JSON, and XML. Finally, we present an empirical evaluation of two application scenarios for ASPEN: XML parsing, and frequent subtree mining. The proposed architecture achieves an average 704.5 ns per KB parsing XML compared to 9983 ns per KB in a state-of-the-art XML parser across 23 benchmarks. We also demonstrate a 37.2x and 6x better end-to-end speedup over CPU and GPU implementations of subtree mining.

---
### Morph: Flexible Acceleration for 3D CNN-Based Video Understanding.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00080
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574597
* **Key Words**: Three-dimensional displays, Two dimensional displays, Convolution, System-on-chip, Image recognition, Streaming media, Acceleration, convolution, energy consumption, feedforward neural nets, hardware-software codesign, image recognition, Internet, object recognition, video signal processing, CNN inference, image recognition, video recognition, video traffic, design space exploration, Morph hardware, 3D convolutional neural networks, 3D CNN-based video, 2D CNN accelerator, 3D CNN accelerator, core kernel, memory footprint, software infrastructure, energy consumption, energy reduction, Eyeriss, Internet traffic, 3D Convolutional Neural Networks, Hardware/Software codesign, Video recognition, Dataflow, Hardware acceleration, 
* **Abstract**: The past several years have seen both an explosion in the use of Convolutional Neural Networks (CNNs) and accelerators to make CNN inference practical. In the architecture community, the lion share of effort has targeted CNN inference for image recognition. The closely related problem of video recognition has received far less attention as an accelerator target. This is surprising, as video recognition is more computationally intensive than image recognition, and video traffic is predicted to be the majority of internet traffic in the coming years. This paper fills the gap between algorithmic and hardware advances for video recognition by providing a design space exploration and flexible architecture for accelerating 3D Convolutional Neural Networks (3D CNNs)-the core kernel in modern video understanding. When compared to (2D) CNNs used for image recognition, efficiently accelerating 3D CNNs poses a significant engineering challenge due to their large (and variable over time) memory footprint and higher dimensionality. To address these challenges, we design a novel accelerator called "Morph," that can adaptively support different spatial and temporal tiling strategies depending on the needs of each layer of each target 3D CNN. We codesign a software infrastructure alongside the Morph hardware to find good-fit parameters to control the hardware. Evaluated on state-of-the-art 3D CNNs, Morph achieves up to 2.7× (1.9× average) reduction in energy consumption and improves performance/watt up to 4.4× (3× average) compared to a baseline 3D CNN accelerator, with an area overhead of 2%. Morph further achieves a 11.6× average energy reduction on 3D CNNs when compared to Eyeriss, a popular 2D CNN accelerator, while reducing efficiency compared to Eyeriss on a 2D CNN by 71%.

---
### CheckMate: Automated Synthesis of Hardware Exploits and Security Litmus Tests.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00081
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574598
* **Key Words**: Microarchitecture, Hardware, Side-channel attacks, Timing, Tools, Kernel, cache storage, data privacy, program diagnostics, program verification, security of data, program synthesis, microarchitecture, pattern specifications, out-of-order processor, Flush+Reload cache side-channel attacks, formal hardware security verification, CheckMate tool, security litmus tests, security vulnerabilities, confidential data, programmer-observable microarchitectural state, automated tool, security exploits, hardware execution events, μhb graphs, hardware execution patterns, cache pollution, hardware exploits automated synthesis, timing side-channel attack, microarchitecturally happens-before graphs, relational model finding techniques, automated exploit program synthesis, Spectre attacks, Meltdown attacks, Prime+Probe, speculative cache line invalidations, hardware security, automated verification, relational model finding, exploit synthesis, side-channel attacks, 
* **Abstract**: Recent research has uncovered a broad class of security vulnerabilities in which confidential data is leaked through programmer-observable microarchitectural state. In this paper, we present CheckMate, a rigorous approach and automated tool for determining if a microarchitecture is susceptible to specified classes of security exploits, and for synthesizing proof-of-concept exploit code when it is. Our approach adopts "microarchitecturally happens-before" (μhb) graphs which prior work designed to capture the subtle orderings and interleavings of hardware execution events when programs run on a microarchitecture. CheckMate extends μhb graphs to facilitate modeling of security exploit scenarios and hardware execution patterns indicative of classes of exploits. Furthermore, it leverages relational model finding techniques to enable automated exploit program synthesis from microarchitecture and exploit pattern specifications. As a case study, we use CheckMate to evaluate the susceptibility of a speculative out-of-order processor to Flush+Reload cache side-channel attacks. The automatically synthesized results are programs representative of Meltdown and Spectre attacks. We then evaluate the same processor on its susceptibility to a different timing side-channel attack: Prime+Probe. Here, CheckMate synthesized new exploits that are similar to Meltdown and Spectre in that they leverage speculative execution, but unique in that they exploit distinct microarchitectural behaviors-speculative cache line invalidations rather than speculative cache pollution-to form a side-channel. Most importantly, our results validate the CheckMate approach to formal hardware security verification and the ability of the CheckMate tool to detect real-world vulnerabilities.

---
### Shadow Block: Accelerating ORAM Accesses with Data Duplication.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00082
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574599
* **Key Words**: Timing, Random access memory, Hardware, Computer science, Encryption, cache storage, cryptography, random-access storage, storage management, Shadow block, memory access patterns, intended data block, data blocks, ORAM access, extra dummy ORAM accesses, huge memory access overheads, access order, ORAM security, method leverages dummy blocks, early access, data duplication techniques, data block access, Oblivious RAM, Dummy Block, Duplication, 
* **Abstract**: Oblivious RAM (ORAM) is a cryptographic primitive designed to hide memory access patterns. To achieve this objective, the intended data block is loaded and evicted back together with other data blocks and dummy blocks in each ORAM access. To further protect the timing pattern, extra dummy ORAM accesses are triggered periodically. Such designs lead to huge memory access overheads. Many techniques have been proposed to mitigate this problem by reducing the total number of ORAM accesses and the number of blocks per access. However, the impact of the access order of intended data block in an ORAM access is not addressed yet. In this work, we argue that higher performance can be achieved by advancing the access to the intended data block in ORAM accesses. However, changing the access order of blocks directly compromises the ORAM security. To solve this problem, we propose a duplication method to advance the access to the intended data blocks without compromising the ORAM security. The method leverages dummy blocks to store extra copies of data blocks, to facilitate early access of intended data blocks. These dummy blocks with valid data duplications are called Shadow blocks in this work. We further introduce two data duplication techniques, called RD-Dup and HD-Dup, to reorder the data block access for different purposes. In addition, we propose ORAM space partitioning to make RD-Dup and HD-Dup cooperate with each other efficiently. Compared with state-of-the-art ORAMs, our design can achieve a 32% reduction in system execution time on average, with negligible hardware overheads.

---
### DAWG: A Defense Against Cache Timing Attacks in Speculative Execution Processors.
* **Publisher**: IEEE
* **DOI**: https://doi.org/10.1109/MICRO.2018.00083
* **PDF**: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8574600
* **Key Words**: Receivers, Program processors, Security, Transmitters, Metadata, Hardware, cache storage, security of data, cache timing attacks, dynamically allocated way guard, Intels Cache Allocation Technology, memory caches, generic mechanism, cache state covert channel, entire attack surface, patch specific attacks, existing defense mechanisms, exfiltration channel, cache tag state, speculative processor architectures, channel attacks, speculative execution processors, cache subsystem, minimal modifications, service mechanisms, set associative structure, DAWG, cache partitioning, side channel attacks, speculative execution, 
* **Abstract**: Software side channel attacks have become a serious concern with the recent rash of attacks on speculative processor architectures. Most attacks that have been demonstrated exploit the cache tag state as their exfiltration channel. While many existing defense mechanisms that can be implemented solely in software have been proposed, these mechanisms appear to patch specific attacks, and can be circumvented. In this paper, we propose minimal modifications to hardware to defend against a broad class of attacks, including those based on speculation, with the goal of eliminating the entire attack surface associated with the cache state covert channel. We propose DAWG, Dynamically Allocated Way Guard, a generic mechanism for secure way partitioning of set associative structures including memory caches. DAWG endows a set associative structure with a notion of protection domains to provide strong isolation. When applied to a cache, unlike existing quality of service mechanisms such as Intel's Cache Allocation Technology (CAT), DAWG fully isolates hits, misses, and metadata updates across protection domains. We describe how DAWG can be implemented on a processor with minimal modifications to modern operating systems. We describe a non-interference property that is orthogonal to speculative execution and therefore argue that existing attacks such as Spectre Variant 1 and 2 will not work on a system equipped with DAWG. Finally, we evaluate the performance impact of DAWG on the cache subsystem.
